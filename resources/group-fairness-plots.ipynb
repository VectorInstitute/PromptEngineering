{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy pandas plotly scikit-learn seaborn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Iterable\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix as ConfusionMatrix\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "DEBUG = bool(os.environ.get(\"DEBUG\"))\n",
    "\n",
    "COLUMN_NAMES = [\n",
    "    \"y_pred\",\n",
    "    \"logit_negative\",\n",
    "    \"logit_neutral\",\n",
    "    \"logit_positive\",\n",
    "    \"y_true\",\n",
    "    \"category\",\n",
    "    \"group\",\n",
    "    \"text\",\n",
    "    \"run_id\",\n",
    "    \"model\",\n",
    "    \"dataset\",\n",
    "    \"eval_accuracy\",\n",
    "    \"num_params\",\n",
    "]\n",
    "\n",
    "INDEPENDENT_VARIABLES = [\"category\", \"run_id\", \"model\", \"dataset\", \"eval_accuracy\", \"num_params\"]\n",
    "\n",
    "SELECTED_GROUPS = {\n",
    "    \"grouped_religion\": [\"christianity\", \"islam\", \"judaism\"],\n",
    "}\n",
    "\n",
    "DATASET_RENAME = {\n",
    "    \"semeval_3\": \"SemEval\",\n",
    "    \"sst5_mapped_extreme\": \"SST5- Extreme Only\",\n",
    "    \"sst5_mapped_grouped\": \"SST5\",\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fairness Metric Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(dataframe: pd.DataFrame) -> pd.Series:\n",
    "    output: Dict[str, float] = {}\n",
    "\n",
    "    num_examples = len(dataframe)\n",
    "    num_correct = np.sum(dataframe[\"y_pred\"] == dataframe[\"y_true\"])\n",
    "\n",
    "    output[\"accuracy\"] = num_correct / num_examples\n",
    "\n",
    "    # See https://stackoverflow.com/a/50671617\n",
    "    confusion_matrix = ConfusionMatrix(dataframe[\"y_true\"], dataframe[\"y_pred\"])\n",
    "\n",
    "    FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "    FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "    TP = np.diag(confusion_matrix)\n",
    "    TN = confusion_matrix.sum() - (FP + FN + TP)\n",
    "\n",
    "    FPR = FP / (FP + TN)\n",
    "    FNR = FN / (TP + FN)\n",
    "    TPR = 1 - FNR\n",
    "\n",
    "    # output[\"accuracy\"] = (TP + TN) / (FP+FN+TP+TN)\n",
    "\n",
    "    for label_index, label in enumerate([\"negative\", \"neutral\", \"positive\"]):\n",
    "        output[label + \"_FPR\"] = FPR[label_index]\n",
    "        output[label + \"_TPR\"] = TPR[label_index]\n",
    "\n",
    "    return pd.Series(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_metric_name(metric_name):\n",
    "    output_words = []\n",
    "    for word in metric_name.split(\"_\"):\n",
    "        if word.upper() in [\"TPR\", \"FPR\", \"SST5\"]:\n",
    "            output_words.append(word.upper())\n",
    "\n",
    "        elif word in [\"mapped\"]:\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "            output_words.append(word.title())\n",
    "\n",
    "    return \" \".join(output_words)\n",
    "\n",
    "\n",
    "def rgb_to_hex(rgb):\n",
    "    r, g, b = rgb\n",
    "    r = int(256 * r)\n",
    "    g = int(256 * g)\n",
    "    b = int(256 * b)\n",
    "    return \"#{:02x}{:02x}{:02x}\".format(r, g, b)\n",
    "\n",
    "\n",
    "def get_model_palette(models: Iterable[str]) -> List[str]:\n",
    "    output = []\n",
    "    palettes = {\n",
    "        \"opt\": list(sns.color_palette(\"flare\", n_colors=6))[::-1],  # type: ignore\n",
    "        \"galactica\": list(sns.color_palette(\"crest\", n_colors=6))[::-1],  # type: ignore\n",
    "    }\n",
    "\n",
    "    hex_lookups: Dict[str, Dict[str, str]] = {k: {} for k in palettes.keys()}\n",
    "    variations = [\"125m\", \"350m\", \"1.3b\", \"2.7b\", \"6.7b\", \"13b\"]\n",
    "\n",
    "    for variation_index, model_variation in enumerate(variations):\n",
    "        for model, palette in palettes.items():\n",
    "            hex_lookups[model][model_variation] = rgb_to_hex(palette[variation_index])\n",
    "\n",
    "    for model in models:\n",
    "        model, model_variation = model.split(\"-\")\n",
    "        output.append(hex_lookups[model][model_variation])\n",
    "\n",
    "    return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Predictions Ready"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Prediction Files\n",
    "Load prediction files from a given folder. Calculate basic metrics e.g., accuracy and false-positive rate for each protected group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_folder: str = \"predictions/\"\n",
    "prediction_files: List[str] = os.listdir(prediction_folder)\n",
    "prediction_tsv_paths: List[str] = [\n",
    "    os.path.join(prediction_folder, prediction_file)\n",
    "    for prediction_file in prediction_files\n",
    "    if prediction_file.endswith(\".txt\")\n",
    "]\n",
    "\n",
    "output_folder: str = \"plots\"\n",
    "output_table = pd.DataFrame()\n",
    "\n",
    "for prediction_tsv_path in tqdm(prediction_tsv_paths, ncols=80):\n",
    "    prediction_tsv_filename = os.path.basename(prediction_tsv_path)\n",
    "    dataframe = pd.read_csv(prediction_tsv_path, delimiter=\"\\t\", names=COLUMN_NAMES)\n",
    "    del dataframe[\"text\"]\n",
    "\n",
    "    if output_table is None:\n",
    "        output_table = dataframe\n",
    "    else:\n",
    "        assert isinstance(output_table, pd.DataFrame)\n",
    "        output_table = pd.concat([output_table, dataframe])\n",
    "\n",
    "# output_table\n",
    "analyzed_dataframe = output_table.groupby([*INDEPENDENT_VARIABLES, \"group\"]).apply(get_stats)\n",
    "analyzed_dataframe.query('category == \"grouped_race\"')[[\"positive_TPR\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_table = analyzed_dataframe.reset_index()\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "output_table.to_csv(os.path.join(output_folder, \"aggregated.csv\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Statistics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Gap\": How each group deviates from the category mean\n",
    "Caclulate how each group deviates from the mean of that category (e.g., how the \"young\" group deviates from the median of the \"age\" category.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"accuracy\"]\n",
    "for label in [\"negative\", \"neutral\", \"positive\"]:\n",
    "    metrics.append(label + \"_FPR\")\n",
    "    metrics.append(label + \"_TPR\")\n",
    "\n",
    "gap_metrics = []\n",
    "metrics_hypotheses = []\n",
    "for metric in metrics:\n",
    "    gap_metrics.append(f\"{metric}_gap\")\n",
    "    metrics_hypotheses.append(f\"{metric}_gap_hypothesis\")\n",
    "\n",
    "groups_to_include = []\n",
    "for category in output_table[\"category\"].unique():\n",
    "    groups = SELECTED_GROUPS.get(category)\n",
    "    if groups is None:\n",
    "        groups = list(set(output_table.query(f'category == \"{category}\"')[\"group\"]))\n",
    "\n",
    "    groups_to_include.extend(groups)\n",
    "\n",
    "print(\"Groups selected:\", \", \".join(groups_to_include))\n",
    "output_table = output_table.query(\"group == @groups_to_include\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence Interval Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_table = (\n",
    "    output_table[[*INDEPENDENT_VARIABLES, \"group\", \"accuracy\"]]\n",
    "    .groupby([*INDEPENDENT_VARIABLES, \"group\"])\n",
    "    .agg([\"mean\"])\n",
    ")\n",
    "\n",
    "for metric, gap_metric, hypothesis in zip(metrics, gap_metrics, metrics_hypotheses):\n",
    "    # per Metric, per Group, per Run\n",
    "    group_by = [*INDEPENDENT_VARIABLES]\n",
    "    grouped = output_table[[*group_by, metric]].groupby(group_by)\n",
    "    median = grouped.transform(\"median\")\n",
    "    normalized_values = output_table[metric] - median[metric]\n",
    "    output_table[gap_metric] = normalized_values\n",
    "\n",
    "    # See https://stackoverflow.com/a/53522680\n",
    "    # per group\n",
    "    stats = (\n",
    "        output_table[[*INDEPENDENT_VARIABLES, \"group\", gap_metric]]\n",
    "        .groupby([*INDEPENDENT_VARIABLES, \"group\"])\n",
    "        .agg([\"mean\", \"count\", \"std\"])\n",
    "    )\n",
    "\n",
    "    mean_values = []\n",
    "    ci_width = []\n",
    "    lower_values = []\n",
    "    upper_values = []\n",
    "    hypothesis_selected = []\n",
    "\n",
    "    for index in stats.index:\n",
    "        mean, n, stdev = stats.loc[index]  # type: ignore\n",
    "        lower = mean - 1.96 * stdev / sqrt(n)\n",
    "        upper = mean + 1.96 * stdev / sqrt(n)\n",
    "\n",
    "        mean_values.append(mean)\n",
    "        ci_width.append(1.96 * stdev / sqrt(n))\n",
    "        lower_values.append(lower)\n",
    "        upper_values.append(upper)\n",
    "\n",
    "        if lower > 0:\n",
    "            hypothesis_selected.append(1)\n",
    "        elif upper < 0:\n",
    "            hypothesis_selected.append(-1)\n",
    "        else:\n",
    "            hypothesis_selected.append(0)\n",
    "\n",
    "    stats_table[gap_metric] = mean_values\n",
    "    stats_table[gap_metric + \"_width\"] = ci_width\n",
    "    stats_table[gap_metric + \"_lower\"] = lower_values\n",
    "    stats_table[gap_metric + \"_upper\"] = upper_values\n",
    "    stats_table[hypothesis] = hypothesis_selected\n",
    "\n",
    "stats_table = stats_table.reset_index()\n",
    "# stats_table.query(\"category == \\\"groupd_race\\\"\")\n",
    "stats_table[stats_table[\"category\"] == \"grouped_race\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_table_aggregated = (\n",
    "    output_table[[*INDEPENDENT_VARIABLES, \"group\", \"accuracy\"]]\n",
    "    .groupby([*INDEPENDENT_VARIABLES, \"group\"])\n",
    "    .agg([\"mean\"])\n",
    ")\n",
    "\n",
    "for metric in [\"accuracy\", \"positive_TPR\"]:\n",
    "    # See https://stackoverflow.com/a/53522680\n",
    "    # per group\n",
    "    stats = (\n",
    "        output_table[[*INDEPENDENT_VARIABLES, \"group\", metric]]\n",
    "        .groupby([*INDEPENDENT_VARIABLES, \"group\"])\n",
    "        .agg([\"mean\", \"count\", \"std\"])\n",
    "    )\n",
    "\n",
    "    mean_values = []\n",
    "    ci_width = []\n",
    "    lower_values = []\n",
    "    upper_values = []\n",
    "    hypothesis_selected = []\n",
    "\n",
    "    for index in stats.index:\n",
    "        mean, n, stdev = stats.loc[index]\n",
    "        lower = mean - 1.96 * stdev / sqrt(n)\n",
    "        upper = mean + 1.96 * stdev / sqrt(n)\n",
    "\n",
    "        mean_values.append(mean)\n",
    "        ci_width.append(1.96 * stdev / sqrt(n))\n",
    "        lower_values.append(lower)\n",
    "        upper_values.append(upper)\n",
    "\n",
    "        if lower > 0:\n",
    "            hypothesis_selected.append(1)\n",
    "        elif upper < 0:\n",
    "            hypothesis_selected.append(-1)\n",
    "        else:\n",
    "            hypothesis_selected.append(0)\n",
    "\n",
    "    stats_table_aggregated[metric] = mean_values\n",
    "    stats_table_aggregated[metric + \"_width\"] = ci_width\n",
    "    stats_table_aggregated[metric + \"_lower\"] = lower_values\n",
    "    stats_table_aggregated[metric + \"_upper\"] = upper_values\n",
    "    stats_table_aggregated[hypothesis] = hypothesis_selected\n",
    "\n",
    "stats_table_aggregated = stats_table_aggregated.reset_index()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare between Groups of each category \n",
    "E.g. all groups in the \"age\" category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = stats_table[\"dataset\"].unique()\n",
    "categories = stats_table[\"category\"].unique()\n",
    "\n",
    "for model in [*(stats_table[\"model\"].unique()), \"all\"]:\n",
    "    for dataset in tqdm(datasets):\n",
    "        group_output_path = os.path.join(output_folder, model, dataset)\n",
    "        os.makedirs(group_output_path, exist_ok=True)\n",
    "\n",
    "        for category in tqdm(categories, leave=False):\n",
    "            for metric, gap_metric, metric_hypothesis in list(zip(metrics, gap_metrics, metrics_hypotheses)):\n",
    "                if model == \"all\":\n",
    "                    filtered_table = stats_table[\n",
    "                        (stats_table[\"category\"] == category) & (stats_table[\"dataset\"] == dataset)\n",
    "                    ]\n",
    "                else:\n",
    "                    filtered_table = stats_table[\n",
    "                        (stats_table[\"category\"] == category)\n",
    "                        & (stats_table[\"model\"] == model)\n",
    "                        & (stats_table[\"dataset\"] == dataset)\n",
    "                    ]\n",
    "\n",
    "                # Removing the grouped from grouped_{category_name}\n",
    "                category_name = category.split(\"_\")[-1]\n",
    "                filename_pdf = f\"{metric}_{category_name}_{dataset}_{model}.pdf\"\n",
    "                filename_png = f\"{metric}_{category_name}_{dataset}_{model}.png\"\n",
    "                title = (\n",
    "                    f\"{format_metric_name(gap_metric)} for {category_name.title()}\"\n",
    "                    + f\"- {format_metric_name(dataset)}\"\n",
    "                )\n",
    "\n",
    "                fig = px.scatter(\n",
    "                    filtered_table,\n",
    "                    x=\"group\",\n",
    "                    facet_col=\"group\",\n",
    "                    y=gap_metric,\n",
    "                    color=\"model\",\n",
    "                    error_y=gap_metric + \"_width\",\n",
    "                    labels={\n",
    "                        \"group\": \"Protected Group\",\n",
    "                        gap_metric: format_metric_name(gap_metric),\n",
    "                        \"model\": \"LM\",\n",
    "                    },\n",
    "                    title=title,\n",
    "                    height=600,\n",
    "                )\n",
    "\n",
    "                fig.update_xaxes(matches=None, tickangle=90)\n",
    "                fig.for_each_xaxis(lambda x: x.update(title=\"\"))\n",
    "                fig.for_each_annotation(lambda a: a.update(text=\"\"))\n",
    "\n",
    "                fig = fig.update_layout(\n",
    "                    scattermode=\"group\",\n",
    "                    font_color=\"black\",\n",
    "                    font_size=18,\n",
    "                    title_x=0.5,\n",
    "                )\n",
    "                fig.write_image(os.path.join(group_output_path, filename_pdf))\n",
    "                fig.write_image(os.path.join(group_output_path, filename_png))\n",
    "\n",
    "print(\"Find plots in plots/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare between models on the same protected group \n",
    "E.g. \"young\" people of the \"age\" category.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in [*(stats_table[\"model\"].unique()), \"all\"]:\n",
    "    for category in tqdm(categories):\n",
    "        if model == \"all\":\n",
    "            filtered_table = stats_table[(stats_table[\"category\"] == category) & (stats_table[\"dataset\"] != \"\")]\n",
    "        else:\n",
    "            filtered_table = stats_table[\n",
    "                (stats_table[\"category\"] == category)\n",
    "                & (stats_table[\"model\"] == model)\n",
    "                & (stats_table[\"dataset\"] != \"\")\n",
    "            ]\n",
    "        groups = filtered_table[\"group\"].unique()\n",
    "\n",
    "        for group in tqdm(groups, leave=False):\n",
    "            group_output_path = os.path.join(output_folder, model, \"by_group\", category, group)\n",
    "            os.makedirs(group_output_path, exist_ok=True)\n",
    "\n",
    "            group_filtered_table = filtered_table[filtered_table[\"group\"] == group]\n",
    "            for metric, gap_metric, metric_hypothesis in tqdm(\n",
    "                list(zip(metrics, gap_metrics, metrics_hypotheses)), leave=False\n",
    "            ):\n",
    "\n",
    "                # Removing the word \"grouped\" from grouped_{category_name}\n",
    "                category_name = category.split(\"_\")[-1]\n",
    "                filename_pdf = f\"{metric}_{category_name}_{group}_{model}.pdf\"\n",
    "                filename_png = f\"{metric}_{category_name}_{group}_{model}.png\"\n",
    "                title = f\"{format_metric_name(gap_metric)} for {group.title()} \" + f\"- {format_metric_name(dataset)}\"\n",
    "\n",
    "                fig = px.scatter(\n",
    "                    group_filtered_table,\n",
    "                    x=\"dataset\",\n",
    "                    facet_col=\"dataset\",\n",
    "                    y=gap_metric,\n",
    "                    color=\"model\",\n",
    "                    error_y=gap_metric + \"_width\",\n",
    "                    labels={\n",
    "                        \"group\": \"Prompt-Tuning Dataset\",\n",
    "                        gap_metric: format_metric_name(gap_metric),\n",
    "                        \"model\": \"LM\",\n",
    "                        **{dataset: format_metric_name(dataset) for dataset in filtered_table[\"dataset\"].unique()},\n",
    "                    },\n",
    "                    title=title,\n",
    "                    height=600,\n",
    "                )\n",
    "\n",
    "                fig.update_xaxes(matches=None, tickangle=90)\n",
    "                fig.for_each_xaxis(lambda x: x.update(title=\"\"))\n",
    "                fig.for_each_annotation(lambda a: a.update(text=\"\"))\n",
    "\n",
    "                fig = fig.update_layout(\n",
    "                    scattermode=\"group\",\n",
    "                    font_color=\"black\",\n",
    "                    font_size=18,\n",
    "                    title_x=0.5,\n",
    "                )\n",
    "                fig.write_image(os.path.join(group_output_path, filename_pdf))\n",
    "                fig.write_image(os.path.join(group_output_path, filename_png))\n",
    "\n",
    "print(\"Find plots in plots/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "25f6c606dd86a3cae59df91b75f7a72b0b2bf1388cda4c8c160c9bd2d7ceb352"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
