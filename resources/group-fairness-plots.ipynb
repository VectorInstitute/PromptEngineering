{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias Visualization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "Note that we use the scikit-learn confusion matrix utility to help calculate some of the fairness metrics. We generate the visualizations using plotly Express."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Iterable\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix as ConfusionMatrix\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "\n",
    "from math import sqrt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Constants"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will visualize how each of the following variable affects the fairness towards a particular protected group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEPENDENT_VARIABLES = [\"model\", \"dataset\", \"num_params\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_metric_name(metric_name):\n",
    "    \"\"\"\n",
    "    Prettify diagram texts by replacing\n",
    "    snake case with regular title case.\n",
    "    \"\"\"\n",
    "    output_words = []\n",
    "    for word in metric_name.split(\"_\"):\n",
    "        output_words.append(word.title())\n",
    "\n",
    "    return \" \".join(output_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Predictions Ready"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Prediction Files\n",
    "Load all prediction `tsv` files from a given folder into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_folder: str = \"predictions/\"\n",
    "prediction_files: List[str] = os.listdir(prediction_folder)\n",
    "prediction_tsv_paths: List[str] = [\n",
    "    os.path.join(prediction_folder, prediction_file)\n",
    "    for prediction_file in prediction_files\n",
    "    if prediction_file.endswith(\".tsv\")\n",
    "]\n",
    "\n",
    "output_folder: str = \"plots\"\n",
    "output_table = pd.DataFrame()\n",
    "\n",
    "for prediction_tsv_path in tqdm(prediction_tsv_paths, ncols=80):\n",
    "    prediction_tsv_filename = os.path.basename(prediction_tsv_path)\n",
    "    dataframe = pd.read_csv(prediction_tsv_path, delimiter=\"\\t\")\n",
    "    del dataframe[\"text\"]\n",
    "    del dataframe[\"eval_accuracy\"]\n",
    "\n",
    "    if output_table is None:\n",
    "        output_table = dataframe\n",
    "    else:\n",
    "        assert isinstance(output_table, pd.DataFrame)\n",
    "        output_table = pd.concat([output_table, dataframe])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Statistics\n",
    "Since there can be multiple examples for each protected group, we apply the `groupby` method to compute aggregated statistics across all the examples of each protected group.\n",
    "\n",
    "Refer to the following function for details on how we implemented the metrics."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for aggregating statistics\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you will find a function that takes in a dataframe. This dataframe is will be automatically generated using the `pd.DataFrame.groupby`. The content of this dataframe is a subset of one particular prediction tsv for a particular protected group (e.g., the \"young\" group for the protected class \"age\"). \n",
    "\n",
    "The function will return a `pd.Series` (like a dictionary) mapping the following fairness metrics to their floating point values:\n",
    "- Accuracy\n",
    "- TPR(Positive)\n",
    "- TPR(Neutral)\n",
    "- TPR(Negative)\n",
    "- FPR(Positive)\n",
    "- FPR(Neutral)\n",
    "- FPR(Negative)\n",
    "\n",
    "Also refer to https://stackoverflow.com/a/50671617 for details on how the TPR and FPR values are computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(dataframe: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Input: dataframe representing all predictions for a particular\n",
    "    protected group in a particular run.\n",
    "    \"\"\"\n",
    "    output: Dict[str, float] = {}\n",
    "\n",
    "    num_examples = len(dataframe)\n",
    "    num_correct = np.sum(dataframe[\"y_pred\"] == dataframe[\"y_true\"])\n",
    "\n",
    "    output[\"accuracy\"] = num_correct / num_examples\n",
    "\n",
    "    # See https://stackoverflow.com/a/50671617\n",
    "    confusion_matrix = ConfusionMatrix(dataframe[\"y_true\"], dataframe[\"y_pred\"])\n",
    "\n",
    "    FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "    FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "    TP = np.diag(confusion_matrix)\n",
    "    TN = confusion_matrix.sum() - (FP + FN + TP)\n",
    "\n",
    "    FPR = FP / (FP + TN)\n",
    "    FNR = FN / (TP + FN)\n",
    "    TPR = 1 - FNR\n",
    "\n",
    "    for label_index, label in enumerate([\"negative\", \"neutral\", \"positive\"]):\n",
    "        output[label + \"_FPR\"] = FPR[label_index]\n",
    "        output[label + \"_TPR\"] = TPR[label_index]\n",
    "\n",
    "    return pd.Series(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Per-Group Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_table\n",
    "analyzed_dataframe = output_table.groupby([*INDEPENDENT_VARIABLES, \"run_id\", \"category\", \"group\"]).apply(get_stats)\n",
    "analyzed_dataframe.query('category == \"grouped_race\"')[[\"positive_TPR\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_table = analyzed_dataframe.reset_index()\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "output_table.to_csv(os.path.join(output_folder, \"aggregated.csv\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Statistics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin, note that of the protected classes include a large number of groups. You may filter on the protected groups to include in each category. If a category isn't in this dictionary, the visualizations will include all the groups under that category by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTED_GROUPS = {\n",
    "    \"grouped_religion\": [\"christianity\", \"islam\", \"judaism\"],\n",
    "}\n",
    "\n",
    "groups_to_include = []\n",
    "for category in output_table[\"category\"].unique():\n",
    "    groups = SELECTED_GROUPS.get(category)\n",
    "    if groups is None:\n",
    "        groups = list(set(output_table.query(f'category == \"{category}\"')[\"group\"]))\n",
    "\n",
    "    groups_to_include.extend(groups)\n",
    "\n",
    "print(\"Groups selected:\", \", \".join(groups_to_include))\n",
    "output_table = output_table.query(\"group == @groups_to_include\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Gap\": How each group deviates from the category mean\n",
    "Below, we will calculate how far each group deviates from the mean of that category. For example, how far does the accuracy on examples mentioning the \"young\" group deviate from the mean of all examples of the \"age\" category?\n",
    "\n",
    "If there are more than one runs for each model, we would calculate gap metrics for each run separately. \n",
    "\n",
    "We use a for loop to add the following metrics of interest to our list:\n",
    "- Accuracy\n",
    "- False Positive Rates\n",
    "    - FPR(\"Negative\")\n",
    "    - FPR(\"Neutral\")\n",
    "    - FPR(\"Positive\")\n",
    "- True Positive Rates\n",
    "    - TPR(\"Negative\")\n",
    "    - TPR(\"Neutral\")\n",
    "    - TPR(\"Positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"accuracy\"]\n",
    "for label in [\"negative\", \"neutral\", \"positive\"]:\n",
    "    metrics.append(label + \"_FPR\")\n",
    "    metrics.append(label + \"_TPR\")\n",
    "\n",
    "gap_metrics = []\n",
    "metrics_hypotheses = []\n",
    "for metric in metrics:\n",
    "    gap_metrics.append(f\"{metric}_gap\")\n",
    "\n",
    "print(gap_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate gap for each metric and\n",
    "# save the results in a column named f\"{metrics}_gap\".\n",
    "for metric, gap_metric in zip(metrics, gap_metrics):\n",
    "    # Calculate mean for each metric, group, and run/seed.\n",
    "    group_by = [*INDEPENDENT_VARIABLES, \"category\", \"run_id\"]\n",
    "    grouped = output_table[[*INDEPENDENT_VARIABLES, \"category\", \"run_id\", metric]].groupby(group_by)\n",
    "    mean = grouped.transform(\"mean\")\n",
    "\n",
    "    # Broadcast the mean values across the table\n",
    "    # to find the gap for each entry.\n",
    "    normalized_values = output_table[metric] - mean[metric]\n",
    "    output_table[gap_metric] = normalized_values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence Interval Calculations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous step, we've found the gaps for each (metric, category, group, model, run/seed). If we have more than one runs/seeds for each model, we can aggregate the results to find a confidence interval for each gap variable.\n",
    "\n",
    "To do so, we will aggregate the previous table (metric, category, group, model, run/seed) along the run/seed axis. The result will be a table with indices (metric, category, group, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a (metric, group) placeholder table for storing the output values.\n",
    "# Unlike in the previous step, \"run_id\" isn't part of groupby,\n",
    "# and this axis will be squeezed in the result.\n",
    "stats_table = (\n",
    "    output_table[[*INDEPENDENT_VARIABLES, \"category\", \"group\", \"accuracy\"]]\n",
    "    .groupby([*INDEPENDENT_VARIABLES, \"category\", \"group\"])\n",
    "    .agg([\"mean\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric, gap_metric in zip(metrics, gap_metrics):\n",
    "    group_by = [*INDEPENDENT_VARIABLES, \"category\"]\n",
    "    grouped = output_table[[*INDEPENDENT_VARIABLES, \"category\", metric]].groupby(group_by)\n",
    "    median = grouped.transform(\"median\")\n",
    "    normalized_values = output_table[metric] - median[metric]\n",
    "    output_table[gap_metric] = normalized_values\n",
    "\n",
    "    # See https://stackoverflow.com/a/53522680\n",
    "    # Calculate mean and stdev for each (metric, category, group, model).\n",
    "    stats = (\n",
    "        output_table[[*INDEPENDENT_VARIABLES, \"category\", \"group\", gap_metric]]\n",
    "        .groupby([*INDEPENDENT_VARIABLES, \"category\", \"group\"])\n",
    "        .agg([\"mean\", \"count\", \"std\"])\n",
    "    )\n",
    "\n",
    "    mean_values = []\n",
    "    ci_width = []\n",
    "    lower_values = []\n",
    "    upper_values = []\n",
    "    hypothesis_selected = []\n",
    "\n",
    "    # Calculate 95% Confidence interval for each\n",
    "    # (metric, category, group, model).\n",
    "    for index in stats.index:\n",
    "        mean, n, stdev = stats.loc[index]  # type: ignore\n",
    "        lower = mean - 1.96 * stdev / sqrt(n)\n",
    "        upper = mean + 1.96 * stdev / sqrt(n)\n",
    "\n",
    "        mean_values.append(mean)\n",
    "        ci_width.append(1.96 * stdev / sqrt(n))\n",
    "        lower_values.append(lower)\n",
    "        upper_values.append(upper)\n",
    "\n",
    "        if lower > 0:\n",
    "            hypothesis_selected.append(1)\n",
    "        elif upper < 0:\n",
    "            hypothesis_selected.append(-1)\n",
    "        else:\n",
    "            hypothesis_selected.append(0)\n",
    "\n",
    "    stats_table[gap_metric] = mean_values\n",
    "    stats_table[gap_metric + \"_width\"] = ci_width\n",
    "    stats_table[gap_metric + \"_lower\"] = lower_values\n",
    "    stats_table[gap_metric + \"_upper\"] = upper_values\n",
    "\n",
    "stats_table = stats_table.reset_index()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare between Groups of each category \n",
    "E.g. all groups in the \"age\" category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = stats_table[\"dataset\"].unique()\n",
    "categories = stats_table[\"category\"].unique()\n",
    "\n",
    "for model in [*(stats_table[\"model\"].unique()), \"all\"]:\n",
    "    for dataset in tqdm(datasets):\n",
    "        group_output_path = os.path.join(output_folder, model, dataset)\n",
    "        os.makedirs(group_output_path, exist_ok=True)\n",
    "\n",
    "        for category in tqdm(categories, leave=False):\n",
    "            for metric, gap_metric, metric_hypothesis in list(zip(metrics, gap_metrics, metrics_hypotheses)):\n",
    "                if model == \"all\":\n",
    "                    filtered_table = stats_table[\n",
    "                        (stats_table[\"category\"] == category) & (stats_table[\"dataset\"] == dataset)\n",
    "                    ]\n",
    "                else:\n",
    "                    filtered_table = stats_table[\n",
    "                        (stats_table[\"category\"] == category)\n",
    "                        & (stats_table[\"model\"] == model)\n",
    "                        & (stats_table[\"dataset\"] == dataset)\n",
    "                    ]\n",
    "\n",
    "                if len(filtered_table) == 0:\n",
    "                    continue\n",
    "\n",
    "                # Removing the grouped from grouped_{category_name}\n",
    "                category_name = category.split(\"_\")[-1]\n",
    "                filename_pdf = f\"{metric}_{category_name}_{dataset}_{model}.pdf\"\n",
    "                filename_png = f\"{metric}_{category_name}_{dataset}_{model}.png\"\n",
    "                title = (\n",
    "                    f\"{format_metric_name(gap_metric)} for {category_name.title()}\"\n",
    "                    + f\"- {format_metric_name(dataset)}\"\n",
    "                )\n",
    "\n",
    "                fig = px.scatter(\n",
    "                    filtered_table,\n",
    "                    x=\"group\",\n",
    "                    facet_col=\"group\",\n",
    "                    y=gap_metric,\n",
    "                    color=\"model\",\n",
    "                    error_y=gap_metric + \"_width\",\n",
    "                    labels={\n",
    "                        \"group\": \"Protected Group\",\n",
    "                        gap_metric: format_metric_name(gap_metric),\n",
    "                        \"model\": \"LM\",\n",
    "                    },\n",
    "                    title=title,\n",
    "                    height=600,\n",
    "                )\n",
    "\n",
    "                fig.update_xaxes(matches=None, tickangle=90)\n",
    "                fig.for_each_xaxis(lambda x: x.update(title=\"\"))\n",
    "                fig.for_each_annotation(lambda a: a.update(text=\"\"))\n",
    "\n",
    "                fig = fig.update_layout(\n",
    "                    scattermode=\"group\",\n",
    "                    font_color=\"black\",\n",
    "                    font_size=18,\n",
    "                    title_x=0.5,\n",
    "                )\n",
    "                fig.write_image(os.path.join(group_output_path, filename_pdf))\n",
    "                fig.write_image(os.path.join(group_output_path, filename_png))\n",
    "\n",
    "print(\"Find plots in plots/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare between models on the same protected group \n",
    "E.g. \"young\" people of the \"age\" category.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in tqdm([*(stats_table[\"model\"].unique()), \"all\"]):\n",
    "    for category in tqdm(categories, leave=False):\n",
    "        if model == \"all\":\n",
    "            filtered_table = stats_table[(stats_table[\"category\"] == category) & (stats_table[\"dataset\"] != \"\")]\n",
    "        else:\n",
    "            filtered_table = stats_table[\n",
    "                (stats_table[\"category\"] == category)\n",
    "                & (stats_table[\"model\"] == model)\n",
    "                & (stats_table[\"dataset\"] != \"\")\n",
    "            ]\n",
    "\n",
    "        groups = filtered_table[\"group\"].unique()\n",
    "\n",
    "        for group in tqdm(groups, leave=False):\n",
    "            group_output_path = os.path.join(output_folder, model, \"by_group\", category, group)\n",
    "            os.makedirs(group_output_path, exist_ok=True)\n",
    "\n",
    "            group_filtered_table = filtered_table[filtered_table[\"group\"] == group]\n",
    "            if len(group_filtered_table) == 0:\n",
    "                continue\n",
    "\n",
    "            for metric, gap_metric, metric_hypothesis in list(zip(metrics, gap_metrics, metrics_hypotheses)):\n",
    "                # Removing the grouped from grouped_{category_name}\n",
    "                category_name = category.split(\"_\")[-1]\n",
    "                filename_pdf = f\"{metric}_{category_name}_{group}_{model}.pdf\"\n",
    "                filename_png = f\"{metric}_{category_name}_{group}_{model}.png\"\n",
    "                title = f\"{format_metric_name(gap_metric)} for {group.title()} \" + f\"- {format_metric_name(dataset)}\"\n",
    "\n",
    "                fig = px.scatter(\n",
    "                    group_filtered_table,\n",
    "                    x=\"dataset\",\n",
    "                    facet_col=\"dataset\",\n",
    "                    y=gap_metric,\n",
    "                    color=\"model\",\n",
    "                    error_y=gap_metric + \"_width\",\n",
    "                    labels={\n",
    "                        \"group\": \"Prompt-Tuning Dataset\",\n",
    "                        gap_metric: format_metric_name(gap_metric),\n",
    "                        \"model\": \"LM\",\n",
    "                        **{dataset: format_metric_name(dataset) for dataset in filtered_table[\"dataset\"].unique()},\n",
    "                    },\n",
    "                    title=title,\n",
    "                    height=600,\n",
    "                )\n",
    "\n",
    "                fig.update_xaxes(matches=None, tickangle=90)\n",
    "                fig.for_each_xaxis(lambda x: x.update(title=\"\"))\n",
    "                fig.for_each_annotation(lambda a: a.update(text=\"\"))\n",
    "\n",
    "                fig = fig.update_layout(\n",
    "                    scattermode=\"group\",\n",
    "                    font_color=\"black\",\n",
    "                    font_size=18,\n",
    "                    title_x=0.5,\n",
    "                )\n",
    "                fig.write_image(os.path.join(group_output_path, filename_pdf))\n",
    "                fig.write_image(os.path.join(group_output_path, filename_png))\n",
    "\n",
    "print(\"Find plots in plots/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "25f6c606dd86a3cae59df91b75f7a72b0b2bf1388cda4c8c160c9bd2d7ceb352"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
