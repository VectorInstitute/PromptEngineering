{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias Visualization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "Note that we use the scikit-learn confusion matrix utility to help calculate some of the fairness metrics. We generate the visualizations using plotly Express."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install kaleido numpy pandas plotly scikit-learn tqdm ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from math import sqrt\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from IPython.display import clear_output, display\n",
    "from sklearn.metrics import confusion_matrix as ConfusionMatrix\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Constants"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will visualize how each of the following variable affects the fairness towards a particular protected group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEPENDENT_VARIABLES = [\"model\", \"dataset\", \"num_params\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_metric_name(metric_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Prettify diagram texts by replacing\n",
    "    snake case with regular title case.\n",
    "    \"\"\"\n",
    "    output_words = []\n",
    "    for word in metric_name.split(\"_\"):\n",
    "        output_words.append(word.title())\n",
    "\n",
    "    return \" \".join(output_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Predictions Ready"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Prediction Files\n",
    "Load all prediction `tsv` files from a given folder into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_folder: str = \"resources/predictions/\"\n",
    "prediction_files: List[str] = os.listdir(prediction_folder)\n",
    "prediction_tsv_paths: List[str] = [\n",
    "    os.path.join(prediction_folder, prediction_file)\n",
    "    for prediction_file in prediction_files\n",
    "    if prediction_file.endswith(\".tsv\")\n",
    "]\n",
    "\n",
    "output_folder: str = \"plots\"\n",
    "output_table = pd.DataFrame()\n",
    "\n",
    "for prediction_tsv_path in tqdm(prediction_tsv_paths, ncols=80):\n",
    "    prediction_tsv_filename = os.path.basename(prediction_tsv_path)\n",
    "    dataframe = pd.read_csv(prediction_tsv_path, delimiter=\"\\t\")\n",
    "    del dataframe[\"text\"]\n",
    "    del dataframe[\"eval_accuracy\"]\n",
    "\n",
    "    if output_table is None:\n",
    "        output_table = dataframe\n",
    "    else:\n",
    "        assert isinstance(output_table, pd.DataFrame)\n",
    "        output_table = pd.concat([output_table, dataframe])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Statistics\n",
    "Since there can be multiple examples for each protected group, we apply the `groupby` method to compute aggregated statistics across all the examples of each protected group.\n",
    "\n",
    "Refer to the following function for details on how we implemented the metrics."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for aggregating statistics\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you will find a function that takes in a dataframe. This dataframe is will be automatically generated using the `pd.DataFrame.groupby`. The content of this dataframe is a subset of one particular prediction tsv for a particular protected group (e.g., the \"young\" group for the protected class \"age\"). \n",
    "\n",
    "The function will return a `pd.Series` (like a dictionary) mapping the following fairness metrics to their floating point values:\n",
    "- Accuracy\n",
    "- TPR(Positive)\n",
    "- TPR(Neutral)\n",
    "- TPR(Negative)\n",
    "- FPR(Positive)\n",
    "- FPR(Neutral)\n",
    "- FPR(Negative)\n",
    "\n",
    "Also refer to https://stackoverflow.com/a/50671617 for details on how the TPR and FPR values are computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(dataframe: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Input: dataframe representing all predictions for a particular\n",
    "    protected group in a particular run.\n",
    "    \"\"\"\n",
    "    output: Dict[str, float] = {}\n",
    "\n",
    "    num_examples = len(dataframe)\n",
    "    num_correct = np.sum(dataframe[\"y_pred\"] == dataframe[\"y_true\"])\n",
    "\n",
    "    output[\"accuracy\"] = num_correct / num_examples\n",
    "\n",
    "    # See https://stackoverflow.com/a/50671617\n",
    "    confusion_matrix = ConfusionMatrix(dataframe[\"y_true\"], dataframe[\"y_pred\"])\n",
    "\n",
    "    FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "    FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "    TP = np.diag(confusion_matrix)\n",
    "    TN = confusion_matrix.sum() - (FP + FN + TP)\n",
    "\n",
    "    FPR = FP / (FP + TN)\n",
    "    FNR = FN / (TP + FN)\n",
    "    TPR = 1 - FNR\n",
    "\n",
    "    for label_index, label in enumerate([\"negative\", \"neutral\", \"positive\"]):\n",
    "        output[label + \"_FPR\"] = FPR[label_index]\n",
    "        output[label + \"_TPR\"] = TPR[label_index]\n",
    "\n",
    "    return pd.Series(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Per-Group Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_table\n",
    "analyzed_dataframe = output_table.groupby([*INDEPENDENT_VARIABLES, \"run_id\", \"category\", \"group\"]).apply(get_stats)\n",
    "analyzed_dataframe.query('category == \"grouped_race\"')[[\"positive_TPR\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_table = analyzed_dataframe.reset_index()\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "output_table.to_csv(os.path.join(output_folder, \"aggregated.csv\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Statistics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin, note that of the protected classes include a large number of groups. You may filter on the protected groups to include in each category. If a category isn't in this dictionary, the visualizations will include all the groups under that category by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTED_GROUPS = {\n",
    "    \"grouped_religion\": [\"christianity\", \"islam\", \"judaism\"],\n",
    "}\n",
    "\n",
    "groups_to_include = []\n",
    "for category in output_table[\"category\"].unique():\n",
    "    groups = SELECTED_GROUPS.get(category)\n",
    "    if groups is None:\n",
    "        groups = list(set(output_table.query(f'category == \"{category}\"')[\"group\"]))\n",
    "\n",
    "    groups_to_include.extend(groups)\n",
    "\n",
    "print(\"Groups selected:\", \", \".join(groups_to_include))\n",
    "output_table = output_table.query(\"group == @groups_to_include\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Gap\": How each group deviates from the category mean\n",
    "Below, we will calculate how far each group deviates from the mean of that category. For example, how far does the accuracy on examples mentioning the \"young\" group deviate from the mean of all examples of the \"age\" category?\n",
    "\n",
    "If there are more than one runs for each model, we would calculate gap metrics for each run separately. \n",
    "\n",
    "We use a for loop to add the following metrics of interest to our list:\n",
    "- Accuracy\n",
    "- False Positive Rates\n",
    "    - FPR(\"Negative\")\n",
    "    - FPR(\"Neutral\")\n",
    "    - FPR(\"Positive\")\n",
    "- True Positive Rates\n",
    "    - TPR(\"Negative\")\n",
    "    - TPR(\"Neutral\")\n",
    "    - TPR(\"Positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"accuracy\"]\n",
    "for label in [\"negative\", \"neutral\", \"positive\"]:\n",
    "    metrics.append(label + \"_FPR\")\n",
    "    metrics.append(label + \"_TPR\")\n",
    "\n",
    "gap_metrics = []\n",
    "for metric in metrics:\n",
    "    gap_metrics.append(f\"{metric}_gap\")\n",
    "\n",
    "print(gap_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate gap for each metric and\n",
    "# save the results in a column named f\"{metrics}_gap\".\n",
    "for metric, gap_metric in zip(metrics, gap_metrics):\n",
    "    # Calculate mean for each metric, group, and run/seed.\n",
    "    group_by = [*INDEPENDENT_VARIABLES, \"category\", \"run_id\"]\n",
    "    grouped = output_table[[*INDEPENDENT_VARIABLES, \"category\", \"run_id\", metric]].groupby(group_by)\n",
    "    mean = grouped.transform(\"mean\")\n",
    "\n",
    "    # Broadcast the mean values across the table\n",
    "    # to find the gap for each entry.\n",
    "    normalized_values = output_table[metric] - mean[metric]\n",
    "    output_table[gap_metric] = normalized_values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence Interval Calculations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous step, we've found the gaps for each (metric, category, group, model, run/seed). If we have more than one runs/seeds for each model, we can aggregate the results to find a confidence interval for each gap variable.\n",
    "\n",
    "To do so, we will aggregate the previous table (metric, category, group, model, run/seed) along the run/seed axis. The result will be a table with indices (metric, category, group, model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a (metric, group) placeholder table for storing the output values.\n",
    "# Unlike in the previous step, \"run_id\" isn't part of groupby,\n",
    "# and this axis will be squeezed in the result.\n",
    "stats_table = (\n",
    "    output_table[[*INDEPENDENT_VARIABLES, \"category\", \"group\", \"accuracy\"]]\n",
    "    .groupby([*INDEPENDENT_VARIABLES, \"category\", \"group\"])\n",
    "    .agg([\"mean\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric, gap_metric in zip(metrics, gap_metrics):\n",
    "    group_by = [*INDEPENDENT_VARIABLES, \"category\"]\n",
    "    grouped = output_table[[*INDEPENDENT_VARIABLES, \"category\", metric]].groupby(group_by)\n",
    "    median = grouped.transform(\"median\")\n",
    "    normalized_values = output_table[metric] - median[metric]\n",
    "    output_table[gap_metric] = normalized_values\n",
    "\n",
    "    # See https://stackoverflow.com/a/53522680\n",
    "    # Calculate mean and stdev for each (metric, category, group, model).\n",
    "    stats = (\n",
    "        output_table[[*INDEPENDENT_VARIABLES, \"category\", \"group\", gap_metric]]\n",
    "        .groupby([*INDEPENDENT_VARIABLES, \"category\", \"group\"])\n",
    "        .agg([\"mean\", \"count\", \"std\"])\n",
    "    )\n",
    "\n",
    "    mean_values = []\n",
    "    ci_width = []\n",
    "    lower_values = []\n",
    "    upper_values = []\n",
    "    hypothesis_selected = []\n",
    "\n",
    "    # Calculate 95% Confidence interval for each\n",
    "    # (metric, category, group, model).\n",
    "    for index in stats.index:\n",
    "        mean, n, stdev = stats.loc[index]  # type: ignore\n",
    "        lower = mean - 1.96 * stdev / sqrt(n)\n",
    "        upper = mean + 1.96 * stdev / sqrt(n)\n",
    "\n",
    "        mean_values.append(mean)\n",
    "        ci_width.append(1.96 * stdev / sqrt(n))\n",
    "        lower_values.append(lower)\n",
    "        upper_values.append(upper)\n",
    "\n",
    "        if lower > 0:\n",
    "            hypothesis_selected.append(1)\n",
    "        elif upper < 0:\n",
    "            hypothesis_selected.append(-1)\n",
    "        else:\n",
    "            hypothesis_selected.append(0)\n",
    "\n",
    "    stats_table[gap_metric] = mean_values\n",
    "    stats_table[gap_metric + \"_width\"] = ci_width\n",
    "    stats_table[gap_metric + \"_lower\"] = lower_values\n",
    "    stats_table[gap_metric + \"_upper\"] = upper_values\n",
    "\n",
    "stats_table = stats_table.reset_index()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare between Groups of each category \n",
    "E.g. all groups in the \"age\" category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = stats_table[\"dataset\"].unique()\n",
    "models = [\"(all)\", *(stats_table[\"model\"].unique())]\n",
    "categories = stats_table[\"category\"].unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interactive Visualization Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_selector = widgets.Dropdown(options=datasets, value=datasets[0], description=\"Dataset:\")\n",
    "model_selector = widgets.Dropdown(options=models, value=models[0], description=\"Model:\")\n",
    "category_selector = widgets.Dropdown(options=categories, value=categories[0], description=\"Category:\")\n",
    "metric_selector = widgets.Dropdown(options=metrics, value=metrics[0], description=\"Metric:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_fn(button: Any) -> None:\n",
    "    \"\"\"\n",
    "    Function to be called when the button is pressed.\n",
    "    This function clears previous output and any widget\n",
    "    that has already been rendered. To add these widgets\n",
    "    back (including the button) the \"button\" parameter\n",
    "    needs to be supplied from the button action.\n",
    "    \"\"\"\n",
    "    clear_output()\n",
    "\n",
    "    display(dataset_selector)\n",
    "    display(model_selector)\n",
    "    display(category_selector)\n",
    "    display(metric_selector)\n",
    "    display(button)\n",
    "\n",
    "    model = model_selector.value\n",
    "    dataset = dataset_selector.value\n",
    "    category: str = category_selector.value  # type: ignore\n",
    "    metric: str = metric_selector.value  # type: ignore\n",
    "\n",
    "    metric += \"_gap\"\n",
    "\n",
    "    if model == \"(all)\":\n",
    "        filtered_table = stats_table[(stats_table[\"category\"] == category) & (stats_table[\"dataset\"] == dataset)]\n",
    "    else:\n",
    "        filtered_table = stats_table[\n",
    "            (stats_table[\"category\"] == category)\n",
    "            & (stats_table[\"model\"] == model)\n",
    "            & (stats_table[\"dataset\"] == dataset)\n",
    "        ]\n",
    "\n",
    "    if len(filtered_table) == 0:\n",
    "        print(\"None of the runs in the table matched the selections.\")\n",
    "        return\n",
    "\n",
    "    # Removing the \"grouped\" from grouped_{category_name}\n",
    "    category_name = category.split(\"_\")[-1]\n",
    "    title = f\"{format_metric_name(gap_metric)} for {category_name.title()}\" + f\"- {format_metric_name(dataset)}\"\n",
    "\n",
    "    # Generate scatterplot with confidence interval.\n",
    "    fig = px.scatter(\n",
    "        filtered_table,\n",
    "        x=\"group\",\n",
    "        facet_col=\"group\",\n",
    "        y=metric,\n",
    "        color=\"model\",\n",
    "        error_y=metric + \"_width\",\n",
    "        labels={\n",
    "            \"group\": \"Protected Group\",\n",
    "            gap_metric: format_metric_name(gap_metric),\n",
    "            \"model\": \"LM\",\n",
    "        },\n",
    "        title=title,\n",
    "        height=600,\n",
    "    )\n",
    "\n",
    "    # Hide redundant x axis labels.\n",
    "    fig.update_xaxes(matches=None, tickangle=90)\n",
    "    fig.for_each_xaxis(lambda x: x.update(title=\"\"))\n",
    "    fig.for_each_annotation(lambda a: a.update(text=\"\"))\n",
    "\n",
    "    fig = fig.update_layout(\n",
    "        scattermode=\"group\",\n",
    "        font_color=\"black\",\n",
    "        font_size=18,\n",
    "        title_x=0.5,\n",
    "    )\n",
    "\n",
    "    plot_output_path = \"plot.png\"\n",
    "    fig.write_image(plot_output_path, scale=5)\n",
    "    with open(plot_output_path, \"rb\") as img_file:\n",
    "        image_widget = widgets.Image(value=img_file.read(), format=\"png\", width=600)\n",
    "        display(image_widget)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interative Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "button = widgets.Button(description=\"Visualize\")\n",
    "button.on_click(visualize_fn)\n",
    "visualize_fn(button)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare between models and datasets on the same protected group \n",
    "E.g. \"young\" people of the \"age\" category.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interative Visualization Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_groups = stats_table[\"group\"].unique()\n",
    "\n",
    "dataset_selector = widgets.Dropdown(options=datasets, value=datasets[0], description=\"Dataset:\")\n",
    "model_selector = widgets.Dropdown(options=models, value=models[0], description=\"Model:\")\n",
    "group_selector = widgets.Dropdown(options=models, value=models[0], description=\"Model:\")\n",
    "group_selector = widgets.Dropdown(options=groups, value=unique_groups[0], description=\"Group:\")\n",
    "metric_selector = widgets.Dropdown(options=metrics, value=metrics[0], description=\"Metric:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_fn_2(button: Any) -> None:\n",
    "    clear_output()\n",
    "    display(dataset_selector)\n",
    "    display(model_selector)\n",
    "    display(group_selector)\n",
    "    display(metric_selector)\n",
    "    display(button)\n",
    "\n",
    "    model = model_selector.value\n",
    "    dataset = dataset_selector.value\n",
    "    group: str = group_selector.value  # type: ignore\n",
    "    metric: str = metric_selector.value  # type: ignore\n",
    "\n",
    "    gap_metric = metric + \"_gap\"\n",
    "\n",
    "    if model == \"(all)\":\n",
    "        filtered_table = stats_table[(stats_table[\"dataset\"] != \"\")]\n",
    "    else:\n",
    "        filtered_table = stats_table[(stats_table[\"model\"] == model) & (stats_table[\"dataset\"] != \"\")]\n",
    "\n",
    "    group_filtered_table = filtered_table[filtered_table[\"group\"] == group]\n",
    "\n",
    "    if len(group_filtered_table) == 0:\n",
    "        print(\"None of the runs in the table matched the selections.\")\n",
    "        return\n",
    "\n",
    "    title = f\"{format_metric_name(gap_metric)} for {group.title()} \" + f\"- {format_metric_name(dataset)}\"\n",
    "\n",
    "    fig = px.scatter(\n",
    "        group_filtered_table,\n",
    "        x=\"dataset\",\n",
    "        facet_col=\"dataset\",\n",
    "        y=gap_metric,\n",
    "        color=\"model\",\n",
    "        error_y=gap_metric + \"_width\",\n",
    "        labels={\n",
    "            \"group\": \"Prompt-Tuning Dataset\",\n",
    "            gap_metric: format_metric_name(gap_metric),\n",
    "            \"model\": \"LM\",\n",
    "            **{dataset: format_metric_name(dataset) for dataset in filtered_table[\"dataset\"].unique()},\n",
    "        },\n",
    "        title=title,\n",
    "        height=600,\n",
    "    )\n",
    "\n",
    "    fig.update_xaxes(matches=None, tickangle=90)\n",
    "    fig.for_each_xaxis(lambda x: x.update(title=\"\"))\n",
    "    fig.for_each_annotation(lambda a: a.update(text=\"\"))\n",
    "\n",
    "    fig = fig.update_layout(\n",
    "        scattermode=\"group\",\n",
    "        font_color=\"black\",\n",
    "        font_size=18,\n",
    "        title_x=0.5,\n",
    "    )\n",
    "\n",
    "    plot_output_path = \"plot.png\"\n",
    "    fig.write_image(plot_output_path, scale=5)\n",
    "    with open(plot_output_path, \"rb\") as img_file:\n",
    "        image_widget = widgets.Image(value=img_file.read(), format=\"png\", width=600)\n",
    "        display(image_widget)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interative Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "button = widgets.Button(description=\"Visualize\")\n",
    "button.on_click(visualize_fn)\n",
    "visualize_fn_2(button)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "25f6c606dd86a3cae59df91b75f7a72b0b2bf1388cda4c8c160c9bd2d7ceb352"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
