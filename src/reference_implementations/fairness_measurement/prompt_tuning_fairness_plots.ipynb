{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias Visualization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "Note that we use the scikit-learn confusion matrix utility to help calculate some of the fairness metrics. We generate the visualizations using plotly Express."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install kaleido numpy pandas plotly scikit-learn tqdm ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from math import sqrt\n",
    "from typing import Any, Dict\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from IPython.display import clear_output, display\n",
    "from sklearn.metrics import confusion_matrix as ConfusionMatrix\n",
    "from tqdm.auto import tqdm\n",
    "from os import listdir\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Constants"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will visualize how each of the following variable affects the fairness towards a particular protected group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEPENDENT_VARIABLES = [\"model\", \"dataset\", \"num_params\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_metric_name(metric_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Prettify diagram texts by replacing\n",
    "    snake case with regular title case.\n",
    "    \"\"\"\n",
    "    output_words = []\n",
    "    for word in metric_name.split(\"_\"):\n",
    "        word = word.title() if word not in [\"FPR\", \"TPR\"] else word.upper()\n",
    "        output_words.append(word)\n",
    "\n",
    "    return \" \".join(output_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Predictions Ready"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Prediction File\n",
    "Load the prediction `tsv` file from the provided paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59e4ad72aab54915822ecb47b2a33337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                    | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resources/final_all/predictions_regard_2000.tsv\n",
      "12221\n",
      "resources/final_all/predictions_regard_5023.tsv\n",
      "24442\n",
      "resources/final_all/predictions_regard_4.tsv\n",
      "36663\n",
      "resources/final_all/predictions_regard_3679.tsv\n",
      "48884\n",
      "resources/final_all/regard_roberta-base_SST5.tsv\n",
      "109989\n",
      "resources/final_all/regard_roberta-large_SST5.tsv\n",
      "171094\n",
      "resources/final_all/predictions_regard.tsv\n",
      "232199\n",
      "resources/final_all/predictions_regard_16087.tsv\n",
      "244420\n",
      "resources/final_all/predictions_regard_862.tsv\n",
      "256641\n",
      "resources/final_all/predictions_regard_902.tsv\n",
      "268862\n",
      "resources/final_all/predictions_regard_455.tsv\n",
      "281083\n",
      "resources/final_all/regard_opt-350m_SST5.tsv\n",
      "342188\n",
      "resources/final_all/predictions_regard_89.tsv\n",
      "354409\n",
      "resources/final_all/regard_opt-125m_SST5.tsv\n",
      "415514\n",
      "resources/final_all/predictions_regard_8.tsv\n",
      "427735\n"
     ]
    }
   ],
   "source": [
    "def find_csv_filenames(path_to_dir, template_name, suffix=\".tsv\"):\n",
    "    filenames = listdir(path_to_dir)\n",
    "    return [path_to_dir + filename for filename in filenames if filename.endswith(suffix) and template_name in filename]\n",
    "\n",
    "# template_name = \"amazon\"\n",
    "# template_name = \"NS-prompts\"\n",
    "template_name = \"regard\"\n",
    "\n",
    "results_path_to_dir = \"resources/final_all/\"\n",
    "prediction_tsv_paths = find_csv_filenames(results_path_to_dir, template_name)\n",
    "\n",
    "\n",
    "plot_name = template_name\n",
    "# model_names = [\"opt-125m\", \"opt-350m\", \"opt-1d3b\", \"opt-6d7b\", \"roberta-base\", \"roberta-large\", \"llama-7b\"]\n",
    "# case_names = [template_name + \"_\" + model_name + \"_SST5\" for model_name in model_names]\n",
    "# prediction_tsv_paths = [\n",
    "#     \"resources/final_all/\" + case_name + \".tsv\" for case_name in case_names\n",
    "# ]\n",
    "\n",
    "\n",
    "# prediction_tsv_paths = [\n",
    "#     \"/Users/david/Documents/SoftPromptTuningFairness/TACLRevisionsData/July_12_2023_SST5_Experiments/\"\n",
    "#     \"predictions-eval-sweep-20230709a2.tsv\"\n",
    "# ]\n",
    "\n",
    "output_folder: str = \"stats\"\n",
    "output_table = pd.DataFrame()\n",
    "\n",
    "for prediction_tsv_path in tqdm(prediction_tsv_paths, ncols=80):\n",
    "    prediction_tsv_filename = os.path.basename(prediction_tsv_path)\n",
    "    dataframe = pd.read_csv(prediction_tsv_path, delimiter=\"\\t\")\n",
    "    del dataframe[\"text\"]\n",
    "\n",
    "    if output_table is None:\n",
    "        output_table = dataframe\n",
    "    else:\n",
    "        assert isinstance(output_table, pd.DataFrame)\n",
    "        output_table = pd.concat([output_table, dataframe])\n",
    "    print(prediction_tsv_path)\n",
    "    print(output_table.size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Statistics\n",
    "Since there can be multiple examples for each protected group, we apply the `groupby` method to compute aggregated statistics across all the examples of each protected group.\n",
    "\n",
    "Refer to the following function for details on how we implemented the metrics."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for aggregating statistics\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you will find a function that takes in a dataframe. This dataframe is will be automatically generated using the `pd.DataFrame.groupby`. The content of this dataframe is a subset of one particular prediction tsv for a particular protected group (e.g., the \"young\" group for the protected class \"age\"). \n",
    "\n",
    "The function will return a `pd.Series` (like a dictionary) mapping the following fairness metrics to their floating point values:\n",
    "- Accuracy\n",
    "- TPR(Positive)\n",
    "- TPR(Neutral)\n",
    "- TPR(Negative)\n",
    "- FPR(Positive)\n",
    "- FPR(Neutral)\n",
    "- FPR(Negative)\n",
    "\n",
    "Also refer to https://stackoverflow.com/a/50671617 for details on how the TPR and FPR values are computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_filter = 5\n",
    "unique_runs_with_accuracy = output_table[[\"model\", \"run_id\", \"dataset\"]].drop_duplicates()\n",
    "unique_runs_with_accuracy = unique_runs_with_accuracy.sort_values(\n",
    "    [\"model\", \"dataset\"], ascending=False\n",
    ")\n",
    "best_runs_by_model_dataset = unique_runs_with_accuracy.groupby([\"model\", \"dataset\"]).head(top_n_filter).reset_index()\n",
    "run_ids_to_keep = best_runs_by_model_dataset[\"run_id\"].tolist()\n",
    "output_table = output_table.loc[output_table[\"run_id\"].isin(run_ids_to_keep)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38885, 11)\n",
      "['/ssd005/projects/llm/fair-llm/llama/llama-2-7b' 'facebook/opt-1.3b'\n",
      " 'roberta-base fine-tuned' 'roberta-large fine-tuned' 'facebook/opt-6.7b'\n",
      " 'opt-350m fine-tuned' 'opt-125m fine-tuned']\n"
     ]
    }
   ],
   "source": [
    "print(output_table.shape)\n",
    "print(output_table[\"model\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the names of the models and the datasets for display purposes\n",
    "def modify_model_name(model_name: str) -> str:\n",
    "    if model_name.startswith(\"facebook/\"):\n",
    "        return model_name.replace(\"facebook/\", \"\").upper()\n",
    "    if model_name.startswith(\"/ssd005/projects/llm/fair-llm/llama/\"):\n",
    "        return model_name.replace(\"/ssd005/projects/llm/fair-llm/llama/\", \"\").replace(\"llama\", \"LLaMA\")\n",
    "    if \"fine-tuned\" in model_name:\n",
    "        if model_name.startswith(\"opt\"):\n",
    "            return model_name.replace(\"fine-tuned\", \"\").replace(\"opt\", \"OPT\")\n",
    "        if model_name.startswith(\"roberta\"):\n",
    "            return model_name.replace(\"fine-tuned\", \"\").replace(\"roberta\", \"RoBERTa\")   \n",
    "    else:\n",
    "        llama_name = model_name.replace(\"/data/models/\", \"\")\n",
    "        return llama_name.replace(\"llama/\", \"LLaMA-\")\n",
    "\n",
    "\n",
    "def modify_dataset_name(dataset_name: str) -> str:\n",
    "    # dataset_name = dataset_name.replace(\"jacobthebanana/\", \"\")\n",
    "    # if \"_mapped_\" in dataset_name:\n",
    "    #     return dataset_name.replace(\"sst5_mapped_grouped\", \"SST5 Grouped\")\n",
    "    # elif \"-mapped-\" in dataset_name:\n",
    "    #     return dataset_name.replace(\"sst5-mapped-extreme\", \"SST5 Extreme\")\n",
    "    # elif \"data/processed/semeval_3\" in dataset_name:\n",
    "    #     return \"SemEval\"\n",
    "    # return \"\"\n",
    "    return \"SST5 Extreme\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(dataframe: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Input: dataframe representing all predictions for a particular\n",
    "    protected group in a particular run.\n",
    "    \"\"\"\n",
    "    output: Dict[str, float] = {}\n",
    "\n",
    "    num_examples = len(dataframe)\n",
    "    num_correct = np.sum(dataframe[\"y_pred\"] == dataframe[\"y_true\"])\n",
    "\n",
    "    output[\"accuracy\"] = num_correct / num_examples\n",
    "\n",
    "    # See https://stackoverflow.com/a/50671617\n",
    "    confusion_matrix = ConfusionMatrix(dataframe[\"y_true\"], dataframe[\"y_pred\"], labels=[0,1,2])\n",
    "\n",
    "    FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "    FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "    TP = np.diag(confusion_matrix)\n",
    "    TN = confusion_matrix.sum() - (FP + FN + TP)\n",
    "\n",
    "    eps = 10 ** -10\n",
    "\n",
    "    FP = np.copy(FP) + eps\n",
    "    FN = np.copy(FN) + eps\n",
    "    TP = np.copy(TP) + eps\n",
    "    TN = np.copy(TN) + eps\n",
    "    \n",
    "    FPR = FP / (FP + TN)\n",
    "    FNR = FN / (TP + FN)\n",
    "    TPR = 1 - FNR\n",
    "\n",
    "    for label_index, label in enumerate([\"negative\", \"neutral\", \"positive\"]):\n",
    "        output[label + \"_FPR\"] = FPR[label_index]\n",
    "        output[label + \"_TPR\"] = TPR[label_index]\n",
    "\n",
    "    return pd.Series(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Per-Group Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>positive_TPR</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th>dataset</th>\n",
       "      <th>num_params</th>\n",
       "      <th>run_id</th>\n",
       "      <th>category</th>\n",
       "      <th>group</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">/ssd005/projects/llm/fair-llm/llama/llama-2-7b</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">jacobthebanana/sst5-mapped-extreme</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">-8.00</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">2sjwxsn3</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">grouped_race</th>\n",
       "      <th>african_american</th>\n",
       "      <td>0.057143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>american_indian</th>\n",
       "      <td>0.057143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asian</th>\n",
       "      <td>0.057143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hispanic</th>\n",
       "      <td>0.057143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pacific_islander</th>\n",
       "      <td>0.057143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">roberta-large fine-tuned</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">SST5</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">0.35</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">r4</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">grouped_race</th>\n",
       "      <th>american_indian</th>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asian</th>\n",
       "      <td>0.085714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hispanic</th>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pacific_islander</th>\n",
       "      <td>0.171429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>white</th>\n",
       "      <td>0.057143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>210 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                     positive_TPR\n",
       "model                                          dataset                            num_params run_id   category     group                         \n",
       "/ssd005/projects/llm/fair-llm/llama/llama-2-7b jacobthebanana/sst5-mapped-extreme -8.00      2sjwxsn3 grouped_race african_american      0.057143\n",
       "                                                                                                                   american_indian       0.057143\n",
       "                                                                                                                   asian                 0.057143\n",
       "                                                                                                                   hispanic              0.057143\n",
       "                                                                                                                   pacific_islander      0.057143\n",
       "...                                                                                                                                           ...\n",
       "roberta-large fine-tuned                       SST5                                0.35      r4       grouped_race american_indian       0.142857\n",
       "                                                                                                                   asian                 0.085714\n",
       "                                                                                                                   hispanic              0.142857\n",
       "                                                                                                                   pacific_islander      0.171429\n",
       "                                                                                                                   white                 0.057143\n",
       "\n",
       "[210 rows x 1 columns]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output_table\n",
    "analyzed_dataframe = output_table.groupby([*INDEPENDENT_VARIABLES, \"run_id\", \"category\", \"group\"]).apply(get_stats)\n",
    "analyzed_dataframe.query('category == \"grouped_race\"')[[\"positive_TPR\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_table = analyzed_dataframe.reset_index()\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "output_table.to_csv(os.path.join(output_folder, \"aggregated.csv\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Statistics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin, note that some of the protected classes include a large number of groups. You may filter on the protected groups to include in each category. If a category isn't in this dictionary, the visualizations will include all the groups under that category by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groups selected: pacific_islander, african_american, hispanic, american_indian, white, asian\n"
     ]
    }
   ],
   "source": [
    "SELECTED_GROUPS = {\n",
    "    \"grouped_religion\": [\"christianity\", \"islam\", \"judaism\"],\n",
    "    \"grouped_disability\": [\n",
    "        \"without\",\n",
    "        \"cognitive\",\n",
    "        \"physical\",\n",
    "        \"hearing\",\n",
    "        \"sight\",\n",
    "        \"chronic_illness\",\n",
    "        \"mobility\",\n",
    "        \"mental_health\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "groups_to_include = []\n",
    "for category in output_table[\"category\"].unique():\n",
    "    groups = SELECTED_GROUPS.get(category)\n",
    "    if groups is None:\n",
    "        groups = list(set(output_table.query(f'category == \"{category}\"')[\"group\"]))\n",
    "\n",
    "    groups_to_include.extend(groups)\n",
    "\n",
    "print(\"Groups selected:\", \", \".join(groups_to_include))\n",
    "output_table = output_table.query(\"group == @groups_to_include\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Gap\": How each group deviates from the category mean\n",
    "Below, we will calculate how far each group deviates from the mean of that category. For example, how far does the accuracy on examples mentioning the \"young\" group deviate from the mean of all examples of the \"age\" category?\n",
    "\n",
    "If there are more than one runs for each model, we would calculate gap metrics for each run separately. \n",
    "\n",
    "We use a for loop to add the following metrics of interest to our list:\n",
    "- Accuracy\n",
    "- False Positive Rates\n",
    "    - FPR(\"Negative\")\n",
    "    - FPR(\"Neutral\")\n",
    "    - FPR(\"Positive\")\n",
    "- True Positive Rates\n",
    "    - TPR(\"Negative\")\n",
    "    - TPR(\"Neutral\")\n",
    "    - TPR(\"Positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accuracy_gap', 'negative_FPR_gap', 'negative_TPR_gap', 'neutral_FPR_gap', 'neutral_TPR_gap', 'positive_FPR_gap', 'positive_TPR_gap']\n"
     ]
    }
   ],
   "source": [
    "metrics = [\"accuracy\"]\n",
    "for label in [\"negative\", \"neutral\", \"positive\"]:\n",
    "    metrics.append(label + \"_FPR\")\n",
    "    metrics.append(label + \"_TPR\")\n",
    "\n",
    "gap_metrics = []\n",
    "for metric in metrics:\n",
    "    gap_metrics.append(f\"{metric}_gap\")\n",
    "\n",
    "print(gap_metrics)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence Interval Calculations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous step, we've found the gaps for each (metric, category, group, model, run/seed). If we have more than one runs/seeds for each model, we can aggregate the results to find a confidence interval for each gap variable.\n",
    "\n",
    "To do so, we will aggregate the previous table (metric, category, group, model, run/seed) along the run/seed axis. The result will be a table with indices (metric, category, group, model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a (metric, group) placeholder table for storing the output values.\n",
    "# Unlike in the previous step, \"run_id\" isn't part of groupby,\n",
    "# and this axis will be squeezed in the result.\n",
    "stats_table = (\n",
    "    output_table[[*INDEPENDENT_VARIABLES, \"category\", \"group\", \"accuracy\"]]\n",
    "    .groupby([*INDEPENDENT_VARIABLES, \"category\", \"group\"])\n",
    "    .agg([\"mean\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric, gap_metric in zip(metrics, gap_metrics):\n",
    "    group_by = [*INDEPENDENT_VARIABLES, \"category\", \"run_id\"]\n",
    "    grouped = output_table[[*INDEPENDENT_VARIABLES, \"category\", \"run_id\", metric]].groupby(group_by)\n",
    "    group_mean = grouped.transform(\"mean\")\n",
    "    normalized_values = output_table[metric] - group_mean[metric]\n",
    "    output_table[gap_metric] = normalized_values\n",
    "\n",
    "    # See https://stackoverflow.com/a/53522680\n",
    "    # Calculate mean and stdev for each (metric, category, group, model).\n",
    "    stats = (\n",
    "        output_table[[*INDEPENDENT_VARIABLES, \"category\", \"group\", gap_metric]]\n",
    "        .groupby([*INDEPENDENT_VARIABLES, \"category\", \"group\"])\n",
    "        .agg([\"mean\", \"count\", \"std\"])\n",
    "    )\n",
    "\n",
    "    mean_values = []\n",
    "    ci_width = []\n",
    "    lower_values = []\n",
    "    upper_values = []\n",
    "    hypothesis_selected = []\n",
    "    z_score = 1.96\n",
    "\n",
    "    # Calculate 95% Confidence interval for each\n",
    "    # (metric, category, group, model).\n",
    "    for index in stats.index:\n",
    "        mean, n, stdev = stats.loc[index]  # type: ignore\n",
    "        lower = mean - z_score * stdev / sqrt(n)\n",
    "        upper = mean + z_score * stdev / sqrt(n)\n",
    "\n",
    "        mean_values.append(mean)\n",
    "        ci_width.append(z_score * stdev / sqrt(n))\n",
    "        lower_values.append(lower)\n",
    "        upper_values.append(upper)\n",
    "\n",
    "        if lower > 0:\n",
    "            hypothesis_selected.append(1)\n",
    "        elif upper < 0:\n",
    "            hypothesis_selected.append(-1)\n",
    "        else:\n",
    "            hypothesis_selected.append(0)\n",
    "\n",
    "    stats_table[gap_metric] = mean_values\n",
    "    stats_table[gap_metric + \"_width\"] = ci_width\n",
    "    stats_table[gap_metric + \"_lower\"] = lower_values\n",
    "    stats_table[gap_metric + \"_upper\"] = upper_values\n",
    "\n",
    "stats_table = stats_table.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_table[\"model\"] = stats_table[\"model\"].apply(modify_model_name)\n",
    "stats_table[\"dataset\"] = stats_table[\"dataset\"].apply(modify_dataset_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare between Groups of each category \n",
    "E.g. all groups in the \"age\" category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = stats_table[\"dataset\"].unique()\n",
    "models = [\"(all)\", *(stats_table[\"model\"].unique())]\n",
    "categories = stats_table[\"category\"].unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interactive Visualization Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selector = widgets.Dropdown(options=models, value=models[0], description=\"Model:\")\n",
    "category_selector = widgets.Dropdown(options=categories, value=categories[0], description=\"Category:\")\n",
    "metric_selector = widgets.Dropdown(options=metrics, value=metrics[0], description=\"Metric:\")\n",
    "dataset_selector = widgets.Dropdown(options=datasets, value=datasets[0], description=\"Dataset:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_fn(button: Any) -> None:\n",
    "    \"\"\"\n",
    "    Function to be called when the button is pressed.\n",
    "    This function clears previous output and any widget\n",
    "    that has already been rendered. To add these widgets\n",
    "    back (including the button) the \"button\" parameter\n",
    "    needs to be supplied from the button action.\n",
    "    \"\"\"\n",
    "    clear_output()\n",
    "\n",
    "    display(model_selector)\n",
    "    display(category_selector)\n",
    "    display(metric_selector)\n",
    "    display(dataset_selector)\n",
    "    display(button)\n",
    "\n",
    "    model = model_selector.value\n",
    "    category: str = category_selector.value  # type: ignore\n",
    "    metric: str = metric_selector.value  # type: ignore\n",
    "    dataset: str = dataset_selector.value  # type: ignore\n",
    "\n",
    "    metric += \"_gap\"\n",
    "\n",
    "    if model == \"(all)\":\n",
    "        filtered_table = stats_table[(stats_table[\"category\"] == category) & (stats_table[\"dataset\"] == dataset)]\n",
    "    else:\n",
    "        filtered_table = stats_table[\n",
    "            (stats_table[\"category\"] == category)\n",
    "            & (stats_table[\"model\"] == model)\n",
    "            & (stats_table[\"dataset\"] == dataset)\n",
    "        ]\n",
    "\n",
    "    if len(filtered_table) == 0:\n",
    "        print(\"None of the runs in the table matched the selections.\")\n",
    "        return\n",
    "\n",
    "\n",
    "    # Removing the \"grouped\" from grouped_{category_name}\n",
    "    category_name = category.split(\"_\")[-1]\n",
    "    title = f\"Gap for {category_name.title()}<br>{dataset}\"\n",
    "\n",
    "    # Generate scatterplot with confidence interval.\n",
    "    fig = px.scatter(\n",
    "        filtered_table,\n",
    "        x=\"group\",\n",
    "        facet_col=\"group\",\n",
    "        y=metric,\n",
    "        color=\"model\",\n",
    "        error_y=metric + \"_width\",\n",
    "        labels={\n",
    "            \"group\": \"Protected Group\",\n",
    "            metric: format_metric_name(metric),\n",
    "            \"model\": \"LM\",\n",
    "        },\n",
    "        # category_orders={\n",
    "        #     \"model\": [\"OPT-125M\", \"OPT-350M\", \"OPT-1.3B\", \"OPT-2.7B\", \"OPT-6.7B\", \"OPT-13B\", \"LLaMA-7B\", \"LLaMA-13B\"]\n",
    "        # },\n",
    "        title=title,\n",
    "        height=900,\n",
    "        width=1200,\n",
    "    )\n",
    "\n",
    "    # Hide redundant x axis labels.\n",
    "    fig.update_xaxes(\n",
    "        matches=None, tickangle=-35, title_font_family=\"Helvetica\", tickfont_family=\"Helvetica\", tickfont_size=28\n",
    "    )\n",
    "    fig.update_yaxes(title_font_family=\"Helvetica\", tickfont_family=\"Helvetica\", title_font_size=36)\n",
    "    fig.for_each_xaxis(lambda x: x.update(title=\"\"))\n",
    "    fig.for_each_annotation(lambda a: a.update(text=\"\"))\n",
    "    fig.update_traces(\n",
    "        marker=dict(\n",
    "            size=10,\n",
    "        ),\n",
    "        error_y=dict(thickness=5),\n",
    "    )\n",
    "\n",
    "    fig = fig.update_layout(legend=dict(title_font_family=\"Helvetica\", font=dict(size=28)))\n",
    "\n",
    "    fig = fig.update_layout(\n",
    "        scattermode=\"group\",\n",
    "        font_color=\"black\",\n",
    "        title_font_family=\"Helvetica\",\n",
    "        font_size=24,\n",
    "        title_x=0.47,\n",
    "    )\n",
    "\n",
    "    fig.update_yaxes(fixedrange=True)\n",
    "    fig.update_yaxes(range=[-0.2, 0.2])\n",
    "\n",
    "\n",
    "    plot_dir = \"plots/\" + plot_name + \"/\"\n",
    "    if not os.path.exists(plot_dir):\n",
    "        os.makedirs(plot_dir)\n",
    "    plot_output_path = plot_dir + metric + \".png\"\n",
    "    fig.write_image(plot_output_path, width=370)\n",
    "    with open(plot_output_path, \"rb\") as img_file:\n",
    "        image_widget = widgets.Image(value=img_file.read(), format=\"png\", width=370)\n",
    "        display(image_widget)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interative Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4410f65632d14e7aaa575877700a7cb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Model:', options=('(all)', 'LLaMA-2-7b', 'OPT-1.3B', 'OPT-6.7B', 'OPT-125m ', 'OPT-350m …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4f557d8803747f1b53eedef4c81ca93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Category:', options=('grouped_race',), value='grouped_race')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f2f6da9822841a49f5d9bff3cd71fe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Metric:', index=1, options=('accuracy', 'negative_FPR', 'negative_TPR', 'neutral_FPR', '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bef12f0ce7bb4e1ea41348a114d5c26d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Dataset:', options=('SST5 Extreme',), value='SST5 Extreme')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfa787aa2e924da699f519969bae809a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Visualize', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d04a78ae2a34bd6bb220dba5b0aff27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x04\\xb0\\x00\\x00\\x03\\x84\\x08\\x06\\x00\\x00\\x00\\xb1m\\xc8…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "button = widgets.Button(description=\"Visualize\")\n",
    "button.on_click(visualize_fn)\n",
    "visualize_fn(button)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare between models and datasets on the same protected group \n",
    "E.g. \"young\" people of the \"age\" category.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interative Visualization Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_groups = stats_table[\"group\"].unique()\n",
    "model_selector = widgets.Dropdown(options=models, value=models[0], description=\"Model:\")\n",
    "group_selector = widgets.Dropdown(options=unique_groups, value=unique_groups[0], description=\"Group:\")\n",
    "metric_selector = widgets.Dropdown(options=metrics, value=metrics[0], description=\"Metric:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_fn_2(button: Any) -> None:\n",
    "    clear_output()\n",
    "    display(model_selector)\n",
    "    display(group_selector)\n",
    "    display(metric_selector)\n",
    "    display(button)\n",
    "\n",
    "    model = model_selector.value\n",
    "    group: str = group_selector.value  # type: ignore\n",
    "    metric: str = metric_selector.value  # type: ignore\n",
    "\n",
    "    gap_metric = metric + \"_gap\"\n",
    "\n",
    "    if model == \"(all)\":\n",
    "        filtered_table = stats_table[(stats_table[\"dataset\"] != \"\")]\n",
    "    else:\n",
    "        filtered_table = stats_table[(stats_table[\"model\"] == model) & (stats_table[\"dataset\"] != \"\")]\n",
    "\n",
    "    group_filtered_table = filtered_table[filtered_table[\"group\"] == group]\n",
    "\n",
    "    if len(group_filtered_table) == 0:\n",
    "        print(\"None of the runs in the table matched the selections.\")\n",
    "        return\n",
    "\n",
    "    title = f\"Gap for {group.title()} \"\n",
    "\n",
    "    fig = px.scatter(\n",
    "        group_filtered_table,\n",
    "        x=\"dataset\",\n",
    "        facet_col=\"dataset\",\n",
    "        y=gap_metric,\n",
    "        color=\"model\",\n",
    "        error_y=gap_metric + \"_width\",\n",
    "        labels={\n",
    "            \"group\": \"Protected Group\",\n",
    "            gap_metric: format_metric_name(gap_metric),\n",
    "            \"model\": \"LM\",\n",
    "            **{dataset: format_metric_name(dataset) for dataset in filtered_table[\"dataset\"].unique()},\n",
    "        },\n",
    "        title=title,\n",
    "        height=600,\n",
    "    )\n",
    "\n",
    "    fig.update_xaxes(matches=None, tickangle=90)\n",
    "    fig.for_each_xaxis(lambda x: x.update(title=\"\"))\n",
    "    fig.for_each_annotation(lambda a: a.update(text=\"\"))\n",
    "\n",
    "\n",
    "    fig = fig.update_layout(\n",
    "        scattermode=\"group\",\n",
    "        font_color=\"black\",\n",
    "        font_size=18,\n",
    "        title_x=0.5,\n",
    "    )\n",
    "    fig.update_yaxes(fixedrange=True)\n",
    "    fig.update_yaxes(range=[-0.1, 0.1])\n",
    "\n",
    "\n",
    "    plot_dir = \"plots/\" + plot_name + \"/\"\n",
    "    if not os.path.exists(plot_dir):\n",
    "        os.makedirs(plot_dir)\n",
    "    plot_output_path = plot_dir + metric + \".png\"\n",
    "\n",
    "    fig.write_image(plot_output_path, scale=5)\n",
    "    with open(plot_output_path, \"rb\") as img_file:\n",
    "        image_widget = widgets.Image(value=img_file.read(), format=\"png\", width=600)\n",
    "        display(image_widget)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interative Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4410f65632d14e7aaa575877700a7cb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Model:', options=('(all)', 'LLaMA-2-7b', 'OPT-1.3B', 'OPT-6.7B', 'OPT-125m ', 'OPT-350m …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0709bdd6325b4d9392bbca6ada52aade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Group:', options=('african_american', 'american_indian', 'asian', 'hispanic', 'pacific_i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f2f6da9822841a49f5d9bff3cd71fe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Metric:', options=('accuracy', 'negative_FPR', 'negative_TPR', 'neutral_FPR', 'neutral_T…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21948af148cf4245ace7f14b1b48e2dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Visualize', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7c2716659ab426cad2b6e06a8405126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\r\\xac\\x00\\x00\\x0b\\xb8\\x08\\x06\\x00\\x00\\x00\\xb7\\xe5\\x1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "button = widgets.Button(description=\"Visualize\")\n",
    "button.on_click(visualize_fn_2)\n",
    "visualize_fn_2(button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "25f6c606dd86a3cae59df91b75f7a72b0b2bf1388cda4c8c160c9bd2d7ceb352"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
