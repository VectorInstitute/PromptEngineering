{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "from random import sample\n",
    "from typing import List, Tuple\n",
    "\n",
    "import evaluate\n",
    "import lingua\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from evaluate import EvaluationModule\n",
    "from utils import (\n",
    "    copa_preprocessor,\n",
    "    create_first_prompt,\n",
    "    create_first_prompt_label,\n",
    "    create_second_prompt,\n",
    "    create_second_prompt_label,\n",
    "    split_prompts_into_batches,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "There is a bit of documentation on how to interact with the large models [here](https://lingua-sdk.readthedocs.io/en/latest/getting_started.html). The relevant github links to the SDK are [here](https://github.com/VectorInstitute/lingua-sdk) and underlying code [here](https://github.com/VectorInstitute/lingua)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we connect to the service through which, we'll interact with the LLMs and see which models are avaiable to us"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Establish a client connection to the Lingua service\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = lingua.Client(gateway_host=\"llm.cluster.local\", gateway_port=3001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.models\n",
    "client.model_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = client.load_model(\"OPT-175B\")\n",
    "# If this model is not actively running, it will get launched in the background.\n",
    "# In this case, wait until it moves into an \"ACTIVE\" state before proceeding.\n",
    "while model.state != \"ACTIVE\":\n",
    "    time.sleep(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to configure the model to generate in the way we want it to. We set important parameters.\n",
    "\n",
    "*`max_tokens` sets the number the model generates before haulting generation.\n",
    "*`top_k`: Range: 0-Vocab size. At each generation step this is the number of tokens to select from with relative probabilities associated with their likliehoods. Setting this to 1 is \"Greedy decoding.\" If top_k is set to zero them we exclusively use nucleus sample (i.e. top_p below).\n",
    "*`top_p`: Range: 0.0-1.0, nucleus sampling. At each generation step, the tokens the largest probabilities, adding up to `top_p` are sampled from relative to their likliehoods.\n",
    "*`rep_penalty`: Range >= 1.0. This attempts to decrease the likelihood of tokens in a generation process if they have been generated before. A value of 1.0 means no penalty and larger values increasingly penalize repeated values. 1.2 has been reported as a good default value.\n",
    "*`temperature`: Range >=0.0. This value \"sharpens\" or flattens the softmax calculation done to produce probabilties over the vocab. As temperature goes to zero: only the largest probabilities will remain non-zero (approaches greedy decoding). As it approaches infinity, the distribution spreads out evenly over the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = {\"max_tokens\": 35, \"top_k\": 4, \"top_p\": 3, \"rep_penalty\": 1.2, \"temperature\": 1.0}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Balanced Choice of Plausible Alternatives (CoPA)\n",
    "We'll work on an updated (harder) version of the CoPA dataset. We're only going to work with a small subset of the true development set in order to expedite LLM evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many of the initial data points should be reserved for demonstrations\n",
    "demonstration_candidates = 50\n",
    "# Number of demonstrations to be used per prompt\n",
    "n_demonstrations = 10\n",
    "\n",
    "copa_data_set = copa_preprocessor(\"resources/copa_sample.tsv\")\n",
    "\n",
    "demonstration_pool = copa_data_set[0:demonstration_candidates]\n",
    "test_pool = copa_data_set[demonstration_candidates:]\n",
    "demonstrations = sample(demonstration_pool, n_demonstrations)\n",
    "prompts: List[str] = []\n",
    "labels: List[str] = []\n",
    "int_labels: List[int] = []\n",
    "choices: List[Tuple[str, str]] = []\n",
    "for premise, label, phrase, first_choice, second_choice in test_pool:\n",
    "    int_labels.append(label)\n",
    "    choices.append((first_choice, second_choice))\n",
    "    labels.append(create_first_prompt_label(first_choice.lower(), second_choice.lower(), label))\n",
    "    prompts.append(create_first_prompt(demonstrations, premise, phrase, first_choice, second_choice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompts[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this prompt performs on a small sample of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_generation_text(original_texts: List[str]) -> List[str]:\n",
    "    responses = []\n",
    "    for single_generation in original_texts:\n",
    "        generation_text: List[str] = re.findall(r\".*?[.!\\?]\", single_generation)\n",
    "        response_text = generation_text[0] if len(generation_text) > 0 else single_generation\n",
    "        responses.append(response_text)\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split prompts into batches for memory management.\n",
    "n_samples_to_run = 3\n",
    "responses: List[str] = []\n",
    "prompt_batches = split_prompts_into_batches(prompts[0:n_samples_to_run])\n",
    "for batch_number, prompt_batch in enumerate(prompt_batches):\n",
    "    generations = model.generate(prompt_batch, generation_config)\n",
    "    print(f\"Batch number {batch_number+1} Complete\")\n",
    "    responses.extend(process_generation_text(generations.generation[\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for response, label in zip(responses, labels[0:n_samples_to_run]):\n",
    "    print(f\"Response: {response}\\nLabel: {label}\\n\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring generated Responses for First Prompts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run all of the examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = []\n",
    "prompt_batches = split_prompts_into_batches(prompts)\n",
    "for batch_number, prompt_batch in enumerate(prompt_batches):\n",
    "    generations = model.generate(prompt_batch, generation_config)\n",
    "    print(f\"Batch number {batch_number+1} Complete\")\n",
    "    responses.extend(process_generation_text(generations.generation[\"text\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform scoring based on the generated text, by considering the rouge score of the responses using the label as the reference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_metric = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_response_via_rouge(\n",
    "    response: str, first_choice: str, second_choice: str, rouge_metric: EvaluationModule\n",
    ") -> int:\n",
    "    response = response.lower()\n",
    "    first_choice = first_choice.lower()\n",
    "    second_choice = second_choice.lower()\n",
    "    rouge_0 = rouge_metric.compute(predictions=[response.lower()], references=[first_choice.lower()])\n",
    "    rouge_1 = rouge_metric.compute(predictions=[response.lower()], references=[second_choice.lower()])\n",
    "    score_0 = (rouge_0[\"rouge1\"] + rouge_0[\"rouge2\"]) / 2.0\n",
    "    score_1 = (rouge_1[\"rouge1\"] + rouge_1[\"rouge2\"]) / 2.0\n",
    "    return 0 if score_0 > score_1 else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "for response, label_int, (first_choice, second_choice) in zip(responses, int_labels, choices):\n",
    "    predicted_label = score_response_via_rouge(response, first_choice, second_choice, rouge_metric)\n",
    "    if predicted_label == label_int:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "print(f\"Accuracy: {correct/total}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, for each prompt and the two responses, we can score the candidate responses by log likelihood and choose the higher one as our label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_prompts_with_choices(prompt_batch: List[Tuple[str, Tuple[str, str]]]) -> List[str]:\n",
    "    prompts_with_choices = []\n",
    "    for prompt, (first_choice, second_choice) in prompt_batch:\n",
    "        prompts_with_choices.append(f\"{prompt}{first_choice}\")\n",
    "        prompts_with_choices.append(f\"{prompt}{second_choice}\")\n",
    "    return prompts_with_choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihoods = []\n",
    "prompts_and_choices = pair_prompts_with_choices(list(zip(prompts, choices)))\n",
    "prompt_batches = split_prompts_into_batches(prompts_and_choices)\n",
    "for batch_number, prompt_batch in enumerate(prompt_batches):\n",
    "    activations = model.get_activations(prompt_batch, [], generation_config)\n",
    "    print(f\"Batch number {batch_number+1} Complete\")\n",
    "    # Log probs stores all of the activations associated with the input prompt (which has been completed with one of\n",
    "    # the two sentences)\n",
    "    for logprobs, tokens in zip(activations.logprobs, activations.tokens):\n",
    "        index = list(reversed(tokens)).index(\"\\n\") - 1\n",
    "        likelihoods.append(sum(logprobs[-index:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_likelihoods_to_labels(likelihoods: List[float]) -> Tuple[List[int], List[torch.Tensor]]:\n",
    "    # Need to group logprobs in twos because they represent likelihoods of the two completions\n",
    "    assert len(likelihoods) % 2 == 0\n",
    "    paired_likelihoods = [likelihoods[x : x + 2] for x in range(0, len(likelihoods), 2)]\n",
    "    predicted_labels = []\n",
    "    predicted_probs = []\n",
    "    softmax = nn.Softmax(dim=0)\n",
    "    for paired_likelihood in paired_likelihoods:\n",
    "        distribution = softmax(torch.tensor(paired_likelihood))\n",
    "        predicted_labels.append(np.argmax(paired_likelihood, axis=0))\n",
    "        predicted_probs.append(distribution)\n",
    "    return predicted_labels, predicted_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_1_pred_labels, prompt_1_pred_probs = post_process_likelihoods_to_labels(likelihoods)\n",
    "total = 0\n",
    "correct = 0\n",
    "for predicted_label, label_int in zip(prompt_1_pred_labels, int_labels):\n",
    "    if predicted_label == label_int:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "print(f\"Accuracy: {correct/total}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Alternative Prompts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first alternative prompt will simply be a resampling of the demonstration examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many of the initial data points should be reserved for demonstrations\n",
    "demonstration_candidates = 50\n",
    "# Number of demonstrations to be used per prompt\n",
    "n_demonstrations = 10\n",
    "\n",
    "demonstration_pool = copa_data_set[0:demonstration_candidates]\n",
    "test_pool = copa_data_set[demonstration_candidates:]\n",
    "demonstrations = sample(demonstration_pool, n_demonstrations)\n",
    "prompts = []\n",
    "labels = []\n",
    "int_labels = []\n",
    "choices = []\n",
    "for premise, label, phrase, first_choice, second_choice in test_pool:\n",
    "    int_labels.append(label)\n",
    "    choices.append((first_choice, second_choice))\n",
    "    labels.append(create_first_prompt_label(first_choice.lower(), second_choice.lower(), label))\n",
    "    prompts.append(create_first_prompt(demonstrations, premise, phrase, first_choice, second_choice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihoods = []\n",
    "prompts_and_choices = pair_prompts_with_choices(list(zip(prompts, choices)))\n",
    "prompt_batches = split_prompts_into_batches(prompts_and_choices)\n",
    "for batch_number, prompt_batch in enumerate(prompt_batches):\n",
    "    activations = model.get_activations(prompt_batch, [], generation_config)\n",
    "    print(f\"Batch number {batch_number+1} Complete\")\n",
    "    # Log probs stores all of the activations associated with the input prompt (which has been completed with one of\n",
    "    # the two sentences)\n",
    "    for logprobs, tokens in zip(activations.logprobs, activations.tokens):\n",
    "        index = list(reversed(tokens)).index(\"\\n\") - 1\n",
    "        likelihoods.append(sum(logprobs[-index:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_2_pred_labels, prompt_2_pred_probs = post_process_likelihoods_to_labels(likelihoods)\n",
    "total = 0\n",
    "correct = 0\n",
    "for predicted_label, label_int in zip(prompt_2_pred_labels, int_labels):\n",
    "    if predicted_label == label_int:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "print(f\"Accuracy: {correct/total}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second alternative prompt will simply score completion of the sentence with the possible completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demonstration_candidates = 50\n",
    "# Number of demonstrations to be used per prompt\n",
    "n_demonstrations = 10\n",
    "\n",
    "demonstration_pool = copa_data_set[0:demonstration_candidates]\n",
    "test_pool = copa_data_set[demonstration_candidates:]\n",
    "\n",
    "demonstrations = sample(demonstration_pool, n_demonstrations)\n",
    "prompts = []\n",
    "labels = []\n",
    "int_labels = []\n",
    "choices = []\n",
    "for premise, label, phrase, first_choice, second_choice in test_pool:\n",
    "    int_labels.append(label)\n",
    "    choices.append((first_choice, second_choice))\n",
    "    labels.append(create_second_prompt_label(first_choice.lower(), second_choice.lower(), label))\n",
    "    prompts.append(create_second_prompt(demonstrations, premise, phrase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihoods = []\n",
    "prompts_and_choices = pair_prompts_with_choices(list(zip(prompts, choices)))\n",
    "prompt_batches = split_prompts_into_batches(prompts_and_choices)\n",
    "for batch_number, prompt_batch in enumerate(prompt_batches):\n",
    "    activations = model.get_activations(prompt_batch, [], generation_config)\n",
    "    print(f\"Batch number {batch_number+1} Complete\")\n",
    "    # Log probs stores all of the activations associated with the input prompt (which has been completed with one of\n",
    "    # the two sentences)\n",
    "    for logprobs, tokens in zip(activations.logprobs, activations.tokens):\n",
    "        index = list(reversed(tokens)).index(\"\\n\") - 1\n",
    "        likelihoods.append(sum(logprobs[-index:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_3_pred_labels, prompt_3_pred_probs = post_process_likelihoods_to_labels(likelihoods)\n",
    "total = 0\n",
    "correct = 0\n",
    "for predicted_label, label_int in zip(prompt_3_pred_labels, int_labels):\n",
    "    if predicted_label == label_int:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "print(f\"Accuracy: {correct/total}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting and Averaging Ensembles"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've collected predictions from three prompting setups, we'll ensemble the responses. We'll start with simple voting and measure accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "for vote_1, vote_2, vote_3, label_int in zip(\n",
    "    prompt_1_pred_labels, prompt_2_pred_labels, prompt_3_pred_labels, int_labels\n",
    "):\n",
    "    vote_tally = sum([vote_1, vote_2, vote_3])\n",
    "    predicted_label = 0 if vote_tally < 1.5 else 1\n",
    "    if predicted_label == label_int:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "print(f\"Accuracy: {correct/total}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above works quite poorly because we one prompt that is much better than the other two. Voting is better when there are a lot of prompts, or the classifiers perform similarly"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's consider averaging the label probabilities and taking the largest value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "weighting = torch.tensor([1.0, 1.0, 1.5]).reshape(-1, 1)\n",
    "for probs_1, probs_2, probs_3, label_int in zip(\n",
    "    prompt_1_pred_probs, prompt_2_pred_probs, prompt_3_pred_probs, int_labels\n",
    "):\n",
    "    probs = torch.sum(torch.stack((probs_1, probs_2, probs_3)) * weighting, dim=0) / sum(weighting)\n",
    "    predicted_label = torch.argmax(probs)\n",
    "    if predicted_label == label_int:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "print(f\"Accuracy: {correct/total}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above doesn't work very well because one of our prompts dominates in terms of accuracy. However, we can get improvement over either individual weaker prompt, if we ensemble the two,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "weighting = torch.tensor([1.0, 1.0]).reshape(-1, 1)\n",
    "for probs_1, probs_2, label_int in zip(prompt_1_pred_probs, prompt_2_pred_probs, int_labels):\n",
    "    probs = torch.sum(torch.stack((probs_1, probs_2)) * weighting, dim=0) / sum(weighting)\n",
    "    predicted_label = torch.argmax(probs)\n",
    "    if predicted_label == label_int:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "print(f\"Accuracy: {correct/total}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try combining multiple runs of our highly performance prompt (i.e. same prompt different demonstration examples) to get better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demonstration_candidates = 50\n",
    "# Number of demonstrations to be used per prompt\n",
    "n_demonstrations = 10\n",
    "\n",
    "demonstration_pool = copa_data_set[0:demonstration_candidates]\n",
    "test_pool = copa_data_set[demonstration_candidates:]\n",
    "\n",
    "demonstrations = sample(demonstration_pool, n_demonstrations)\n",
    "prompts = []\n",
    "labels = []\n",
    "int_labels = []\n",
    "choices = []\n",
    "for premise, label, phrase, first_choice, second_choice in test_pool:\n",
    "    int_labels.append(label)\n",
    "    choices.append((first_choice, second_choice))\n",
    "    labels.append(create_second_prompt_label(first_choice.lower(), second_choice.lower(), label))\n",
    "    prompts.append(create_second_prompt(demonstrations, premise, phrase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihoods = []\n",
    "prompts_and_choices = pair_prompts_with_choices(list(zip(prompts, choices)))\n",
    "prompt_batches = split_prompts_into_batches(prompts_and_choices)\n",
    "for batch_number, prompt_batch in enumerate(prompt_batches):\n",
    "    activations = model.get_activations(prompt_batch, [], generation_config)\n",
    "    print(f\"Batch number {batch_number+1} Complete\")\n",
    "    # Log probs stores all of the activations associated with the input prompt (which has been completed with one of\n",
    "    # the two sentences)\n",
    "    for logprobs, tokens in zip(activations.logprobs, activations.tokens):\n",
    "        index = list(reversed(tokens)).index(\"\\n\") - 1\n",
    "        likelihoods.append(sum(logprobs[-index:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_4_pred_labels, prompt_4_pred_probs = post_process_likelihoods_to_labels(likelihoods)\n",
    "total = 0\n",
    "correct = 0\n",
    "for predicted_label, label_int in zip(prompt_4_pred_labels, int_labels):\n",
    "    if predicted_label == label_int:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "print(f\"Accuracy: {correct/total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demonstration_candidates = 50\n",
    "# Number of demonstrations to be used per prompt\n",
    "n_demonstrations = 10\n",
    "\n",
    "demonstration_pool = copa_data_set[0:demonstration_candidates]\n",
    "test_pool = copa_data_set[demonstration_candidates:]\n",
    "\n",
    "demonstrations = sample(demonstration_pool, n_demonstrations)\n",
    "prompts = []\n",
    "labels = []\n",
    "int_labels = []\n",
    "choices = []\n",
    "for premise, label, phrase, first_choice, second_choice in test_pool:\n",
    "    int_labels.append(label)\n",
    "    choices.append((first_choice, second_choice))\n",
    "    labels.append(create_second_prompt_label(first_choice.lower(), second_choice.lower(), label))\n",
    "    prompts.append(create_second_prompt(demonstrations, premise, phrase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihoods = []\n",
    "prompts_and_choices = pair_prompts_with_choices(list(zip(prompts, choices)))\n",
    "prompt_batches = split_prompts_into_batches(prompts_and_choices)\n",
    "for batch_number, prompt_batch in enumerate(prompt_batches):\n",
    "    activations = model.get_activations(prompt_batch, [], generation_config)\n",
    "    print(f\"Batch number {batch_number+1} Complete\")\n",
    "    # Log probs stores all of the activations associated with the input prompt (which has been completed with one of\n",
    "    # the two sentences)\n",
    "    for logprobs, tokens in zip(activations.logprobs, activations.tokens):\n",
    "        index = list(reversed(tokens)).index(\"\\n\") - 1\n",
    "        likelihoods.append(sum(logprobs[-index:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_5_pred_labels, prompt_5_pred_probs = post_process_likelihoods_to_labels(likelihoods)\n",
    "total = 0\n",
    "correct = 0\n",
    "for predicted_label, label_int in zip(prompt_5_pred_labels, int_labels):\n",
    "    if predicted_label == label_int:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "print(f\"Accuracy: {correct/total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "weighting = torch.tensor([1.0, 1.0, 1.0]).reshape(-1, 1)\n",
    "for probs_3, probs_4, probs_5, label_int in zip(\n",
    "    prompt_3_pred_probs, prompt_4_pred_probs, prompt_5_pred_probs, int_labels\n",
    "):\n",
    "    probs = torch.sum(torch.stack((probs_3, probs_4, probs_5)) * weighting, dim=0) / sum(weighting)\n",
    "    predicted_label = torch.argmax(probs)\n",
    "    if predicted_label == label_int:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "print(f\"Accuracy: {correct/total}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Booststrap\" Ensembling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than designing new prompts. Let's consider the effect of using the same prompt, but generating multiple responses that are then used for voting. We'll work with a smaller dataset to reduce the number of generations required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = client.load_model(\"OPT-175B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_prompts_n_times(n_repeats: int, prompts: List[str]) -> List[str]:\n",
    "    repeated_prompts: List[str] = []\n",
    "    for prompt in prompts:\n",
    "        for i in range(n_repeats):\n",
    "            repeated_prompts.append(prompt)\n",
    "    return repeated_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many of the initial data points should be reserved for demonstrations\n",
    "demonstration_candidates = 50\n",
    "# Number of demonstrations to be used per prompt\n",
    "n_demonstrations = 10\n",
    "# Number of repeats of an example\n",
    "n_repeats = 5\n",
    "\n",
    "demonstration_pool = copa_data_set[0:demonstration_candidates]\n",
    "test_pool = copa_data_set[demonstration_candidates : demonstration_candidates + 50]\n",
    "demonstrations = sample(demonstration_pool, n_demonstrations)\n",
    "prompts = []\n",
    "labels = []\n",
    "int_labels = []\n",
    "choices = []\n",
    "for premise, label, phrase, first_choice, second_choice in test_pool:\n",
    "    int_labels.append(label)\n",
    "    choices.append((first_choice, second_choice))\n",
    "    labels.append(create_first_prompt_label(first_choice.lower(), second_choice.lower(), label))\n",
    "    prompts.append(create_first_prompt(demonstrations, premise, phrase, first_choice, second_choice))\n",
    "repeated_prompts = repeat_prompts_n_times(n_repeats, prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = []\n",
    "prompt_batches = split_prompts_into_batches(repeated_prompts, batch_size=10)\n",
    "for batch_number, prompt_batch in enumerate(prompt_batches):\n",
    "    generations = model.generate(prompt_batch, generation_config)\n",
    "    print(f\"Batch number {batch_number+1} Complete\")\n",
    "    responses.extend(process_generation_text(generations.generation[\"text\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompts are run through the model 5 times to sample response generations. So we gather them and use basic voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_examples_and_vote(n_repeats: int, responses: List[str], choices: List[Tuple[str, str]]) -> List[int]:\n",
    "    # Ensure divisibilty by repeats.\n",
    "    assert len(responses) % n_repeats == 0\n",
    "    # Gather the responses and vote on a label. We return a single label prediction for each response\n",
    "    predicted_labels = []\n",
    "    responses_processed = 0\n",
    "    grouped_responses = [responses[x : x + n_repeats] for x in range(0, len(responses), n_repeats)]\n",
    "    for response_group, (first_choice, second_choice) in zip(grouped_responses, choices):\n",
    "        if responses_processed % 10 == 0:\n",
    "            print(f\"Processed {responses_processed} Response Groups\")\n",
    "        group_labels = [\n",
    "            score_response_via_rouge(response, first_choice, second_choice, rouge_metric)\n",
    "            for response in response_group\n",
    "        ]\n",
    "        votes = sum(group_labels)\n",
    "        predicted_label = 0 if votes < 2.5 else 1\n",
    "        predicted_labels.append(predicted_label)\n",
    "        responses_processed += 1\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "predicted_labels = gather_examples_and_vote(n_repeats, responses, choices)\n",
    "for predicted_label, label_int in zip(predicted_labels, int_labels):\n",
    "    if predicted_label == label_int:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "print(f\"Accuracy: {correct/total}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opt_notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
