{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/prompt_ensembling_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "from random import sample\n",
    "from typing import List, Tuple\n",
    "\n",
    "import evaluate\n",
    "import kscope\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from evaluate import EvaluationModule\n",
    "from utils import (\n",
    "    copa_preprocessor,\n",
    "    create_first_prompt,\n",
    "    create_first_prompt_label,\n",
    "    create_second_prompt,\n",
    "    create_second_prompt_label,\n",
    "    split_prompts_into_batches,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "There is a bit of documentation on how to interact with the large models [here](https://kaleidoscope-sdk.readthedocs.io/en/latest/). The relevant github links to the SDK are [here](https://github.com/VectorInstitute/kaleidoscope-sdk) and underlying code [here](https://github.com/VectorInstitute/kaleidoscope)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we connect to the service through which we'll interact with the LLMs and see which models are avaiable to us"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish a client connection to the Kaleidoscope service\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = kscope.Client(gateway_host=\"llm.cluster.local\", gateway_port=6001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '65084219-31ef-4430-922e-fb31f219ed49',\n",
       "  'name': 'OPT-6.7B',\n",
       "  'state': 'ACTIVE'},\n",
       " {'id': '5fa88ef2-d4d9-4d78-bd47-9e6288e5c8f4',\n",
       "  'name': 'OPT-175B',\n",
       "  'state': 'ACTIVE'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.models\n",
    "client.model_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = client.load_model(\"OPT-175B\")\n",
    "# If this model is not actively running, it will get launched in the background.\n",
    "# In this case, wait until it moves into an \"ACTIVE\" state before proceeding.\n",
    "while model.state != \"ACTIVE\":\n",
    "    time.sleep(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to configure the model to generate in the way we want it to. So we set a number of important parameters. For a discussion of the configuration parameters see: `src/reference_implementations/prompting_vector_llms/CONFIG_README.md`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = {\"max_tokens\": 35, \"top_k\": 4, \"top_p\": 1.0, \"rep_penalty\": 1.2, \"temperature\": 1.0}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Balanced Choice of Plausible Alternatives (CoPA)\n",
    "\n",
    "We'll work on an updated (harder) version of the CoPA dataset. We're only going to work with a small subset of the true development set in order to expedite LLM evaluation. \n",
    "\n",
    "The task, in short, is, given a context and a premise of either cause or effect, the model must choose between two distinct sentences to determine which is the logical following sentence. An example is:\n",
    "\n",
    "From the following choices,\n",
    "1) The author faded into obscurity.\n",
    "2) It was adapted into a movie.\n",
    "\n",
    "and an __effect__ premise, which logically follows the sentence \"The book became a huge failure.\" The answer is \"The author faded into obscurity.\" You can inspect the preprocessed dataset at \n",
    "\n",
    "`src/reference_implementations/prompting_vector_llms/prompt_ensembling/resources/copa_sample.tsv`\n",
    "\n",
    "We print out some of the demonstrations that we've setup below for additional reference.\n",
    "\n",
    "__NOTE__: Construction of the prompts and some other functions that are used throughout this notebook have been pulled into a utils file \n",
    "\n",
    "`src/reference_implementations/prompting_vector_llms/prompt_ensembling/utils.py`\n",
    "\n",
    "In general, when we see an \"effect\" premise the string \", so\" is added to the phrase to be completed. If the premise is \"cause\" then then string \", because\" is added to the phrase to be completed. We strip out the ending period to improve fluency. See the demonstrations below for an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many of the initial data points should be reserved for demonstrations\n",
    "demonstration_candidates = 50\n",
    "# Number of demonstrations to be used per prompt\n",
    "n_demonstrations = 20\n",
    "\n",
    "copa_data_set = copa_preprocessor(\"resources/copa_sample.tsv\")\n",
    "\n",
    "demonstration_pool = copa_data_set[0:demonstration_candidates]\n",
    "test_pool = copa_data_set[demonstration_candidates:]\n",
    "demonstrations = sample(demonstration_pool, n_demonstrations)\n",
    "prompts: List[str] = []\n",
    "labels: List[str] = []\n",
    "int_labels: List[int] = []\n",
    "choices: List[Tuple[str, str]] = []\n",
    "for premise, label, phrase, first_choice, second_choice in test_pool:\n",
    "    int_labels.append(label)\n",
    "    choices.append((first_choice, second_choice))\n",
    "    labels.append(create_first_prompt_label(first_choice.lower(), second_choice.lower(), label))\n",
    "    prompts.append(create_first_prompt(demonstrations, premise, phrase, first_choice, second_choice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose the sentence that best completes the phrase\n",
      "\n",
      "\"i brushed against poison ivy in my yard.\" or \"i eradicated the poison ivy from my yard.\"\n",
      "My skin broke out into a rash, because i brushed against poison ivy in my yard.\n",
      "\n",
      "\"the caller waited on the line.\" or \"the caller's phone lost reception.\"\n",
      "The secretary put the caller on hold, so the caller waited on the line.\n",
      "\n",
      "\"she got her ears pierced.\" or \"she got a tattoo.\"\n",
      "The girl wanted to wear earrings, so she got her ears pierced.\n",
      "\n",
      "\"the applicant failed a background check.\" or \"the applicant had experience for the job.\"\n",
      "The executive decided to hire the applicant, because the applicant had experience for the job.\n",
      "\n",
      "\"he was tired of carrying her.\" or \"she learned to walk.\"\n",
      "The father put his daughter in her stroller, because he was tired of carrying her.\n",
      "\n",
      "\"her friend choked.\" or \"her friend fell over.\"\n",
      "The girl pushed her friend, so her friend fell over.\n",
      "\n",
      "\"her friend choked.\" or \"her friend fell over.\"\n",
      "The girl strangled her friend, so her friend choked.\n",
      "\n",
      "\"it scratched me.\" or \"i petted it.\"\n",
      "I snapped at the cat, because it scratched me.\n",
      "\n",
      "\"she threw away her napkin after eating.\" or \"she put her napkin on her lap before eating.\"\n",
      "The girl demonstrated poor etiquette, so she threw away her napkin after eating.\n",
      "\n",
      "\"it migrated for the winter.\" or \"it injured its wing.\"\n",
      "The bird couldn't fly, because it injured its wing.\n",
      "\n",
      "\"he trusted the therapist.\" or \"he disagreed with the therapist.\"\n",
      "The man revealed personal information to the therapist, because he trusted the therapist.\n",
      "\n",
      "\"the motorcyclist died.\" or \"the bridge collapsed.\"\n",
      "The truck crashed into the motorcycle on the bridge, so the motorcyclist died.\n",
      "\n",
      "\"its products received positive consumer reviews.\" or \"some of its products were manufactured defectively.\"\n",
      "The company lost money, because some of its products were manufactured defectively.\n",
      "\n",
      "\"the woman apologized.\" or \"the woman was relieved.\"\n",
      "The group was offended by the woman's faux pas, so the woman apologized.\n",
      "\n",
      "\"she was irritated with her friend's whining.\" or \"she wanted to ask her friend for a favor.\"\n",
      "The woman lavished her friend with flattery, because she wanted to ask her friend for a favor.\n",
      "\n",
      "\"the student's phone rang.\" or \"the student took notes.\"\n",
      "Everyone in the class turned to stare at the student, because the student's phone rang.\n",
      "\n",
      "\"the stack towered over the boy's head.\" or \"the blocks scattered all over the rug.\"\n",
      "The child kicked the stack of blocks, so the blocks scattered all over the rug.\n",
      "\n",
      "\"the girl applied her makeup.\" or \"the girl turned on the fan.\"\n",
      "The mirror in the bathroom fogged up, so the girl turned on the fan.\n",
      "\n",
      "\"the teacher dismissed her.\" or \"the teacher corrected her.\"\n",
      "The student misspelled the word, so the teacher corrected her.\n",
      "\n",
      "\"the surveillance camera was out of focus.\" or \"he noticed some suspicious activity.\"\n",
      "The security guard could not identify the thief, because the surveillance camera was out of focus.\n",
      "\n",
      "\"They rested.\" or \"They kissed.\"\n",
      "The couple was very tired, so  \n"
     ]
    }
   ],
   "source": [
    "print(prompts[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this prompt performs on a small sample of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_generation_text(original_texts: List[str]) -> List[str]:\n",
    "    responses = []\n",
    "    for single_generation in original_texts:\n",
    "        generation_text: List[str] = re.findall(r\".*?[.!\\?]\", single_generation)\n",
    "        response_text = generation_text[0] if len(generation_text) > 0 else single_generation\n",
    "        responses.append(response_text)\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number 1 Complete\n"
     ]
    }
   ],
   "source": [
    "# Split prompts into batches for memory management.\n",
    "n_samples_to_run = 3\n",
    "responses: List[str] = []\n",
    "prompt_batches = split_prompts_into_batches(prompts[0:n_samples_to_run])\n",
    "for batch_number, prompt_batch in enumerate(prompt_batches):\n",
    "    generations = model.generate(prompt_batch, generation_config)\n",
    "    print(f\"Batch number {batch_number+1} Complete\")\n",
    "    responses.extend(process_generation_text(generations.generation[\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:  They rested.\n",
      "Label: they rested.\n",
      "\n",
      "\n",
      "Response:   I discarded the new issue.\n",
      "Label: i discarded the new issue.\n",
      "\n",
      "\n",
      "Response:                                    \n",
      "Label: she felt self-conscious.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for response, label in zip(responses, labels[0:n_samples_to_run]):\n",
    "    print(f\"Response: {response}\\nLabel: {label}\\n\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring generated Responses for the 20-shot Prompt Above\n",
    "\n",
    "Here we consider the performance of the demonstration prompt above on our subsampling of the CoPA dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run all of the examples through the model and collect the responses into the responses list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number 1 Complete\n",
      "Batch number 2 Complete\n",
      "Batch number 3 Complete\n",
      "Batch number 4 Complete\n",
      "Batch number 5 Complete\n",
      "Batch number 6 Complete\n",
      "Batch number 7 Complete\n",
      "Batch number 8 Complete\n",
      "Batch number 9 Complete\n",
      "Batch number 10 Complete\n"
     ]
    }
   ],
   "source": [
    "responses = []\n",
    "prompt_batches = split_prompts_into_batches(prompts)\n",
    "for batch_number, prompt_batch in enumerate(prompt_batches):\n",
    "    generations = model.generate(prompt_batch, generation_config)\n",
    "    print(f\"Batch number {batch_number+1} Complete\")\n",
    "    responses.extend(process_generation_text(generations.generation[\"text\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform scoring based on the generated text, by considering the rouge score of the responses using the label as the reference. We choose between the two available choices for the logical completion of the reference phrase. The model has provided a response and we treat each choice as a reference for the ROUGE metric. We take as the model's prediction the phrase with the highest ROUGE score compared to the response text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_metric = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_response_via_rouge(\n",
    "    response: str, first_choice: str, second_choice: str, rouge_metric: EvaluationModule\n",
    ") -> int:\n",
    "    response = response.lower()\n",
    "    first_choice = first_choice.lower()\n",
    "    second_choice = second_choice.lower()\n",
    "    # Use the rouge metric to score the response against the first choice or second choice as reference\n",
    "    rouge_0 = rouge_metric.compute(predictions=[response], references=[first_choice])\n",
    "    rouge_1 = rouge_metric.compute(predictions=[response], references=[second_choice])\n",
    "    # We take the average of the unigram and bi-gram rouge scores for the first and second choice results.\n",
    "    score_0 = (rouge_0[\"rouge1\"] + rouge_0[\"rouge2\"]) / 2.0\n",
    "    score_1 = (rouge_1[\"rouge1\"] + rouge_1[\"rouge2\"]) / 2.0\n",
    "    # If the first score is larger we select the first choice\n",
    "    return 0 if score_0 > score_1 else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.56\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "for response, label_int, (first_choice, second_choice) in zip(responses, int_labels, choices):\n",
    "    predicted_label = score_response_via_rouge(response, first_choice, second_choice, rouge_metric)\n",
    "    if predicted_label == label_int:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "print(f\"Accuracy: {correct/total}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, for each prompt and the two responses, we can score the candidate responses by log likelihood and choose the higher one as our label. That is, we complete the prompt with both labels and then extract the log-likelihoods of that input text from the perspective of the model. See the comments in the code below for more details on how this is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_prompts_with_choices(prompt_batch: List[Tuple[str, Tuple[str, str]]]) -> List[str]:\n",
    "    # We want to complete our prompt with the two possible choices and score those completions using our LM.\n",
    "    prompts_with_choices = []\n",
    "    for prompt, (first_choice, second_choice) in prompt_batch:\n",
    "        prompts_with_choices.append(f\"{prompt}{first_choice}\")\n",
    "        prompts_with_choices.append(f\"{prompt}{second_choice}\")\n",
    "    return prompts_with_choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number 1 Complete\n",
      "Batch number 2 Complete\n",
      "Batch number 3 Complete\n",
      "Batch number 4 Complete\n",
      "Batch number 5 Complete\n",
      "Batch number 6 Complete\n",
      "Batch number 7 Complete\n",
      "Batch number 8 Complete\n",
      "Batch number 9 Complete\n",
      "Batch number 10 Complete\n",
      "Batch number 11 Complete\n",
      "Batch number 12 Complete\n",
      "Batch number 13 Complete\n",
      "Batch number 14 Complete\n",
      "Batch number 15 Complete\n",
      "Batch number 16 Complete\n",
      "Batch number 17 Complete\n",
      "Batch number 18 Complete\n",
      "Batch number 19 Complete\n",
      "Batch number 20 Complete\n"
     ]
    }
   ],
   "source": [
    "likelihoods = []\n",
    "prompts_and_choices = pair_prompts_with_choices(list(zip(prompts, choices)))\n",
    "# prompts and choices is now twice as long as the original prompts and choices because the prompts have been completed\n",
    "# with the two possible choices\n",
    "prompt_batches = split_prompts_into_batches(prompts_and_choices)\n",
    "for batch_number, prompt_batch in enumerate(prompt_batches):\n",
    "    activations = model.get_activations(prompt_batch, [], generation_config)\n",
    "    print(f\"Batch number {batch_number+1} Complete\")\n",
    "    # Log probs stores all of the activations associated with the input prompt (which has been completed with one of\n",
    "    # the two sentences)\n",
    "    for logprobs, tokens in zip(activations.logprobs, activations.tokens):\n",
    "        # We only really care about the logprobs associated with the sentence to be completed\n",
    "        # (i.e. not the demonstrations or the question). So search for the last endline in the tokens and only\n",
    "        # sum the logprobs from there.\n",
    "        index = list(reversed(tokens)).index(\"\\n\") - 1\n",
    "        likelihoods.append(sum(logprobs[-index:]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a log likelihood for each prompt completion corresponding to completion with the first or second potential phrase. In the code below, we pair those up and compute which has the higher likelihood between the two options. This then becomes our \"predicted\" label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_likelihoods_to_labels(likelihoods: List[float]) -> Tuple[List[int], List[torch.Tensor]]:\n",
    "    # Need to group logprobs in twos because they represent likelihoods of the two completions\n",
    "    assert len(likelihoods) % 2 == 0\n",
    "    paired_likelihoods = [likelihoods[x : x + 2] for x in range(0, len(likelihoods), 2)]\n",
    "    predicted_labels = []\n",
    "    predicted_probs = []\n",
    "    softmax = nn.Softmax(dim=0)\n",
    "    for paired_likelihood in paired_likelihoods:\n",
    "        # Paired likelihood is the logprob sum for the first and second choice together\n",
    "        distribution = softmax(torch.tensor(paired_likelihood))\n",
    "        predicted_labels.append(np.argmax(paired_likelihood, axis=0))\n",
    "        predicted_probs.append(distribution)\n",
    "    return predicted_labels, predicted_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.59\n"
     ]
    }
   ],
   "source": [
    "prompt_1_pred_labels, prompt_1_pred_probs = post_process_likelihoods_to_labels(likelihoods)\n",
    "total = 0\n",
    "correct = 0\n",
    "for predicted_label, label_int in zip(prompt_1_pred_labels, int_labels):\n",
    "    if predicted_label == label_int:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "print(f\"Accuracy: {correct/total}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Alternative Prompts\n",
    "\n",
    "Our performance above isn't great. So let's try to alternatives. \n",
    "\n",
    "1. Maybe our demonstration examples weren't very good for the prompt above. What happens if we grab new demonstration examples. Does performance improve?\n",
    "2. We try a different prompt structure. Rather than having the two options followed by the premise, why don't we just have the phrases to be completed as examples and then have a final phrase to be completed and use our completion likelihoods."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first alternative prompt will simply be a resampling of the demonstration examples. That is, we just get new demonstrations and see if performance improves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many of the initial data points should be reserved for demonstrations\n",
    "demonstration_candidates = 50\n",
    "# Number of demonstrations to be used per prompt\n",
    "n_demonstrations = 20\n",
    "\n",
    "demonstration_pool = copa_data_set[0:demonstration_candidates]\n",
    "test_pool = copa_data_set[demonstration_candidates:]\n",
    "demonstrations = sample(demonstration_pool, n_demonstrations)\n",
    "prompts = []\n",
    "labels = []\n",
    "int_labels = []\n",
    "choices = []\n",
    "for premise, label, phrase, first_choice, second_choice in test_pool:\n",
    "    int_labels.append(label)\n",
    "    choices.append((first_choice, second_choice))\n",
    "    labels.append(create_first_prompt_label(first_choice.lower(), second_choice.lower(), label))\n",
    "    prompts.append(create_first_prompt(demonstrations, premise, phrase, first_choice, second_choice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number 1 Complete\n",
      "Batch number 2 Complete\n",
      "Batch number 3 Complete\n",
      "Batch number 4 Complete\n",
      "Batch number 5 Complete\n",
      "Batch number 6 Complete\n",
      "Batch number 7 Complete\n",
      "Batch number 8 Complete\n",
      "Batch number 9 Complete\n",
      "Batch number 10 Complete\n",
      "Batch number 11 Complete\n",
      "Batch number 12 Complete\n",
      "Batch number 13 Complete\n",
      "Batch number 14 Complete\n",
      "Batch number 15 Complete\n",
      "Batch number 16 Complete\n",
      "Batch number 17 Complete\n",
      "Batch number 18 Complete\n",
      "Batch number 19 Complete\n",
      "Batch number 20 Complete\n"
     ]
    }
   ],
   "source": [
    "likelihoods = []\n",
    "prompts_and_choices = pair_prompts_with_choices(list(zip(prompts, choices)))\n",
    "# prompts and choices is now twice as long as the original prompts and choices because the prompts have been completed\n",
    "# with the two possible choices\n",
    "prompt_batches = split_prompts_into_batches(prompts_and_choices)\n",
    "for batch_number, prompt_batch in enumerate(prompt_batches):\n",
    "    activations = model.get_activations(prompt_batch, [], generation_config)\n",
    "    print(f\"Batch number {batch_number+1} Complete\")\n",
    "    # Log probs stores all of the activations associated with the input prompt (which has been completed with one of\n",
    "    # the two sentences)\n",
    "    for logprobs, tokens in zip(activations.logprobs, activations.tokens):\n",
    "        # We only really care about the logprobs associated with the sentence to be completed\n",
    "        # (i.e. not the demonstrations or the question). So search for the last endline in the tokens and only\n",
    "        # sum the logprobs from there.\n",
    "        index = list(reversed(tokens)).index(\"\\n\") - 1\n",
    "        likelihoods.append(sum(logprobs[-index:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.58\n"
     ]
    }
   ],
   "source": [
    "prompt_2_pred_labels, prompt_2_pred_probs = post_process_likelihoods_to_labels(likelihoods)\n",
    "total = 0\n",
    "correct = 0\n",
    "for predicted_label, label_int in zip(prompt_2_pred_labels, int_labels):\n",
    "    if predicted_label == label_int:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "print(f\"Accuracy: {correct/total}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second alternative prompt will simply score completion of the sentence with the possible completions. See the example below this cell for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "demonstration_candidates = 50\n",
    "# Number of demonstrations to be used per prompt\n",
    "n_demonstrations = 20\n",
    "\n",
    "demonstration_pool = copa_data_set[0:demonstration_candidates]\n",
    "test_pool = copa_data_set[demonstration_candidates:]\n",
    "\n",
    "demonstrations = sample(demonstration_pool, n_demonstrations)\n",
    "prompts = []\n",
    "labels = []\n",
    "int_labels = []\n",
    "choices = []\n",
    "for premise, label, phrase, first_choice, second_choice in test_pool:\n",
    "    int_labels.append(label)\n",
    "    choices.append((first_choice, second_choice))\n",
    "    labels.append(create_second_prompt_label(first_choice.lower(), second_choice.lower(), label))\n",
    "    prompts.append(create_second_prompt(demonstrations, premise, phrase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete the phrase with a logical phrase.\n",
      "\n",
      "My skin broke out into a rash, because i brushed against poison ivy in my yard.\n",
      "\n",
      "The fugitive hid from the police, so the fugitive remained at large.\n",
      "\n",
      "The child kicked the stack of blocks, so the blocks scattered all over the rug.\n",
      "\n",
      "The benefactor looked extremely excited, because he supported the cause behind his donation.\n",
      "\n",
      "The woman was summoned for jury duty, so she cancelled her appointments.\n",
      "\n",
      "The pants had no defects, because the pants were new.\n",
      "\n",
      "The secretary put the caller on hold, so the caller waited on the line.\n",
      "\n",
      "The woman hired a public relations consultant, because she decided to run for office.\n",
      "\n",
      "The clouds looked dark, so i brought my umbrella to work.\n",
      "\n",
      "I snapped at the cat, because it scratched me.\n",
      "\n",
      "The host cancelled the party, because she was certain she had the flu.\n",
      "\n",
      "The man revealed personal information to the therapist, because he trusted the therapist.\n",
      "\n",
      "The student forgot to do her assignment, so she made up an excuse to tell the teacher.\n",
      "\n",
      "The group was offended by the woman's faux pas, so the woman apologized.\n",
      "\n",
      "The woman exaggerated the details of the story, because she wanted attention.\n",
      "\n",
      "The bird couldn't fly, because it injured its wing.\n",
      "\n",
      "The father put his daughter in her stroller, because he was tired of carrying her.\n",
      "\n",
      "The woman asked the man to leave, because he insulted her.\n",
      "\n",
      "The security guard could not identify the thief, because the surveillance camera was out of focus.\n",
      "\n",
      "The company was seeking highly specialized talent, so it moved its headquarters to a suburban location.\n",
      "\n",
      "The couple was very tired, so \n"
     ]
    }
   ],
   "source": [
    "print(prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number 1 Complete\n",
      "Batch number 2 Complete\n",
      "Batch number 3 Complete\n",
      "Batch number 4 Complete\n",
      "Batch number 5 Complete\n",
      "Batch number 6 Complete\n",
      "Batch number 7 Complete\n",
      "Batch number 8 Complete\n",
      "Batch number 9 Complete\n",
      "Batch number 10 Complete\n",
      "Batch number 11 Complete\n",
      "Batch number 12 Complete\n",
      "Batch number 13 Complete\n",
      "Batch number 14 Complete\n",
      "Batch number 15 Complete\n",
      "Batch number 16 Complete\n",
      "Batch number 17 Complete\n",
      "Batch number 18 Complete\n",
      "Batch number 19 Complete\n",
      "Batch number 20 Complete\n"
     ]
    }
   ],
   "source": [
    "likelihoods = []\n",
    "prompts_and_choices = pair_prompts_with_choices(list(zip(prompts, choices)))\n",
    "# prompts and choices is now twice as long as the original prompts and choices because the prompts have been completed\n",
    "# with the two possible choices\n",
    "prompt_batches = split_prompts_into_batches(prompts_and_choices)\n",
    "for batch_number, prompt_batch in enumerate(prompt_batches):\n",
    "    activations = model.get_activations(prompt_batch, [], generation_config)\n",
    "    print(f\"Batch number {batch_number+1} Complete\")\n",
    "    # Log probs stores all of the activations associated with the input prompt (which has been completed with one of\n",
    "    # the two sentences)\n",
    "    for logprobs, tokens in zip(activations.logprobs, activations.tokens):\n",
    "        # We only really care about the logprobs associated with the sentence to be completed\n",
    "        # (i.e. not the demonstrations or the question). So search for the last endline in the tokens and only\n",
    "        # sum the logprobs from there.\n",
    "        index = list(reversed(tokens)).index(\"\\n\") - 1\n",
    "        likelihoods.append(sum(logprobs[-index:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.76\n"
     ]
    }
   ],
   "source": [
    "prompt_3_pred_labels, prompt_3_pred_probs = post_process_likelihoods_to_labels(likelihoods)\n",
    "total = 0\n",
    "correct = 0\n",
    "for predicted_label, label_int in zip(prompt_3_pred_labels, int_labels):\n",
    "    if predicted_label == label_int:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "print(f\"Accuracy: {correct/total}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting and Averaging Ensembles"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've collected predictions from three prompting setups, we'll ensemble the responses. We'll start with simple voting and measure accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "for vote_1, vote_2, vote_3, label_int in zip(\n",
    "    prompt_1_pred_labels, prompt_2_pred_labels, prompt_3_pred_labels, int_labels\n",
    "):\n",
    "    vote_tally = sum([vote_1, vote_2, vote_3])\n",
    "    predicted_label = 0 if vote_tally < 1.5 else 1\n",
    "    if predicted_label == label_int:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "print(f\"Accuracy: {correct/total}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above works quite poorly because we have one prompt that is much better than the other two. Voting is better when there are a lot of prompts and/or the classifiers perform similarly."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's consider averaging the label probabilities and taking the largest value. We have the likelihoods associated with each completion phrase that the model thinks makes the most sense. Let's simply take the average of all the probabilities (weighted by the `weighting` vector) and determine which phrase is the \"right\" one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.73\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "weighting = torch.tensor([1.0, 1.0, 1.5]).reshape(-1, 1)\n",
    "for probs_1, probs_2, probs_3, label_int in zip(\n",
    "    prompt_1_pred_probs, prompt_2_pred_probs, prompt_3_pred_probs, int_labels\n",
    "):\n",
    "    probs = torch.sum(torch.stack((probs_1, probs_2, probs_3)) * weighting, dim=0) / sum(weighting)\n",
    "    predicted_label = torch.argmax(probs)\n",
    "    if predicted_label == label_int:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "print(f\"Accuracy: {correct/total}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the above doesn't work very well because one of our prompts dominates in terms of accuracy. However, we can get improvement over either individual weaker prompt, if we ensemble the two through the probabilities averaging approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "weighting = torch.tensor([1.0, 1.0]).reshape(-1, 1)\n",
    "for probs_1, probs_2, label_int in zip(prompt_1_pred_probs, prompt_2_pred_probs, int_labels):\n",
    "    probs = torch.sum(torch.stack((probs_1, probs_2)) * weighting, dim=0) / sum(weighting)\n",
    "    predicted_label = torch.argmax(probs)\n",
    "    if predicted_label == label_int:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "print(f\"Accuracy: {correct/total}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensembling our High Performing Prompt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try combining multiple runs of our high performing prompt to get better performance. By multiple runs, we mean sampling new demonstrations for two new runs of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "demonstration_candidates = 50\n",
    "# Number of demonstrations to be used per prompt\n",
    "n_demonstrations = 20\n",
    "\n",
    "demonstration_pool = copa_data_set[0:demonstration_candidates]\n",
    "test_pool = copa_data_set[demonstration_candidates:]\n",
    "\n",
    "demonstrations = sample(demonstration_pool, n_demonstrations)\n",
    "prompts = []\n",
    "labels = []\n",
    "int_labels = []\n",
    "choices = []\n",
    "for premise, label, phrase, first_choice, second_choice in test_pool:\n",
    "    int_labels.append(label)\n",
    "    choices.append((first_choice, second_choice))\n",
    "    labels.append(create_second_prompt_label(first_choice.lower(), second_choice.lower(), label))\n",
    "    prompts.append(create_second_prompt(demonstrations, premise, phrase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number 1 Complete\n",
      "Batch number 2 Complete\n",
      "Batch number 3 Complete\n",
      "Batch number 4 Complete\n",
      "Batch number 5 Complete\n",
      "Batch number 6 Complete\n",
      "Batch number 7 Complete\n",
      "Batch number 8 Complete\n",
      "Batch number 9 Complete\n",
      "Batch number 10 Complete\n",
      "Batch number 11 Complete\n",
      "Batch number 12 Complete\n",
      "Batch number 13 Complete\n",
      "Batch number 14 Complete\n",
      "Batch number 15 Complete\n",
      "Batch number 16 Complete\n",
      "Batch number 17 Complete\n",
      "Batch number 18 Complete\n",
      "Batch number 19 Complete\n",
      "Batch number 20 Complete\n"
     ]
    }
   ],
   "source": [
    "likelihoods = []\n",
    "prompts_and_choices = pair_prompts_with_choices(list(zip(prompts, choices)))\n",
    "# prompts and choices is now twice as long as the original prompts and choices because the prompts have been completed\n",
    "# with the two possible choices\n",
    "prompt_batches = split_prompts_into_batches(prompts_and_choices)\n",
    "for batch_number, prompt_batch in enumerate(prompt_batches):\n",
    "    activations = model.get_activations(prompt_batch, [], generation_config)\n",
    "    print(f\"Batch number {batch_number+1} Complete\")\n",
    "    # Log probs stores all of the activations associated with the input prompt (which has been completed with one of\n",
    "    # the two sentences)\n",
    "    for logprobs, tokens in zip(activations.logprobs, activations.tokens):\n",
    "        # We only really care about the logprobs associated with the sentence to be completed\n",
    "        # (i.e. not the demonstrations or the question). So search for the last endline in the tokens and only\n",
    "        # sum the logprobs from there.\n",
    "        index = list(reversed(tokens)).index(\"\\n\") - 1\n",
    "        likelihoods.append(sum(logprobs[-index:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.77\n"
     ]
    }
   ],
   "source": [
    "prompt_4_pred_labels, prompt_4_pred_probs = post_process_likelihoods_to_labels(likelihoods)\n",
    "total = 0\n",
    "correct = 0\n",
    "for predicted_label, label_int in zip(prompt_4_pred_labels, int_labels):\n",
    "    if predicted_label == label_int:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "print(f\"Accuracy: {correct/total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "demonstration_candidates = 50\n",
    "# Number of demonstrations to be used per prompt\n",
    "n_demonstrations = 20\n",
    "\n",
    "demonstration_pool = copa_data_set[0:demonstration_candidates]\n",
    "test_pool = copa_data_set[demonstration_candidates:]\n",
    "\n",
    "demonstrations = sample(demonstration_pool, n_demonstrations)\n",
    "prompts = []\n",
    "labels = []\n",
    "int_labels = []\n",
    "choices = []\n",
    "for premise, label, phrase, first_choice, second_choice in test_pool:\n",
    "    int_labels.append(label)\n",
    "    choices.append((first_choice, second_choice))\n",
    "    labels.append(create_second_prompt_label(first_choice.lower(), second_choice.lower(), label))\n",
    "    prompts.append(create_second_prompt(demonstrations, premise, phrase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number 1 Complete\n",
      "Batch number 2 Complete\n",
      "Batch number 3 Complete\n",
      "Batch number 4 Complete\n",
      "Batch number 5 Complete\n",
      "Batch number 6 Complete\n",
      "Batch number 7 Complete\n",
      "Batch number 8 Complete\n",
      "Batch number 9 Complete\n",
      "Batch number 10 Complete\n",
      "Batch number 11 Complete\n",
      "Batch number 12 Complete\n",
      "Batch number 13 Complete\n",
      "Batch number 14 Complete\n",
      "Batch number 15 Complete\n",
      "Batch number 16 Complete\n",
      "Batch number 17 Complete\n",
      "Batch number 18 Complete\n",
      "Batch number 19 Complete\n",
      "Batch number 20 Complete\n"
     ]
    }
   ],
   "source": [
    "likelihoods = []\n",
    "prompts_and_choices = pair_prompts_with_choices(list(zip(prompts, choices)))\n",
    "# prompts and choices is now twice as long as the original prompts and choices because the prompts have been completed\n",
    "# with the two possible choices\n",
    "prompt_batches = split_prompts_into_batches(prompts_and_choices)\n",
    "for batch_number, prompt_batch in enumerate(prompt_batches):\n",
    "    activations = model.get_activations(prompt_batch, [], generation_config)\n",
    "    print(f\"Batch number {batch_number+1} Complete\")\n",
    "    # Log probs stores all of the activations associated with the input prompt (which has been completed with one of\n",
    "    # the two sentences)\n",
    "    for logprobs, tokens in zip(activations.logprobs, activations.tokens):\n",
    "        # We only really care about the logprobs associated with the sentence to be completed\n",
    "        # (i.e. not the demonstrations or the question). So search for the last endline in the tokens and only\n",
    "        # sum the logprobs from there.\n",
    "        index = list(reversed(tokens)).index(\"\\n\") - 1\n",
    "        likelihoods.append(sum(logprobs[-index:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.78\n"
     ]
    }
   ],
   "source": [
    "prompt_5_pred_labels, prompt_5_pred_probs = post_process_likelihoods_to_labels(likelihoods)\n",
    "total = 0\n",
    "correct = 0\n",
    "for predicted_label, label_int in zip(prompt_5_pred_labels, int_labels):\n",
    "    if predicted_label == label_int:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "print(f\"Accuracy: {correct/total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.76\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "weighting = torch.tensor([1.0, 1.0, 1.0]).reshape(-1, 1)\n",
    "for probs_3, probs_4, probs_5, label_int in zip(\n",
    "    prompt_3_pred_probs, prompt_4_pred_probs, prompt_5_pred_probs, int_labels\n",
    "):\n",
    "    probs = torch.sum(torch.stack((probs_3, probs_4, probs_5)) * weighting, dim=0) / sum(weighting)\n",
    "    predicted_label = torch.argmax(probs)\n",
    "    if predicted_label == label_int:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "print(f\"Accuracy: {correct/total}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately this didn't really boost our performance. However, since we're just measuring on a random sample of the CoPA dataset, It is possible that using a bigger sample would be helpful."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Booststrap\" Ensembling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than designing new prompts. Let's consider the effect of using the same prompt, but generating multiple responses that are then used for voting. We'll work with a smaller dataset to reduce the number of generations required.\n",
    "\n",
    "We fall back to the original prompt structure and the ROUGE scoring technique that we used in the first example at the start of the notebook. We sample model generations 5 times based on those demonstrations and use the ROUGE scoring approach that was covered at the beginning. The prompts are ensembled with a simple voting strategy. This works very well (though still fails to outperform the superiour prompt structure). We move from individual prompt accuracy of around `0.56` to `0.66`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = client.load_model(\"OPT-175B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_prompts_n_times(n_repeats: int, prompts: List[str]) -> List[str]:\n",
    "    repeated_prompts: List[str] = []\n",
    "    for prompt in prompts:\n",
    "        for i in range(n_repeats):\n",
    "            repeated_prompts.append(prompt)\n",
    "    return repeated_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many of the initial data points should be reserved for demonstrations\n",
    "demonstration_candidates = 50\n",
    "# Number of demonstrations to be used per prompt\n",
    "n_demonstrations = 20\n",
    "# Number of repeats of an example\n",
    "n_repeats = 5\n",
    "\n",
    "demonstration_pool = copa_data_set[0:demonstration_candidates]\n",
    "test_pool = copa_data_set[demonstration_candidates : demonstration_candidates + 50]\n",
    "demonstrations = sample(demonstration_pool, n_demonstrations)\n",
    "prompts = []\n",
    "labels = []\n",
    "int_labels = []\n",
    "choices = []\n",
    "for premise, label, phrase, first_choice, second_choice in test_pool:\n",
    "    int_labels.append(label)\n",
    "    choices.append((first_choice, second_choice))\n",
    "    labels.append(create_first_prompt_label(first_choice.lower(), second_choice.lower(), label))\n",
    "    prompts.append(create_first_prompt(demonstrations, premise, phrase, first_choice, second_choice))\n",
    "repeated_prompts = repeat_prompts_n_times(n_repeats, prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number 1 Complete\n",
      "Batch number 2 Complete\n",
      "Batch number 3 Complete\n",
      "Batch number 4 Complete\n",
      "Batch number 5 Complete\n",
      "Batch number 6 Complete\n",
      "Batch number 7 Complete\n",
      "Batch number 8 Complete\n",
      "Batch number 9 Complete\n",
      "Batch number 10 Complete\n",
      "Batch number 11 Complete\n",
      "Batch number 12 Complete\n",
      "Batch number 13 Complete\n",
      "Batch number 14 Complete\n",
      "Batch number 15 Complete\n",
      "Batch number 16 Complete\n",
      "Batch number 17 Complete\n",
      "Batch number 18 Complete\n",
      "Batch number 19 Complete\n",
      "Batch number 20 Complete\n",
      "Batch number 21 Complete\n",
      "Batch number 22 Complete\n",
      "Batch number 23 Complete\n",
      "Batch number 24 Complete\n",
      "Batch number 25 Complete\n"
     ]
    }
   ],
   "source": [
    "responses = []\n",
    "prompt_batches = split_prompts_into_batches(repeated_prompts, batch_size=10)\n",
    "for batch_number, prompt_batch in enumerate(prompt_batches):\n",
    "    generations = model.generate(prompt_batch, generation_config)\n",
    "    print(f\"Batch number {batch_number+1} Complete\")\n",
    "    responses.extend(process_generation_text(generations.generation[\"text\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompts are run through the model 5 times to sample response generations. So we gather them and use basic voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_examples_and_vote(n_repeats: int, responses: List[str], choices: List[Tuple[str, str]]) -> List[int]:\n",
    "    # Ensure divisibilty by repeats.\n",
    "    assert len(responses) % n_repeats == 0\n",
    "    # Gather the responses and vote on a label. We return a single label prediction for each response\n",
    "    predicted_labels = []\n",
    "    responses_processed = 0\n",
    "    grouped_responses = [responses[x : x + n_repeats] for x in range(0, len(responses), n_repeats)]\n",
    "    for response_group, (first_choice, second_choice) in zip(grouped_responses, choices):\n",
    "        if responses_processed % 10 == 0:\n",
    "            print(f\"Processed {responses_processed} Response Groups\")\n",
    "        group_labels = [\n",
    "            score_response_via_rouge(response, first_choice, second_choice, rouge_metric)\n",
    "            for response in response_group\n",
    "        ]\n",
    "        votes = sum(group_labels)\n",
    "        predicted_label = 0 if votes < 2.5 else 1\n",
    "        predicted_labels.append(predicted_label)\n",
    "        responses_processed += 1\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 Response Groups\n",
      "Processed 10 Response Groups\n",
      "Processed 20 Response Groups\n",
      "Processed 30 Response Groups\n",
      "Processed 40 Response Groups\n",
      "Accuracy: 0.66\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "predicted_labels = gather_examples_and_vote(n_repeats, responses, choices)\n",
    "for predicted_label, label_int in zip(predicted_labels, int_labels):\n",
    "    if predicted_label == label_int:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "print(f\"Accuracy: {correct/total}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opt_notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
