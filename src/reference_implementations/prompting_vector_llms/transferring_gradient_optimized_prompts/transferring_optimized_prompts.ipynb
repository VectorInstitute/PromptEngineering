{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import List, Tuple, Union\n",
    "\n",
    "import kscope\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "from torch import cuda\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    PreTrainedTokenizer,\n",
    "    PreTrainedTokenizerFast,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    ")\n",
    "from transformers.modeling_outputs import Seq2SeqLMOutput\n",
    "from transformers.tokenization_utils_base import BatchEncoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll perform a simple experiment. The question is:\n",
    "\n",
    "Do the discrete prompts learned to optimize T5s ability to perform the SST2 sentiment analysis task also improve performance for OPT-6.7B?\n",
    "\n",
    "Our original prompt was \"Generate the sentiment of the next sentence. \". For T5, this prompt induced about 68% accuracy. After gradient based search optimization, we ended up with the prompt \"tumour negative .05. Positive respins the Contains sentence. \" with an accuracy of 81% and \"childcare negative .05. Positive respins wSt Thank sentence.\" with an accuracy of 83%.\n",
    "\n",
    "Let's determine if either of these odd but apparently performant prompts will improve results for OPT-6.7B over the original as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Device cpu\n"
     ]
    }
   ],
   "source": [
    "initial_prompt = \"Generate the sentiment of the next sentence. \"\n",
    "optimized_prompt_1 = \"tumour negative .05. Positive respins the Contains sentence. \"\n",
    "optimized_prompt_2 = \"childcare negative .05. Positive respins wSt Thank sentence. \"\n",
    "\n",
    "opt_tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(\"google/t5-large-lm-adapt\")\n",
    "label_words = [\"negative\", \"positive\"]\n",
    "# How big should inference batches be\n",
    "batch_size = 10\n",
    "# How many batches in total to process from the dataloader (batch_size*batches_to_sample = datapoints to process)\n",
    "batches_to_sample = 20\n",
    "\n",
    "# Determine whether cuda and a GPU are available to speed up processing\n",
    "device = \"cuda\" if cuda.is_available() else \"cpu\"\n",
    "print(f\"Detected Device {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset sst2 (/Users/david/.cache/huggingface/datasets/sst2/default/2.0.0/9896208a8d85db057ac50c72282bcb8fe755accc671a57dd8059d4e130961ed5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'idx': 0,\n",
       " 'sentence': \"it 's a charming and often affecting journey . \",\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"sst2\", split=\"validation\")\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 40025, 877, 5, 5702, 9, 5, 220, 3645, 4, 38, 657, 42, 1569, 328, 2430]\n",
      "[2, 40025, 877, 5, 5702, 9, 5, 220, 3645, 4, 38, 657, 42, 1569, 328, 1313]\n",
      "[6939, 2206, 8, 6493, 13, 8, 416, 7142, 5, 27, 333, 48, 1974, 55, 2841, 1]\n",
      "[6939, 2206, 8, 6493, 13, 8, 416, 7142, 5, 27, 333, 48, 1974, 55, 1465, 1]\n"
     ]
    }
   ],
   "source": [
    "# Need to grab the token id associated with the label words for both opt and t5\n",
    "\n",
    "dummy_sentence = \"I love this movie!\"\n",
    "print(opt_tokenizer(f\"{initial_prompt}{dummy_sentence} negative\")[\"input_ids\"])\n",
    "print(opt_tokenizer(f\"{initial_prompt}{dummy_sentence} positive\")[\"input_ids\"])\n",
    "print(t5_tokenizer(f\"{initial_prompt}{dummy_sentence} negative </s>\", add_special_tokens=False)[\"input_ids\"])\n",
    "print(t5_tokenizer(f\"{initial_prompt}{dummy_sentence} positive </s>\", add_special_tokens=False)[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_label_tokens = {\"negative\": [2430], \"positive\": [1313]}\n",
    "t5_label_tokens = {\"negative\": [2841], \"positive\": [1465]}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by measuring the performance of OPT-6.7B with the initial prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'b11f3264-9c03-4114-9d56-d39a0fa63640',\n",
       "  'name': 'OPT-175B',\n",
       "  'state': 'ACTIVE'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = kscope.Client(gateway_host=\"llm.cluster.local\", gateway_port=6001)\n",
    "client.model_instances"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to configure the model to generate in the way we want it to. So we set a number of important parameters. For a discussion of the configuration parameters see: `src/reference_implementations/prompting_vector_llms/CONFIG_README.md`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = client.load_model(\"OPT-6.7B\")\n",
    "# If this model is not actively running, it will get launched in the background.\n",
    "# In this case, wait until it moves into an \"ACTIVE\" state before proceeding.\n",
    "while model.state != \"ACTIVE\":\n",
    "    time.sleep(1)\n",
    "\n",
    "short_generation_config = {\"max_tokens\": 2, \"top_k\": 4, \"top_p\": 1.0, \"rep_penalty\": 1.0, \"temperature\": 1.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'decoder.output_projection'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We're interested in the activations from the last layer of the model, because this will allow us to calculate the\n",
    "# likelihoods\n",
    "last_layer_name = model.module_names[-1]\n",
    "last_layer_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_str(instruction: str, sentences: List[str]) -> List[str]:\n",
    "    return [f\"{instruction}{sentence}\" for sentence in sentences]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the prompts we're creating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate the sentiment of the next sentence. having had the good sense to cast actors who are , generally speaking , adored by the movie-going public , khouri then gets terrific performances from them all . \n",
      "tumour negative .05. Positive respins the Contains sentence. having had the good sense to cast actors who are , generally speaking , adored by the movie-going public , khouri then gets terrific performances from them all . \n",
      "childcare negative .05. Positive respins wSt Thank sentence. having had the good sense to cast actors who are , generally speaking , adored by the movie-going public , khouri then gets terrific performances from them all . \n"
     ]
    }
   ],
   "source": [
    "example_batch = next(iter(dataloader))\n",
    "example_prompts_initial = create_prompt_str(initial_prompt, example_batch[\"sentence\"])\n",
    "example_prompts_optimized_1 = create_prompt_str(optimized_prompt_1, example_batch[\"sentence\"])\n",
    "example_prompts_optimized_2 = create_prompt_str(optimized_prompt_2, example_batch[\"sentence\"])\n",
    "print(example_prompts_initial[0])\n",
    "print(example_prompts_optimized_1[0])\n",
    "print(example_prompts_optimized_2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_label_from_activations_opt(label_token_ids: torch.Tensor, layer_matrix: torch.Tensor) -> int:\n",
    "    # The activations we care about are the last token (corresponding to our label token) and the values for our label\n",
    "    #  vocabulary\n",
    "    label_activations = layer_matrix[-1][label_token_ids].float()\n",
    "    softmax = nn.Softmax(dim=0)\n",
    "    # Softmax is not strictly necessary, but it helps to contextualize the \"probability\" the model associates with each\n",
    "    # label relative to the others\n",
    "    label_distributions = softmax(label_activations)\n",
    "    # We select the label index with the largest value\n",
    "    max_label_index = torch.argmax(label_distributions)\n",
    "    return max_label_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "report: List[Tuple[str, float]] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number 1 Complete\n",
      "Batch number 2 Complete\n",
      "Batch number 3 Complete\n",
      "Batch number 4 Complete\n",
      "Batch number 5 Complete\n",
      "Batch number 6 Complete\n",
      "Batch number 7 Complete\n",
      "Batch number 8 Complete\n",
      "Batch number 9 Complete\n",
      "Batch number 10 Complete\n",
      "Batch number 11 Complete\n",
      "Batch number 12 Complete\n",
      "Batch number 13 Complete\n",
      "Batch number 14 Complete\n",
      "Batch number 15 Complete\n",
      "Batch number 16 Complete\n",
      "Batch number 17 Complete\n",
      "Batch number 18 Complete\n",
      "Batch number 19 Complete\n",
      "Batch number 20 Complete\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "label_token_ids = torch.Tensor([opt_label_tokens[\"negative\"], opt_label_tokens[\"positive\"]]).long()\n",
    "for batch_num, batch in enumerate(dataloader):\n",
    "    prompts = create_prompt_str(initial_prompt, batch[\"sentence\"])\n",
    "    labels = batch[\"label\"]\n",
    "    activations = model.get_activations(prompts, [last_layer_name], short_generation_config)\n",
    "    print(f\"Batch number {batch_num+1} Complete\")\n",
    "    for activations_single_prompt, label in zip(activations.activations, labels):\n",
    "        last_layer_matrix = activations_single_prompt[last_layer_name]\n",
    "        predicted_label = select_label_from_activations_opt(label_token_ids, last_layer_matrix)\n",
    "        if predicted_label == int(label.item()):\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    if batch_num + 1 == batches_to_sample:\n",
    "        break\n",
    "accuracy = correct / total\n",
    "report.append((initial_prompt, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try both of our \"optimized prompts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number 1 Complete\n",
      "Batch number 2 Complete\n",
      "Batch number 3 Complete\n",
      "Batch number 4 Complete\n",
      "Batch number 5 Complete\n",
      "Batch number 6 Complete\n",
      "Batch number 7 Complete\n",
      "Batch number 8 Complete\n",
      "Batch number 9 Complete\n",
      "Batch number 10 Complete\n",
      "Batch number 11 Complete\n",
      "Batch number 12 Complete\n",
      "Batch number 13 Complete\n",
      "Batch number 14 Complete\n",
      "Batch number 15 Complete\n",
      "Batch number 16 Complete\n",
      "Batch number 17 Complete\n",
      "Batch number 18 Complete\n",
      "Batch number 19 Complete\n",
      "Batch number 20 Complete\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "label_token_ids = torch.Tensor([opt_label_tokens[\"negative\"], opt_label_tokens[\"positive\"]]).long()\n",
    "for batch_num, batch in enumerate(dataloader):\n",
    "    prompts = create_prompt_str(optimized_prompt_1, batch[\"sentence\"])\n",
    "    labels = batch[\"label\"]\n",
    "    activations = model.get_activations(prompts, [last_layer_name], short_generation_config)\n",
    "    print(f\"Batch number {batch_num+1} Complete\")\n",
    "    for activations_single_prompt, label in zip(activations.activations, labels):\n",
    "        last_layer_matrix = activations_single_prompt[last_layer_name]\n",
    "        predicted_label = select_label_from_activations_opt(label_token_ids, last_layer_matrix)\n",
    "        if predicted_label == int(label.item()):\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    if batch_num + 1 == batches_to_sample:\n",
    "        break\n",
    "accuracy = correct / total\n",
    "report.append((optimized_prompt_1, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.555\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number 1 Complete\n",
      "Batch number 2 Complete\n",
      "Batch number 3 Complete\n",
      "Batch number 4 Complete\n",
      "Batch number 5 Complete\n",
      "Batch number 6 Complete\n",
      "Batch number 7 Complete\n",
      "Batch number 8 Complete\n",
      "Batch number 9 Complete\n",
      "Batch number 10 Complete\n",
      "Batch number 11 Complete\n",
      "Batch number 12 Complete\n",
      "Batch number 13 Complete\n",
      "Batch number 14 Complete\n",
      "Batch number 15 Complete\n",
      "Batch number 16 Complete\n",
      "Batch number 17 Complete\n",
      "Batch number 18 Complete\n",
      "Batch number 19 Complete\n",
      "Batch number 20 Complete\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "label_token_ids = torch.Tensor([opt_label_tokens[\"negative\"], opt_label_tokens[\"positive\"]]).long()\n",
    "for batch_num, batch in enumerate(dataloader):\n",
    "    prompts = create_prompt_str(optimized_prompt_2, batch[\"sentence\"])\n",
    "    labels = batch[\"label\"]\n",
    "    activations = model.get_activations(prompts, [last_layer_name], short_generation_config)\n",
    "    print(f\"Batch number {batch_num+1} Complete\")\n",
    "    for activations_single_prompt, label in zip(activations.activations, labels):\n",
    "        last_layer_matrix = activations_single_prompt[last_layer_name]\n",
    "        predicted_label = select_label_from_activations_opt(label_token_ids, last_layer_matrix)\n",
    "        if predicted_label == int(label.item()):\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    if batch_num + 1 == batches_to_sample:\n",
    "        break\n",
    "accuracy = correct / total\n",
    "report.append((optimized_prompt_2, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.615\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HugggingFace T5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try these prompts in the context of the original T5 model from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model and set it to eval mode\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(\"google/t5-large-lm-adapt\").to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_encoder_decoder_inputs(\n",
    "    prompts: List[str], t5_tokenizer: Union[PreTrainedTokenizer, PreTrainedTokenizerFast], device: str\n",
    ") -> Tuple[BatchEncoding, BatchEncoding]:\n",
    "    # Repeat each prompt twice (once for each label)\n",
    "    repeated_prompts = [prompt for prompt in prompts for i in range(2)]\n",
    "    # repeat label words for each repeated prompt\n",
    "    decoder_labels = [f\"{label_word} </s>\" for label_word in label_words] * len(prompts)\n",
    "    encoder_inputs = t5_tokenizer(\n",
    "        repeated_prompts,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=64,\n",
    "        add_special_tokens=False,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(device)\n",
    "    decoder_inputs = t5_tokenizer(\n",
    "        decoder_labels,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=16,\n",
    "        add_special_tokens=False,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(device)\n",
    "    return encoder_inputs, decoder_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_likelihoods_from_t5_ouput(\n",
    "    output: Seq2SeqLMOutput, loss_func: torch.nn.CrossEntropyLoss, decoder_ids: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    loss_func = torch.nn.CrossEntropyLoss(ignore_index=-100, reduction=\"none\")\n",
    "    # Negative of the loss to get back to raw log-probabilities\n",
    "    log_likelihoods = -loss_func(output.logits.view(-1, output.logits.size(-1)), decoder_ids.view(-1))\n",
    "    batch_size, sequence_length, _ = output.logits.size()\n",
    "    # compute per-token log probability in a sequence.\n",
    "    # log_p has log probabilities for the following target output: [pos, it, ive]\n",
    "    log_likelihoods = log_likelihoods.view(batch_size, sequence_length)\n",
    "    # pad tokens have index -100 in huggingface.\n",
    "    good_log_p = log_likelihoods.masked_fill_(decoder_ids == -100, 0.0)\n",
    "    # good_log_p now has the log probability of the output sequence tokens.\n",
    "    # sum over the sequence length to compute the log probability for a full sequence.\n",
    "    return torch.sum(good_log_p, dim=1).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_t5_model_on_encodings(\n",
    "    encoder_encodings: BatchEncoding, decoder_encodings: BatchEncoding, t5_model: T5ForConditionalGeneration\n",
    ") -> Tuple[Seq2SeqLMOutput, torch.Tensor]:\n",
    "    decoder_ids = decoder_encodings.input_ids\n",
    "    # we have to make sure that the PAD token is ignored.\n",
    "    # huggingface ignores a pad token if the token is -100!\n",
    "    decoder_ids = decoder_ids.masked_fill(decoder_ids == t5_tokenizer.pad_token_id, -100)\n",
    "    # Disable gradient tracking for faster inference\n",
    "    with torch.no_grad():\n",
    "        model_output = t5_model(\n",
    "            input_ids=encoder_encodings.input_ids,\n",
    "            attention_mask=encoder_encodings.attention_mask,\n",
    "            decoder_attention_mask=decoder_encodings.attention_mask,\n",
    "            decoder_input_ids=t5_model._shift_right(decoder_ids),\n",
    "            labels=None,\n",
    "        )\n",
    "    return model_output, decoder_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_label_from_likelihoods(softmax_func: nn.Softmax, likelihoods: torch.Tensor) -> torch.tensor:\n",
    "    # Pair the likelihoods associated with negative and positive labels for each prompt\n",
    "    likelihoods = likelihoods.reshape(-1, 2)\n",
    "    likelihoods = softmax_func(likelihoods)\n",
    "    return torch.argmax(likelihoods, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number 1 Complete\n",
      "Batch number 2 Complete\n",
      "Batch number 3 Complete\n",
      "Batch number 4 Complete\n",
      "Batch number 5 Complete\n",
      "Batch number 6 Complete\n",
      "Batch number 7 Complete\n",
      "Batch number 8 Complete\n",
      "Batch number 9 Complete\n",
      "Batch number 10 Complete\n",
      "Batch number 11 Complete\n",
      "Batch number 12 Complete\n",
      "Batch number 13 Complete\n",
      "Batch number 14 Complete\n",
      "Batch number 15 Complete\n",
      "Batch number 16 Complete\n",
      "Batch number 17 Complete\n",
      "Batch number 18 Complete\n",
      "Batch number 19 Complete\n",
      "Batch number 20 Complete\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# We're going to use a loss function to extra the log probabilties of the labels.\n",
    "loss_func = nn.CrossEntropyLoss(ignore_index=-100, reduction=\"none\")\n",
    "softmax = nn.Softmax(dim=1)\n",
    "for batch_num, batch in enumerate(dataloader):\n",
    "    prompts = [f\"{prompt} </s>\" for prompt in create_prompt_str(initial_prompt, batch[\"sentence\"])]\n",
    "    labels = batch[\"label\"].to(device)\n",
    "    encoder_encodings, decoder_encodings = create_encoder_decoder_inputs(prompts, t5_tokenizer, device)\n",
    "\n",
    "    model_output, decoder_ids = run_t5_model_on_encodings(encoder_encodings, decoder_encodings, t5_model)\n",
    "    likelihoods = get_likelihoods_from_t5_ouput(model_output, loss_func, decoder_ids)\n",
    "    predicted_labels = extract_label_from_likelihoods(softmax, likelihoods)\n",
    "    match_tensor = (predicted_labels == labels).long()\n",
    "    correct += torch.sum(match_tensor)\n",
    "    total += len(match_tensor)\n",
    "    print(f\"Batch number {batch_num+1} Complete\")\n",
    "    if batch_num + 1 == batches_to_sample:\n",
    "        break\n",
    "accuracy = correct / total\n",
    "\n",
    "report.append((initial_prompt, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6549999713897705\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number 1 Complete\n",
      "Batch number 2 Complete\n",
      "Batch number 3 Complete\n",
      "Batch number 4 Complete\n",
      "Batch number 5 Complete\n",
      "Batch number 6 Complete\n",
      "Batch number 7 Complete\n",
      "Batch number 8 Complete\n",
      "Batch number 9 Complete\n",
      "Batch number 10 Complete\n",
      "Batch number 11 Complete\n",
      "Batch number 12 Complete\n",
      "Batch number 13 Complete\n",
      "Batch number 14 Complete\n",
      "Batch number 15 Complete\n",
      "Batch number 16 Complete\n",
      "Batch number 17 Complete\n",
      "Batch number 18 Complete\n",
      "Batch number 19 Complete\n",
      "Batch number 20 Complete\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# We're going to use a loss function to extra the log probabilties of the labels.\n",
    "loss_func = nn.CrossEntropyLoss(ignore_index=-100, reduction=\"none\")\n",
    "softmax = nn.Softmax(dim=1)\n",
    "for batch_num, batch in enumerate(dataloader):\n",
    "    prompts = [f\"{prompt} </s>\" for prompt in create_prompt_str(optimized_prompt_1, batch[\"sentence\"])]\n",
    "    labels = batch[\"label\"].to(device)\n",
    "    encoder_encodings, decoder_encodings = create_encoder_decoder_inputs(prompts, t5_tokenizer, device)\n",
    "    model_output, decoder_ids = run_t5_model_on_encodings(encoder_encodings, decoder_encodings, t5_model)\n",
    "    likelihoods = get_likelihoods_from_t5_ouput(model_output, loss_func, decoder_ids)\n",
    "    predicted_labels = extract_label_from_likelihoods(softmax, likelihoods)\n",
    "    match_tensor = (predicted_labels == labels).long()\n",
    "    correct += torch.sum(match_tensor)\n",
    "    total += len(match_tensor)\n",
    "    print(f\"Batch number {batch_num+1} Complete\")\n",
    "    if batch_num + 1 == batches_to_sample:\n",
    "        break\n",
    "accuracy = correct / total\n",
    "report.append((optimized_prompt_1, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7900000214576721\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number 1 Complete\n",
      "Batch number 2 Complete\n",
      "Batch number 3 Complete\n",
      "Batch number 4 Complete\n",
      "Batch number 5 Complete\n",
      "Batch number 6 Complete\n",
      "Batch number 7 Complete\n",
      "Batch number 8 Complete\n",
      "Batch number 9 Complete\n",
      "Batch number 10 Complete\n",
      "Batch number 11 Complete\n",
      "Batch number 12 Complete\n",
      "Batch number 13 Complete\n",
      "Batch number 14 Complete\n",
      "Batch number 15 Complete\n",
      "Batch number 16 Complete\n",
      "Batch number 17 Complete\n",
      "Batch number 18 Complete\n",
      "Batch number 19 Complete\n",
      "Batch number 20 Complete\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# We're going to use a loss function to extra the log probabilties of the labels.\n",
    "loss_func = nn.CrossEntropyLoss(ignore_index=-100, reduction=\"none\")\n",
    "softmax = nn.Softmax(dim=1)\n",
    "for batch_num, batch in enumerate(dataloader):\n",
    "    prompts = [f\"{prompt} </s>\" for prompt in create_prompt_str(optimized_prompt_2, batch[\"sentence\"])]\n",
    "    labels = batch[\"label\"].to(device)\n",
    "    encoder_encodings, decoder_encodings = create_encoder_decoder_inputs(prompts, t5_tokenizer, device)\n",
    "    model_output, decoder_ids = run_t5_model_on_encodings(encoder_encodings, decoder_encodings, t5_model)\n",
    "    likelihoods = get_likelihoods_from_t5_ouput(model_output, loss_func, decoder_ids)\n",
    "    predicted_labels = extract_label_from_likelihoods(softmax, likelihoods)\n",
    "    match_tensor = (predicted_labels == labels).long()\n",
    "    correct += torch.sum(match_tensor)\n",
    "    total += len(match_tensor)\n",
    "    print(f\"Batch number {batch_num+1} Complete\")\n",
    "    if batch_num + 1 == batches_to_sample:\n",
    "        break\n",
    "accuracy = correct / total\n",
    "report.append((optimized_prompt_2, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7549999952316284\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary\n",
      "OPT Performance:\n",
      "Prompt: Generate the sentiment of the next sentence. , Accuracy: 0.6\n",
      "Prompt: tumour negative .05. Positive respins the Contains sentence. , Accuracy: 0.555\n",
      "Prompt: childcare negative .05. Positive respins wSt Thank sentence. , Accuracy: 0.615\n",
      "T5 Performance:\n",
      "Prompt: Generate the sentiment of the next sentence. , Accuracy: 0.6549999713897705\n",
      "Prompt: tumour negative .05. Positive respins the Contains sentence. , Accuracy: 0.7900000214576721\n",
      "Prompt: childcare negative .05. Positive respins wSt Thank sentence. , Accuracy: 0.7549999952316284\n"
     ]
    }
   ],
   "source": [
    "print(\"Summary\")\n",
    "print(\"OPT Performance:\")\n",
    "for prompt, acc in report[0:3]:\n",
    "    print(f\"Prompt: {prompt}, Accuracy: {acc}\")\n",
    "print(\"T5 Performance:\")\n",
    "for prompt, acc in report[3:6]:\n",
    "    print(f\"Prompt: {prompt}, Accuracy: {acc}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's fairly clear that OPT does not do well with this type of prompt, whereas T5 does a pretty good job with this instruction prompt.\n",
    "\n",
    "The amazing part is that these weird prompts seem to improve the performance of T5, but also possibly the performance of OPT a little bit!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opt_notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
