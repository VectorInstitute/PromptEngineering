{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fcafc0ad",
   "metadata": {},
   "source": [
    "Note: If the pipeline is taking too long to load, consider allocating a SLURM job via command line with more CPU memory:\n",
    "```\n",
    "salloc --gres=gpu:2 --qos=high --partition=t4v2 --mem=64G\n",
    "\n",
    "```\n",
    "\n",
    "At a minimum, it is best to allocate at least 32GB of CPU memory to load this model\n",
    "\n",
    "## Prompting Instruction Fine-tuned language models\n",
    "Language Models such as OPT and LLaMA are trained to do next-token prediction (autocompletion). As a result, they might easily miss the fact that we expect it to answer our question instead of providing a reasonable (but not very helpful) \"autocompletion\" to our query. One way to steer the LM away from this behavior is instruction fine-tuning.\n",
    "\n",
    "In this notebook, you will find examples on how to work with instruction fine-tuned (IFT) LLMs, and how they can help simplify your prompt design workflow. \n",
    "\n",
    "Overall steps for calling an IFT model can include:\n",
    "- Loading the model- either locally as a HuggingFace pipeline, or using a remote model API as in the Kaleidoscope examples;\n",
    "- Pre-processing your text query. You might need to add special tokens to your input to achieve full instruction fine-tuning potentials; And,\n",
    "- Extracting structured information from the model output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1155f0e2",
   "metadata": {},
   "source": [
    "To speed things up, we've cached weights of a number of IFT models on the Vector cluster. If you prefer, you may also download these weights directly from the HuggingFace Hub:\n",
    "- Koala-7B: https://huggingface.co/TheBloke/koala-7B-HF\n",
    "- Alpaca-7B: https://huggingface.co/chavinlo/alpaca-native"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687c34db",
   "metadata": {},
   "source": [
    "## Koala IFT model\n",
    "Similar to Alpaca, the [Koala](https://bair.berkeley.edu/blog/2023/04/03/koala/) model is an instruction fine-tuned version of Facebook AI's LLaMA LLM. \n",
    "\n",
    "The following example is based on the 7B version of Koala (13GB at 16-bit).\n",
    "\n",
    "Note that initializing the pipeline might take a while- around five minutes for a 7B model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "concerned-conducting",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9e0e0bdc15a4030859447a87b2fe04a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "MODEL_PATH = \"/ssd005/projects/llm/koala-7b\"\n",
    "generator = pipeline(\"text-generation\", model=MODEL_PATH, torch_dtype=torch.float16, device=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0a0727",
   "metadata": {},
   "source": [
    "### Pre-process Query\n",
    "To build an IFT LLM, we fine-tune the base model (e.g., OPT, LLaMA) on demonstrations where the LLM needs to produce outputs that follow human instructions. To remind the model about its role as the \"assistant\" and not the \"human\", we would add special separator tokens between the human instruction and what the model needs to generate. \n",
    "\n",
    "For example, in the Koala model, the authors added the following separators between instructions and demonstrations:\n",
    "\n",
    "- Add \"BEGINNING OF CONVERSATION: \" to the beginning of each conversation,\n",
    "- Add \"USER: \" before each human input,\n",
    "- Add \"GPT: \" after the human query, and\n",
    "- Add \"\\</s\\>\" (a special token) to the end of each LM output.\n",
    "\n",
    "During inference, we would need to add the same set of tokens to make the most out of the instruction fine-tuned model. \n",
    "\n",
    "Refer to Koala's [documentation](https://github.com/young-geng/EasyLM/blob/main/docs/koala.md) for more details.\n",
    "\n",
    "Google's [blog post](https://ai.googleblog.com/2021/10/introducing-flan-more-generalizable.html) on FLAN provides additional detail on how Instruction Fine-Tuning can potentially improve the zero-shot performance of LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51a3f674",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_koala_input(user_input: str) -> str:\n",
    "    return \"BEGINNING OF CONVERSATION: USER: \" + user_input + \" GPT:\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65beb5e1",
   "metadata": {},
   "source": [
    "### Run pre-processed query through the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "herbal-failing",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"We had a great experience at the restaurant, food was delicious, but the service was kinda bad.             \n",
    "\n",
    "What is the sentiment on the restaurant, positive or negative? Explain your reasoning.\"\"\"  # noqa: W291\n",
    "\n",
    "text_input = preprocess_koala_input(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "117990f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input to model: \n",
      "###\n",
      "BEGINNING OF CONVERSATION: USER: We had a great experience at the restaurant, food was delicious, but the service was kinda bad.             \n",
      "\n",
      "What is the sentiment on the restaurant, positive or negative? Explain your reasoning. GPT:\n",
      "###\n",
      "Pipeline output: \n",
      "###\n",
      "[{'generated_text': 'BEGINNING OF CONVERSATION: USER: We had a great experience at the restaurant, food was delicious, but the service was kinda bad.             \\n\\nWhat is the sentiment on the restaurant, positive or negative? Explain your reasoning. GPT: Based on the information provided, it seems that the sentiment towards the restaurant is mostly positive, with the customer mentioning that the food was delicious. However, the customer also mentions that the service was not as good as they expected. This suggests that the customer had a positive experience with the food, but was disappointed with the service.\\n\\nTherefore, the sentiment towards the restaurant is mostly positive, but with a slight negative tone.'}]\n",
      "###\n"
     ]
    }
   ],
   "source": [
    "print(\"Input to model: \\n###\\n\" + text_input + \"\\n###\")\n",
    "\n",
    "hf_pipeline_output = generator(text_input, max_new_tokens=256)\n",
    "\n",
    "print(\"Pipeline output: \\n###\\n\" + str(hf_pipeline_output) + \"\\n###\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53eb75d",
   "metadata": {},
   "source": [
    "### Extract useful info from model output\n",
    "There are many ways to extract structural information (e.g., binary label for sentiment: positive/label) from the natural text output of the language model. \n",
    "\n",
    "Since the focus of this notebook is on prompting instruction fine-tuned models, we will demonstrate only a basic example where we delete previous input to the model and keep only the newly-generated tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "048ce9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_koala_response(model_output: str, previous_input: str) -> str:\n",
    "    return model_output.replace(previous_input, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51d90293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted output: \n",
      "###\n",
      " Based on the information provided, it seems that the sentiment towards the restaurant is mostly positive, with the customer mentioning that the food was delicious. However, the customer also mentions that the service was not as good as they expected. This suggests that the customer had a positive experience with the food, but was disappointed with the service.\n",
      "\n",
      "Therefore, the sentiment towards the restaurant is mostly positive, but with a slight negative tone.\n",
      "###\n"
     ]
    }
   ],
   "source": [
    "model_output = hf_pipeline_output[0][\"generated_text\"]\n",
    "extracted_output = extract_koala_response(model_output, previous_input=text_input)\n",
    "\n",
    "print(\"Extracted output: \\n###\\n\" + extracted_output + \"\\n###\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompt_engineering",
   "language": "python",
   "name": "prompt_engineering"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
