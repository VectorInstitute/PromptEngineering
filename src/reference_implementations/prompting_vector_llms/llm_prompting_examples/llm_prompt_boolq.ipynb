{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from random import sample\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import kscope\n",
    "import pandas as pd\n",
    "from metrics import report_metrics\n",
    "from transformers import AutoTokenizer\n",
    "from utils import get_label_token_ids, get_label_with_highest_likelihood, split_prompts_into_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "There is a bit of documentation on how to interact with the large models [here](https://kaleidoscope-sdk.readthedocs.io/en/latest/). The relevant github links to the SDK are [here](https://github.com/VectorInstitute/kaleidoscope-sdk) and underlying code [here](https://github.com/VectorInstitute/kaleidoscope)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we connect to the service through which we'll interact with the LLMs and see which models are avaiable to us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a client connection to the kscope service\n",
    "client = kscope.Client(gateway_host=\"llm.cluster.local\", gateway_port=6001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show all supported models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show all model instances that are currently active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.model_instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we obtain a handle to a model. In this example, let's use the OPT-175B model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = client.load_model(\"OPT-175B\")\n",
    "# If this model is not actively running, it will get launched in the background.\n",
    "# In this case, wait until it moves into an \"ACTIVE\" state before proceeding.\n",
    "while model.state != \"ACTIVE\":\n",
    "    time.sleep(1)\n",
    "\n",
    "print(\"The model is active!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to configure the model to generate in the way we want it to. So we set a number of important parameters. For a discussion of the configuration parameters see: `src/reference_implementations/prompting_vector_llms/CONFIG_README.md`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_generation_config = {\"max_tokens\": 2, \"top_k\": 4, \"top_p\": 1.0, \"rep_penalty\": 1.2, \"temperature\": 1.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to have the model attempt to answer questions based on some context. The answer to each question is either true or false. We'll compare zero and few-shot prompts along with two different label spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boolq_preprocessor(path: str) -> Tuple[List[str], List[str], List[str], List[int]]:\n",
    "    boolq_df = pd.read_csv(path)\n",
    "    titles = boolq_df[\"Title\"].tolist()\n",
    "    passages = boolq_df[\"Passage\"].tolist()\n",
    "    questions = boolq_df[\"Question\"].tolist()\n",
    "    labels = boolq_df[\"Answer\"].apply(lambda x: 1 if x else 0).tolist()\n",
    "    return titles, passages, questions, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in a sampling of the BoolQ test dataset and a small sample of training examples from the training dataset for\n",
    "# few-shot prompting\n",
    "bool_q_test_titles, bool_q_test_passages, bool_q_test_questions, bool_q_test_labels = boolq_preprocessor(\n",
    "    \"resources/boolq_task_datasets/test_sample_dataset.csv\"\n",
    ")\n",
    "bool_q_train_titles, bool_q_train_passages, bool_q_train_questions, bool_q_train_labels = boolq_preprocessor(\n",
    "    \"resources/boolq_task_datasets/example_dataset.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In creating prompts, demonstrations are used for few-shot examples. If demonstrations in the `create_prompts` function is an empty string then the prompt is zero shot (that is, it includes no demonstrations). We follow the prompt structure used by the original [GPT-3 paper](https://arxiv.org/pdf/2005.14165.pdf) for the BoolQ task. That is \n",
    "\n",
    "{title} -- {passage}\n",
    "\n",
    "question: {question}\n",
    "\n",
    "answer: {answer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_demonstrations(\n",
    "    demo_titles: List[str],\n",
    "    demo_passages: List[str],\n",
    "    demo_questions: List[str],\n",
    "    demo_labels: List[int],\n",
    "    label_map: Dict[int, str],\n",
    "    n_demos: Optional[int],\n",
    ") -> str:\n",
    "    # n_demos controls how many demonstration examples are included. That is, n_demo-shot prompts are created\n",
    "    demonstrations = []\n",
    "    for demo_title, demo_passage, demo_question, demo_label in zip(\n",
    "        demo_titles, demo_passages, demo_questions, demo_labels\n",
    "    ):\n",
    "        label_str = label_map[demo_label]\n",
    "        demonstration = f\"{demo_title} -- {demo_passage}\\nquestion: {demo_question}?\\nanswer: {label_str}\\n\\n\"\n",
    "        demonstrations.append(demonstration)\n",
    "    demonstration_str = \"\".join(sample(demonstrations, n_demos)) if n_demos else \"\".join(demonstrations)\n",
    "    return demonstration_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompts(\n",
    "    demonstrations: str, test_titles: List[str], test_passages: List[str], test_questions: List[str]\n",
    ") -> List[str]:\n",
    "    prompts = []\n",
    "    for test_title, test_passage, test_question in zip(test_titles, test_passages, test_questions):\n",
    "        prompt = f\"{demonstrations}{test_title} -- {test_passage}\\nquestion: {test_question}?\\nanswer:\"\n",
    "        prompts.append(prompt)\n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're interested in the activations from the last layer of the model, because this will allow us to calculate the\n",
    "# likelihoods.\n",
    "last_layer_name = model.module_names[-1]\n",
    "last_layer_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last layer of the model corresponds to the probabilities of each token in the model vocabulary. That is, it is the conditional probability\n",
    "$$\n",
    "P(y_t \\vert y_{<t}, x),\n",
    "$$\n",
    "The probability distribution over the vocabulary of the next token given the preceding tokens $y_{<t}$, and the prompt text $x$. Thus, for each token $y_{t}$ in our input, we get back a 50K vector corresponding to the probabilities over the vocabulary of $y_{t+1}$. We only care about the last token in our input, as it houses the probability of the, as yet, unseen token the model will generate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to test out the affects of the number of few-shot examples first, then we'll try some different prompts. We'll use our model activations to map to the label as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-Shot Prompting\n",
    "\n",
    "In this section, we won't include any demonstrations in our prompts and will measure the accuracy of the models answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map_1 = {0: \"False\", 1: \"True\"}\n",
    "label_ordering = [\"False\", \"True\"]\n",
    "prompts_1 = create_prompts(\"\", bool_q_test_titles, bool_q_test_passages, bool_q_test_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check one of the prompts.\n",
    "prompts_1[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to instantiate a tokenizer to obtain appropriate token indices for our labels. \n",
    "\n",
    "__NOTE__: All OPT models, regardless of size, used the same tokenizer. However, if you want to use a different type of model, a different tokenizer may be needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "# extract the tokenizer ids associated with our labels\n",
    "label_token_ids = get_label_token_ids(tokenizer, prompts_1[0], label_ordering)\n",
    "# If you ever need to move back from token ids, you can use tokenizer.decode or tokenizer.batch_decode\n",
    "tokenizer.decode(label_token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need the token ids of our labels to extract the probabilties from the vocabulary of the model. The token id corresponds to the index of the token in the vocabulary matrix of the underlying model. For a discussion and demonstration of how this extraction is done, see the `llm_prompt_classification.ipynb` notebook and the comments in the `get_label_with_highest_likelihood` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For memory management, we split the prompts into batches of size 10\n",
    "predicted_labels = []\n",
    "prompt_batches = split_prompts_into_batches(prompts_1)\n",
    "for batch_num, prompt_batch in enumerate(prompt_batches):\n",
    "    activations = model.get_activations(prompt_batch, [last_layer_name], short_generation_config)\n",
    "    print(f\"Batch number {batch_num + 1} Complete\")\n",
    "    for activations_single_prompt in activations.activations:\n",
    "        # For each prompt we extract the activations and calculate which label had the high likelihood.\n",
    "        last_layer_matrix = activations_single_prompt[last_layer_name]\n",
    "        predicted_label = get_label_with_highest_likelihood(last_layer_matrix, label_token_ids, label_map_1)\n",
    "        predicted_labels.append(predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the labels from integers to strings for comparison to the string predicted labels in the confusion matrix\n",
    "bool_q_text_labels_string = [label_map_1[label] for label in bool_q_test_labels]\n",
    "report_metrics(predicted_labels, bool_q_text_labels_string, labels_order=label_ordering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N=1 Few-Shot Examples\n",
    "\n",
    "In this section, we use a single fixed example in our prompt before asking our model to answer the question that we care about for each data point. In the original [GPT-3 paper](https://arxiv.org/pdf/2005.14165.pdf), this resulted in a large jump in performance on the BoolQ task, which we also see here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map_1 = {0: \"False\", 1: \"True\"}\n",
    "label_ordering = [\"False\", \"True\"]\n",
    "demonstrations_1 = create_demonstrations(\n",
    "    bool_q_train_titles,\n",
    "    bool_q_train_passages,\n",
    "    bool_q_train_questions,\n",
    "    bool_q_train_labels,\n",
    "    label_map_1,\n",
    "    1)\n",
    "prompts_1 = create_prompts(\n",
    "    demonstrations_1,\n",
    "    bool_q_test_titles,\n",
    "    bool_q_test_passages,\n",
    "    bool_q_test_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For memory management, we split the prompts into batches of size 10\n",
    "predicted_labels = []\n",
    "prompt_batches = split_prompts_into_batches(prompts_1)\n",
    "for batch_num, prompt_batch in enumerate(prompt_batches):\n",
    "    activations = model.get_activations(prompt_batch, [last_layer_name], short_generation_config)\n",
    "    print(f\"Batch number {batch_num + 1} Complete\")\n",
    "    for activations_single_prompt in activations.activations:\n",
    "        last_layer_matrix = activations_single_prompt[last_layer_name]\n",
    "        predicted_label = get_label_with_highest_likelihood(last_layer_matrix, label_token_ids, label_map_1)\n",
    "        predicted_labels.append(predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_metrics(predicted_labels, bool_q_text_labels_string, labels_order=label_ordering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N=5 Few-Shot Examples (Note that this takes quite a long time to run!)\n",
    "\n",
    "In this example, we consider the affect of adding more demonstrations to our prompts. That is, for this prompt structure, does this result in an accuracy increase over one-shot prompting? The results suggest that there isn't much of a benefit for our current setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map_1 = {0: \"False\", 1: \"True\"}\n",
    "label_ordering = [\"False\", \"True\"]\n",
    "demonstrations_1 = create_demonstrations(\n",
    "    bool_q_train_titles,\n",
    "    bool_q_train_passages,\n",
    "    bool_q_train_questions,\n",
    "    bool_q_train_labels,\n",
    "    label_map_1,\n",
    "    5)\n",
    "prompts_1 = create_prompts(\n",
    "    demonstrations_1,\n",
    "    bool_q_test_titles,\n",
    "    bool_q_test_passages,\n",
    "    bool_q_test_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For memory management, we split the prompts into batches of size 10\n",
    "predicted_labels = []\n",
    "prompt_batches = split_prompts_into_batches(prompts_1)\n",
    "for batch_num, prompt_batch in enumerate(prompt_batches):\n",
    "    activations = model.get_activations(prompt_batch, [last_layer_name], short_generation_config)\n",
    "    print(f\"Batch number {batch_num + 1} Complete\")\n",
    "    for activations_single_prompt in activations.activations:\n",
    "        last_layer_matrix = activations_single_prompt[last_layer_name]\n",
    "        predicted_label = get_label_with_highest_likelihood(last_layer_matrix, label_token_ids, label_map_1)\n",
    "        predicted_labels.append(predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_metrics(predicted_labels, bool_q_text_labels_string, labels_order=label_ordering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try a different set of vocabulary labels (with N=1 for Few-Shot)\n",
    "\n",
    "Rather than asking the model to respond with a True/False answer, which, if you consider the phrasing of the BoolQ questions, was a bit \"un-grammatical\" or less \"fluent\", we consider the simple label space of \"No\" and \"Yes.\" Because these answers fit a bit more naturally into the question wording, we posit that this change will lead to an increase in downstream accuracy. Because one-shot prompting worked quite well above, we again use it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map_2 = {0: \"No\", 1: \"Yes\"}\n",
    "label_ordering_2 = [\"No\", \"Yes\"]\n",
    "demonstrations_2 = create_demonstrations(\n",
    "    bool_q_train_titles,\n",
    "    bool_q_train_passages,\n",
    "    bool_q_train_questions,\n",
    "    bool_q_train_labels,\n",
    "    label_map_2,\n",
    "    1)\n",
    "prompts_2 = create_prompts(\n",
    "    demonstrations_2,\n",
    "    bool_q_test_titles,\n",
    "    bool_q_test_passages,\n",
    "    bool_q_test_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the tokenizer ids associated with our labels\n",
    "label_token_ids = get_label_token_ids(tokenizer, prompts_2[0], label_ordering_2)\n",
    "# If you ever need to move back from token ids, you can use tokenizer.decode or tokenizer.batch_decode\n",
    "tokenizer.decode(label_token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For memory management, we split the prompts into batches of size 10\n",
    "predicted_labels = []\n",
    "prompt_batches = split_prompts_into_batches(prompts_2)\n",
    "for batch_num, prompt_batch in enumerate(prompt_batches):\n",
    "    activations = model.get_activations(prompt_batch, [last_layer_name], short_generation_config)\n",
    "    print(f\"Batch number {batch_num+1} Complete\")\n",
    "    for activations_single_prompt in activations.activations:\n",
    "        last_layer_matrix = activations_single_prompt[last_layer_name]\n",
    "        predicted_label = get_label_with_highest_likelihood(last_layer_matrix, label_token_ids, label_map_2)\n",
    "        predicted_labels.append(predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the labels from integers to strings for comparison to the string predicted labels in the confusion matrix\n",
    "bool_q_text_labels_string = [label_map_2[label] for label in bool_q_test_labels]\n",
    "report_metrics(predicted_labels, bool_q_text_labels_string, labels_order=label_ordering_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompt_engineering",
   "language": "python",
   "name": "prompt_engineering"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
