{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/opt_notebook/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import time\n",
    "from typing import List\n",
    "\n",
    "import evaluate\n",
    "import kscope\n",
    "from utils import split_prompts_into_batches"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "There is a bit of documentation on how to interact with the large models [here](https://kaleidoscope-sdk.readthedocs.io/en/latest/). The relevant github links to the SDK are [here](https://github.com/VectorInstitute/kaleidoscope-sdk) and underlying code [here](https://github.com/VectorInstitute/kaleidoscope)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we connect to the service through which, we'll interact with the LLMs and see which models are avaiable to us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a client connection to the kscope service\n",
    "client = kscope.Client(gateway_host=\"llm.cluster.local\", gateway_port=3001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show all supported models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OPT-175B', 'OPT-6.7B']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show all model instances that are currently active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '1ae3ae36-af03-45f4-b95e-2ec66e797f96',\n",
       "  'name': 'OPT-175B',\n",
       "  'state': 'ACTIVE'},\n",
       " {'id': '65084219-31ef-4430-922e-fb31f219ed49',\n",
       "  'name': 'OPT-6.7B',\n",
       "  'state': 'ACTIVE'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.model_instances"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by querying the OPT-175B model. We'll try other models below. Get a handle to a model. In this example, let's use the OPT-175B model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = client.load_model(\"OPT-175B\")\n",
    "# If this model is not actively running, it will get launched in the background.\n",
    "# In this case, wait until it moves into an \"ACTIVE\" state before proceeding.\n",
    "while model.state != \"ACTIVE\":\n",
    "    time.sleep(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to configure the model to generate in the way we want it to. We set important parameters.\n",
    "\n",
    "* `max_tokens` sets the number the model generates before haulting generation.\n",
    "* `top_k`: Range: 0-Vocab size. At each generation step this is the number of tokens to select from with relative probabilities associated with their likliehoods. Setting this to 1 is \"Greedy decoding.\" If top_k is set to zero them we exclusively use nucleus sample (i.e. top_p below).\n",
    "* `top_p`: Range: 0.0-1.0, nucleus sampling. At each generation step, the tokens the largest probabilities, adding up to `top_p` are sampled from relative to their likliehoods.\n",
    "* `rep_penalty`: Range >= 1.0. This attempts to decrease the likelihood of tokens in a generation process if they have been generated before. A value of 1.0 means no penalty and larger values increasingly penalize repeated values. 1.2 has been reported as a good default value.\n",
    "* `temperature`: Range >=0.0. This value \"sharpens\" or flattens the softmax calculation done to produce probabilties over the vocab. As temperature goes to zero: only the largest probabilities will remain non-zero (approaches greedy decoding). As it approaches infinity, the distribution spreads out evenly over the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_generation_config = {\"max_tokens\": 100, \"top_k\": 4, \"top_p\": 3, \"rep_penalty\": 1.0, \"temperature\": 1.0}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to try out some few shot and zero shot translation from French to English. We take a sample from the very large WMT14 translation dataset, specifically considering French->English translation only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_texts = []\n",
    "english_texts = []\n",
    "with open(\"resources/translation_dataset/wmt14_sample.json\") as file:\n",
    "    data = json.load(file)[\"dataset\"]\n",
    "    for french_english_pair in data:\n",
    "        french_text = french_english_pair[\"fr\"]\n",
    "        french_texts.append(french_text)\n",
    "        english_text = french_english_pair[\"en\"]\n",
    "        english_texts.append(english_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero shot Prompt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our zero-shot prompt example, we use the same format at the original GPT-3 paper. That is:\n",
    "\n",
    "Q: What is the {target language} translation of {source text} A: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_prompts = []\n",
    "for french_text in french_texts:\n",
    "    zero_shot_prompt = f\"Q: What is the English translation of {french_text} A: \"\n",
    "    zero_shot_prompts.append(zero_shot_prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-Shot Prompt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to speed up inference a bit, we only use 10-shot prompts for our translation task. The original GPT-3 paper uses a very large 64-shot prompt to induce their observed performance. The prompt format is distinctly different from the zero-shot setting. We borrow their structure of:\n",
    "\n",
    "{source text} = {target text}\\n\\n\n",
    "\n",
    "but add on an instruction at the beginning \n",
    "\n",
    "\"Translate the follow sentences from French to English.\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_examples = 10\n",
    "demonstrations = []\n",
    "# Create the demonstrations for translation\n",
    "for french_text, english_text in zip(french_texts[0:n_examples], english_texts[0:n_examples]):\n",
    "    demonstrations.append(f\"{french_text} = {english_text}\\n\\n\")\n",
    "\n",
    "demonstration_str = \"\".join(demonstrations)\n",
    "demonstration_str = f\"Translate the follow sentences from French to English.\\n\\n{demonstration_str}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give each a try with some basic french sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_1 = \"J'aime mon chien.\"  # I love my dog.\n",
    "example_2 = \"Il y a des gens partout.\"  # There are people everywhere.\n",
    "example_3 = (\n",
    "    \"Jusqu'à présent, l'hiver a été étrange à Toronto.\"  # It has been an very weird winter in Toronto thus far.\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero Shot Examples. We only grab the first sentence in the response because we are targeting translation of only one sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " You can't spell love without an I.\n"
     ]
    }
   ],
   "source": [
    "# Place examples in the zero shot template\n",
    "zero_shot_prompt = f\"Q: What is the English translation of {example_1} A: \"\n",
    "generation = model.generate(zero_shot_prompt, long_generation_config)\n",
    "# Grab the first sentence output.\n",
    "print(re.findall(r\".*?[.!\\?]\", generation.generation[\"text\"][0])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you!\n"
     ]
    }
   ],
   "source": [
    "zero_shot_prompt = f\"Q: What is the English translation of {example_2} A: \"\n",
    "generation = model.generate(zero_shot_prompt, long_generation_config)\n",
    "# Grab the first sentence output.\n",
    "print(re.findall(r\".*?[.!\\?]\", generation.generation[\"text\"][0])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just que until present, the winter has been strange in Toronto.\n"
     ]
    }
   ],
   "source": [
    "zero_shot_prompt = f\"Q: What is the English translation of {example_3} A: \"\n",
    "generation = model.generate(zero_shot_prompt, long_generation_config)\n",
    "# Grab the first sentence output.\n",
    "print(re.findall(r\".*?[.!\\?]\", generation.generation[\"text\"][0])[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero-shot is clearly not great, but can sometimes get a fairly good translation.\n",
    "\n",
    "Few-shot Examples Next. Again, we only grab the first sentence in the response because we are targeting translation of only one sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place examples in the few shot template\n",
    "few_shot_prompt = f\"{demonstration_str}{example_1} = \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation = model.generate(few_shot_prompt, long_generation_config)\n",
    "# Grab the first sentence output.\n",
    "print(re.findall(r\".*?[.!\\?]\", generation.generation[\"text\"][0])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\n"
     ]
    }
   ],
   "source": [
    "# Place examples in the zero shot template\n",
    "few_shot_prompt = f\"{demonstration_str}{example_2} = \"\n",
    "generation = model.generate(few_shot_prompt, long_generation_config)\n",
    "# Grab the first sentence output.\n",
    "print(re.findall(r\".*?[.!\\?]\", generation.generation[\"text\"][0])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les joueurs du Canadien devaient terminer leur saison à Columbus en Ohio, mais on a dû modifier certaines dates des matchs de la partie déplacée à Colombus.\n"
     ]
    }
   ],
   "source": [
    "# Place examples in the zero shot template\n",
    "few_shot_prompt = f\"{demonstration_str}{example_3} = \"\n",
    "generation = model.generate(few_shot_prompt, long_generation_config)\n",
    "# Grab the first sentence output.\n",
    "print(re.findall(r\".*?[.!\\?]\", generation.generation[\"text\"][0])[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's measure the BLEU scores for the dataset that we have sampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_metric = evaluate.load(\"bleu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero-shot example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number 1 Complete\n",
      "Batch number 2 Complete\n",
      "Batch number 3 Complete\n",
      "Batch number 4 Complete\n",
      "Batch number 5 Complete\n",
      "Batch number 6 Complete\n",
      "Batch number 7 Complete\n",
      "Batch number 8 Complete\n",
      "Batch number 9 Complete\n",
      "Batch number 10 Complete\n"
     ]
    }
   ],
   "source": [
    "# Split prompts into batches for memory management.\n",
    "translations = []\n",
    "zero_shot_batches = split_prompts_into_batches(zero_shot_prompts)\n",
    "for batch_number, zero_shot_batch in enumerate(zero_shot_batches):\n",
    "    generations = model.generate(zero_shot_batch, long_generation_config)\n",
    "    print(f\"Batch number {batch_number+1} Complete\")\n",
    "    for single_generation in generations.generation[\"text\"]:\n",
    "        generation_text = re.findall(r\".*?[.!\\?]\", single_generation)\n",
    "        generation_text = generation_text[0] if len(generation_text) > 0 else single_generation\n",
    "        translations.append(generation_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_references_for_bleu(references: List[str]) -> List[List[str]]:\n",
    "    # The bleu metric requires inputs to be stored as lists of lists. So we encapsulate each reference in a list\n",
    "    return [[reference] for reference in references]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.04100107549013207,\n",
       " 'precisions': [0.2889004149377593,\n",
       "  0.09354485776805252,\n",
       "  0.03872832369942197,\n",
       "  0.017726161369193152],\n",
       " 'brevity_penalty': 0.6247300237005795,\n",
       " 'length_ratio': 0.6800705467372135,\n",
       " 'translation_length': 1928,\n",
       " 'reference_length': 2835}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_metric.compute(predictions=translations, references=convert_references_for_bleu(english_texts))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few-shot example\n",
    "\n",
    "__NOTE__ This takes quite a while to run due to the sequence length associated with 10-shot prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that we're only taking the remaining 90 examples from the test set, since we used the first 10 for few-shot\n",
    "# examples\n",
    "few_shot_prompts = [f\"{demonstration_str}{french_text} = \" for french_text in french_texts[n_examples:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number 1 Complete\n",
      "Batch number 2 Complete\n",
      "Batch number 3 Complete\n",
      "Batch number 4 Complete\n",
      "Batch number 5 Complete\n",
      "Batch number 6 Complete\n",
      "Batch number 7 Complete\n",
      "Batch number 8 Complete\n",
      "Batch number 9 Complete\n"
     ]
    }
   ],
   "source": [
    "# Split prompts into batches for memory management.\n",
    "translations = []\n",
    "few_shot_batches = split_prompts_into_batches(few_shot_prompts)\n",
    "for batch_number, few_shot_batch in enumerate(few_shot_batches):\n",
    "    generations = model.generate(few_shot_batch, long_generation_config)\n",
    "    print(f\"Batch number {batch_number+1} Complete\")\n",
    "    for single_generation in generations.generation[\"text\"]:\n",
    "        generation_text = re.findall(r\".*?[.!\\?]\", single_generation)\n",
    "        generation_text = generation_text[0] if len(generation_text) > 0 else single_generation\n",
    "        translations.append(generation_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.15526350153077717,\n",
       " 'precisions': [0.44246782740348223,\n",
       "  0.19670846394984326,\n",
       "  0.10746147607461476,\n",
       "  0.06213266162888329],\n",
       " 'brevity_penalty': 1.0,\n",
       " 'length_ratio': 1.0134253931722286,\n",
       " 'translation_length': 2642,\n",
       " 'reference_length': 2607}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that we're only taking the remaining 90 examples from the test set, since we used the first 10 for few-shot\n",
    "# examples\n",
    "bleu_metric.compute(predictions=translations, references=convert_references_for_bleu(english_texts[n_examples:]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opt_notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
