{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import time\n",
    "from typing import List\n",
    "\n",
    "import evaluate\n",
    "import kscope\n",
    "from utils import split_prompts_into_batches"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "There is a bit of documentation on how to interact with the large models [here](https://kaleidoscope-sdk.readthedocs.io/en/latest/). The relevant github links to the SDK are [here](https://github.com/VectorInstitute/kaleidoscope-sdk) and underlying code [here](https://github.com/VectorInstitute/kaleidoscope)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we connect to the service through which we'll interact with the LLMs and see which models are avaiable to us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a client connection to the kscope service\n",
    "client = kscope.Client(gateway_host=\"llm.cluster.local\", gateway_port=3001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show all supported models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OPT-175B', 'OPT-6.7B', 'GPT2']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show all model instances that are currently active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '0a8818f1-991a-48ef-9aa6-0bb4b06b1fe9',\n",
       "  'name': 'OPT-175B',\n",
       "  'state': 'ACTIVE'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.model_instances"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we obtain a handle to a model. In this example, let's use the OPT-175B model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = client.load_model(\"OPT-175B\")\n",
    "# If this model is not actively running, it will get launched in the background.\n",
    "# In this case, wait until it moves into an \"ACTIVE\" state before proceeding.\n",
    "while model.state != \"ACTIVE\":\n",
    "    time.sleep(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to configure the model to generate in the way we want it to. So we set a number of important parameters. For a discussion of the configuration parameters see: `src/reference_implementations/prompting_vector_llms/CONFIG_README.md`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_generation_config = {\"max_tokens\": 100, \"top_k\": 4, \"top_p\": 1.0, \"rep_penalty\": 1.2, \"temperature\": 0.5}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to try out some few shot and zero shot translation from French to English. We take a sample from the very large WMT14 translation dataset, specifically considering French->English translation only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_texts = []\n",
    "english_texts = []\n",
    "with open(\"resources/translation_dataset/wmt14_sample.json\") as file:\n",
    "    data = json.load(file)[\"dataset\"]\n",
    "    for french_english_pair in data:\n",
    "        french_text = french_english_pair[\"fr\"]\n",
    "        french_texts.append(french_text)\n",
    "        english_text = french_english_pair[\"en\"]\n",
    "        english_texts.append(english_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-shot Prompt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our zero-shot prompt example, we use the same format at the original [GPT-3 paper](https://arxiv.org/pdf/2005.14165.pdf). That is:\n",
    "\n",
    "Q: What is the {target language} translation of {source text} A: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_prompts = []\n",
    "for french_text in french_texts:\n",
    "    zero_shot_prompt = f\"Q: What is the English translation of {french_text} A: \"\n",
    "    zero_shot_prompts.append(zero_shot_prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-Shot Prompt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to speed up inference a bit, we only use 10-shot prompts for our translation task. The original [GPT-3 paper](https://arxiv.org/pdf/2005.14165.pdf) uses a very large 64-shot prompt to induce their observed performance. The prompt format that we use is distinctly different from the zero-shot setting. We borrow their structure of:\n",
    "\n",
    "{source text} = {target text}\\n\\n\n",
    "\n",
    "but add on an instruction at the beginning \n",
    "\n",
    "\"Translate the follow sentences from French to English.\\n\\n\"\n",
    "\n",
    "To create the demonstrations, we take the first `n_examples` from the dataset sample, and use the rest for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_examples = 10\n",
    "demonstrations = []\n",
    "# Create the demonstrations for translation\n",
    "for french_text, english_text in zip(french_texts[0:n_examples], english_texts[0:n_examples]):\n",
    "    demonstrations.append(f\"{french_text} = {english_text}\\n\\n\")\n",
    "\n",
    "demonstration_str = \"\".join(demonstrations)\n",
    "demonstration_str = f\"Translate the follow sentences from French to English.\\n\\n{demonstration_str}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give each a try with some basic french sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_1 = \"J'aime mon chien.\"  # I love my dog.\n",
    "example_2 = \"Il y a des gens partout.\"  # There are people everywhere.\n",
    "example_3 = (\n",
    "    \"Jusqu'à présent, l'hiver a été étrange à Toronto.\"  # It has been an very weird winter in Toronto thus far.\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero Shot Examples.\n",
    "\n",
    "We only grab the first sentence in the response because we are targeting translation of only one sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I love my dog.\n"
     ]
    }
   ],
   "source": [
    "# Place examples in the zero shot template\n",
    "zero_shot_prompt = f\"Q: What is the English translation of {example_1} A: \"\n",
    "generation = model.generate(zero_shot_prompt, long_generation_config)\n",
    "# Grab the first sentence output.\n",
    "print(re.findall(r\".*?[.!\\?]\", generation.generation[\"text\"][0])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " There are people everywhere.\n"
     ]
    }
   ],
   "source": [
    "zero_shot_prompt = f\"Q: What is the English translation of {example_2} A: \"\n",
    "generation = model.generate(zero_shot_prompt, long_generation_config)\n",
    "# Grab the first sentence output.\n",
    "print(re.findall(r\".*?[.!\\?]\", generation.generation[\"text\"][0])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To this point, the winter has been strange in Toronto.\n"
     ]
    }
   ],
   "source": [
    "zero_shot_prompt = f\"Q: What is the English translation of {example_3} A: \"\n",
    "generation = model.generate(zero_shot_prompt, long_generation_config)\n",
    "# Grab the first sentence output.\n",
    "print(re.findall(r\".*?[.!\\?]\", generation.generation[\"text\"][0])[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero-shot isn't too bad, sometimes get a fairly good translation.\n",
    "\n",
    "### Few-shot Examples \n",
    "\n",
    "Again, we only grab the first sentence in the response because we are targeting translation of only one sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place examples in the few shot template\n",
    "few_shot_prompt = f\"{demonstration_str}{example_1} = \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le chien est bien.\n"
     ]
    }
   ],
   "source": [
    "generation = model.generate(few_shot_prompt, long_generation_config)\n",
    "# Grab the first sentence output.\n",
    "print(re.findall(r\".*?[.!\\?]\", generation.generation[\"text\"][0])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ˆThere are people everywhere.\n"
     ]
    }
   ],
   "source": [
    "# Place examples in the zero shot template\n",
    "few_shot_prompt = f\"{demonstration_str}{example_2} = \"\n",
    "generation = model.generate(few_shot_prompt, long_generation_config)\n",
    "# Grab the first sentence output.\n",
    "print(re.findall(r\".*?[.!\\?]\", generation.generation[\"text\"][0])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Â« Until now, the winter has been strange in Toronto.\n"
     ]
    }
   ],
   "source": [
    "# Place examples in the zero shot template\n",
    "few_shot_prompt = f\"{demonstration_str}{example_3} = \"\n",
    "generation = model.generate(few_shot_prompt, long_generation_config)\n",
    "# Grab the first sentence output.\n",
    "print(re.findall(r\".*?[.!\\?]\", generation.generation[\"text\"][0])[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's measure the BLEU scores for the dataset that we have sampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_metric = evaluate.load(\"bleu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-shot example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number 1 Complete\n",
      "Batch number 2 Complete\n",
      "Batch number 3 Complete\n",
      "Batch number 4 Complete\n",
      "Batch number 5 Complete\n",
      "Batch number 6 Complete\n",
      "Batch number 7 Complete\n",
      "Batch number 8 Complete\n",
      "Batch number 9 Complete\n",
      "Batch number 10 Complete\n"
     ]
    }
   ],
   "source": [
    "# Split prompts into batches for memory management.\n",
    "translations = []\n",
    "zero_shot_batches = split_prompts_into_batches(zero_shot_prompts)\n",
    "for batch_number, zero_shot_batch in enumerate(zero_shot_batches):\n",
    "    generations = model.generate(zero_shot_batch, long_generation_config)\n",
    "    print(f\"Batch number {batch_number+1} Complete\")\n",
    "    for single_generation in generations.generation[\"text\"]:\n",
    "        # Take just the first sentence\n",
    "        generation_text = re.findall(r\".*?[.!\\?]\", single_generation)\n",
    "        generation_text = generation_text[0] if len(generation_text) > 0 else single_generation\n",
    "        translations.append(generation_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_references_for_bleu(references: List[str]) -> List[List[str]]:\n",
    "    # The bleu metric requires inputs to be stored as lists of lists. So we encapsulate each reference in a list\n",
    "    return [[reference] for reference in references]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.10254306983969617,\n",
       " 'precisions': [0.3072719630096679,\n",
       "  0.1448577680525164,\n",
       "  0.08933454876937101,\n",
       "  0.05985748218527316],\n",
       " 'brevity_penalty': 0.8255733409569715,\n",
       " 'length_ratio': 0.8391534391534392,\n",
       " 'translation_length': 2379,\n",
       " 'reference_length': 2835}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_metric.compute(predictions=translations, references=convert_references_for_bleu(english_texts))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot example\n",
    "\n",
    "__NOTE__: This takes quite a while to run due to the sequence length associated with 10-shot prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that we're only taking the remaining 90 examples from the test set, since we used the first 10 for few-shot\n",
    "# examples\n",
    "few_shot_prompts = [f\"{demonstration_str}{french_text} = \" for french_text in french_texts[n_examples:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number 1 Complete\n",
      "Batch number 2 Complete\n",
      "Batch number 3 Complete\n",
      "Batch number 4 Complete\n",
      "Batch number 5 Complete\n",
      "Batch number 6 Complete\n",
      "Batch number 7 Complete\n",
      "Batch number 8 Complete\n",
      "Batch number 9 Complete\n"
     ]
    }
   ],
   "source": [
    "# Split prompts into batches for memory management.\n",
    "translations = []\n",
    "few_shot_batches = split_prompts_into_batches(few_shot_prompts)\n",
    "for batch_number, few_shot_batch in enumerate(few_shot_batches):\n",
    "    generations = model.generate(few_shot_batch, long_generation_config)\n",
    "    print(f\"Batch number {batch_number+1} Complete\")\n",
    "    for single_generation in generations.generation[\"text\"]:\n",
    "        # Take just the first sentence\n",
    "        generation_text = re.findall(r\".*?[.!\\?]\", single_generation)\n",
    "        generation_text = generation_text[0] if len(generation_text) > 0 else single_generation\n",
    "        translations.append(generation_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.28117811753944133,\n",
       " 'precisions': [0.5707779886148008,\n",
       "  0.33398821218074654,\n",
       "  0.21724979658258747,\n",
       "  0.15092748735244518],\n",
       " 'brevity_penalty': 1.0,\n",
       " 'length_ratio': 1.010740314537783,\n",
       " 'translation_length': 2635,\n",
       " 'reference_length': 2607}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that we're only taking the remaining 90 examples from the test set, since we used the first 10 for few-shot\n",
    "# examples\n",
    "bleu_metric.compute(predictions=translations, references=convert_references_for_bleu(english_texts[n_examples:]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly the few-shot prompt does a much better job in translation, at least as measured by the BLEU Score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opt_notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
