{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import time\n",
    "from typing import List, Tuple\n",
    "\n",
    "import kscope\n",
    "import pandas as pd\n",
    "from metrics import map_ag_news_int_labels, report_metrics\n",
    "from transformers import AutoTokenizer\n",
    "from utils import get_label_token_ids, get_label_with_highest_likelihood, split_prompts_into_batches"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "There is a bit of documentation on how to interact with the large models [here](https://kaleidoscope-sdk.readthedocs.io/en/latest/). The relevant github links to the SDK are [here](https://github.com/VectorInstitute/kaleidoscope-sdk) and underlying code [here](https://github.com/VectorInstitute/kaleidoscope)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we connect to the service through which, we'll interact with the LLMs and see which models are avaiable to us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a client connection to the kscope service\n",
    "client = kscope.Client(gateway_host=\"llm.cluster.local\", gateway_port=3001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show all supported models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OPT-175B', 'OPT-6.7B']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show all model instances that are currently active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '1ae3ae36-af03-45f4-b95e-2ec66e797f96',\n",
       "  'name': 'OPT-175B',\n",
       "  'state': 'ACTIVE'},\n",
       " {'id': '65084219-31ef-4430-922e-fb31f219ed49',\n",
       "  'name': 'OPT-6.7B',\n",
       "  'state': 'ACTIVE'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.model_instances"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by querying the OPT-175B model. We'll try other models below. Get a handle to a model. In this example, let's use the OPT-175B model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = client.load_model(\"OPT-175B\")\n",
    "# If this model is not actively running, it will get launched in the background.\n",
    "# In this case, wait until it moves into an \"ACTIVE\" state before proceeding.\n",
    "while model.state != \"ACTIVE\":\n",
    "    time.sleep(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to configure the model to generate in the way we want it to. So we set a number of important parameters. For a discussion of the configuration parameters see: `src/reference_implementations/prompting_vector_llms/CONFIG_README.md`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_generation_config = {\"max_tokens\": 2, \"top_k\": 4, \"top_p\": 1.0, \"rep_penalty\": 1.0, \"temperature\": 1.0}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a basic prompt for factual information.\n",
    "\n",
    "__Note__ that if you run the cell multiple times, you'll get different responses due to sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nO']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation = model.generate(\"What is the capital of Canada?\", short_generation_config)\n",
    "# Extract the text from the returned generation\n",
    "generation.generation[\"text\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to have our model attempt to classify some news articles from the AG News Dataset. Articles have a single label 1-4\n",
    "\n",
    "1. World\n",
    "2. Sports\n",
    "3. Business\n",
    "4. Sci/Tech\n",
    "\n",
    "This is a constrained label space. We'll use the words World, Sports, Business, and Science as our targets for each of the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_markup(text: str) -> str:\n",
    "    text = re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text)\n",
    "    text = re.sub(r\"<.*?>+\", \"\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def ag_news_processor(path: str) -> Tuple[List[str], List[str], List[str]]:\n",
    "    ag_news_data = pd.read_csv(path)\n",
    "    labels = ag_news_data[\"Class Index\"].tolist()\n",
    "    titles = ag_news_data[\"Title\"].apply(lambda x: remove_markup(x)).tolist()\n",
    "    descriptions = ag_news_data[\"Description\"].apply(lambda x: remove_markup(x)).tolist()\n",
    "    return labels, titles, descriptions\n",
    "\n",
    "\n",
    "int_to_label_map = {1: \"world\", 2: \"sports\", 3: \"business\", 4: \"science\"}\n",
    "ag_news_labels, ag_news_titles, ag_news_descriptions = ag_news_processor(\n",
    "    \"resources/ag_news_datasets/ag_news_sample.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "ag_news_labels = map_ag_news_int_labels(ag_news_labels, int_to_label_map)\n",
    "ag_news_descriptions = [description.replace(\"\\\\\", \" \").strip() for description in ag_news_descriptions]\n",
    "ag_news_titles = [title.strip() for title in ag_news_titles]\n",
    "label_words = [\"World\", \"Sports\", \"Business\", \"Science\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input_texts = [\n",
    "    f\"Title: {ag_news_title} Description: {ag_news_description}\"\n",
    "    for ag_news_title, ag_news_description in zip(ag_news_titles, ag_news_descriptions)\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by trying out a basic instruction prompt to see what the model does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " All Categories\n",
      "==================================\n",
      " One of\n",
      "==================================\n",
      "\n",
      "\n",
      "\n",
      "==================================\n"
     ]
    }
   ],
   "source": [
    "prompt_template = \"To which category does this news article belong?\"\n",
    "sample_texts = [f\"{model_input_text} {prompt_template}\" for model_input_text in model_input_texts[0:3]]\n",
    "generation = model.generate(sample_texts, short_generation_config)\n",
    "for text in generation.generation[\"text\"]:\n",
    "    print(text)\n",
    "    print(\"==================================\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not well...Now let's try to constrain the model a bit by including the desired labels in the instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Environment,\n",
      "==================================\n",
      " South &\n",
      "==================================\n",
      "________________.\n",
      "==================================\n"
     ]
    }
   ],
   "source": [
    "prompt_template = \"From World, Sports, Business, Science, the category is \"\n",
    "sample_texts = [f\"{model_input_text} {prompt_template}\" for model_input_text in model_input_texts[0:3]]\n",
    "generation = model.generate(sample_texts, short_generation_config)\n",
    "for text in generation.generation[\"text\"]:\n",
    "    print(text)\n",
    "    print(\"==================================\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model doesn't really answer in the space that we want it to. Let's try with some few-shot examples to see if that helps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Business\n",
      "\n",
      "==================================\n",
      " Business\n",
      "\n",
      "==================================\n",
      " Sports\n",
      "\n",
      "==================================\n"
     ]
    }
   ],
   "source": [
    "prompt_template_prefix = \"\"\"Title: Lane drives in winning run in ninth Description: Jason Lane took an unusual post-game batting practice with hitting coach Gary Gaetti after a disappointing performance Friday night. Category (World, Sports, Business, Science): Sports\n",
    "Title: Arson attack on Jewish centre in Paris (AFP) Description: AFP - A Jewish social centre in central Paris was destroyed by fire overnight in an anti-Semitic arson attack, city authorities said. Category (World, Sports, Business, Science): World\n",
    "Title: Oil prices look set to dominate Description: The price of oil looks set to grab headlines as analysts forecast that its record-breaking run may well continue. Category (World, Sports, Business, Science): Business\n",
    "Title: More Evidence for Past Water on Mars Description: Summary - (Aug 22, 2004) NASA #39;s Spirit rover has dug up plenty of evidence on slopes of  quot;Columbia Hills quot; that water once covered the area. Category (World, Sports, Business, Science): World\n",
    "Title: Indexes in Japan fall short of hype Description: Japanese stocks have failed to measure up to an assessment made in April by Merrill Lynch #39;s chief global strategist, David Bowers, who said Japan was  quot;very much everyone #39;s favorite equity market. Category (World, Sports, Business, Science): Business\n",
    "\"\"\"  # noqa\n",
    "prompt_template_postfix = \"Category (World, Sports, Business, Science):\"\n",
    "sample_texts = [\n",
    "    f\"{prompt_template_prefix}{model_input_text} {prompt_template_postfix}\"\n",
    "    for model_input_text in model_input_texts[0:3]\n",
    "]\n",
    "generation = model.generate(sample_texts, short_generation_config)\n",
    "for text in generation.generation[\"text\"]:\n",
    "    # We'll limit ourselves to the single next token since we want it to respond that way\n",
    "    print(text)\n",
    "    print(\"==================================\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few-shot learning definitely helps a lot! We'll measure an accuracy sample below. However, there is nothing stoping the model from not selecting our labels. So can we do better? We can work around this by understanding the likelihood of our labels from the models perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'decoder.output_projection'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We're interested in the activations from the last layer of the model, because this will allow us to caculation the\n",
    "# likelihoods\n",
    "last_layer_name = model.module_names[-1]\n",
    "last_layer_name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to instantiate a tokenizer to obtain appropriate token indices for our labels. \n",
    "\n",
    "__NOTE__: All OPT models, regardless of size, used the same tokenizing. However, if you want to use a different type of model, a different tokenizer may be needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' World Sports Business Science'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "label_token_ids = get_label_token_ids(tokenizer, prompt_template, label_words)\n",
    "# If you ever need to move back from token ids, you can use tokenizer.decode or tokenizer.batch_decode\n",
    "tokenizer.decode(label_token_ids)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at how we can extract the likelihoods given the label tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_prompted_input = f\"{model_input_texts[0]} {prompt_template} {label_words[0]}\"\n",
    "# Create a prompt with one of the label words as a completion\n",
    "activations = model.get_activations(single_prompted_input, [last_layer_name], short_generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activations matrix shape: torch.Size([62, 50272])\n",
      "Predicted Label: business\n"
     ]
    }
   ],
   "source": [
    "last_layer_matrix = activations.activations[0][last_layer_name]\n",
    "# The shape of this tensor should be number of input tokens by the vocabulary size (n x 50272)\n",
    "print(f\"Activations matrix shape: {last_layer_matrix.shape}\")\n",
    "predicted_label = get_label_with_highest_likelihood(\n",
    "    last_layer_matrix, label_token_ids, int_to_label_map, right_shift=True\n",
    ")\n",
    "print(f\"Predicted Label: {predicted_label}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "Time to compare our results across three methods. \n",
    "1. Measure the accuracy of our few-shot prompting approach.\n",
    "2. Measure the accuracy of our likelihood approach without few-shot.\n",
    "3. Measure the accuracy of our likelihood approach with few-shot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowercase_labels = [word.lower() for word in label_words]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number 1 Complete\n",
      "Batch number 2 Complete\n",
      "Batch number 3 Complete\n",
      "Batch number 4 Complete\n",
      "Batch number 5 Complete\n",
      "Potential Prediction: economics does not match any label\n",
      "Batch number 6 Complete\n",
      "Batch number 7 Complete\n",
      "Batch number 8 Complete\n",
      "Potential Prediction: u does not match any label\n",
      "Batch number 9 Complete\n",
      "Potential Prediction: politics does not match any label\n",
      "Batch number 10 Complete\n"
     ]
    }
   ],
   "source": [
    "prompt_template_prefix = \"\"\"Title: Lane drives in winning run in ninth Description: Jason Lane took an unusual post-game batting practice with hitting coach Gary Gaetti after a disappointing performance Friday night. Category (World, Sports, Business, Science): Sports\n",
    "Title: Arson attack on Jewish centre in Paris (AFP) Description: AFP - A Jewish social centre in central Paris was destroyed by fire overnight in an anti-Semitic arson attack, city authorities said. Category (World, Sports, Business, Science): World\n",
    "Title: Oil prices look set to dominate Description: The price of oil looks set to grab headlines as analysts forecast that its record-breaking run may well continue. Category (World, Sports, Business, Science): Business\n",
    "Title: More Evidence for Past Water on Mars Description: Summary - (Aug 22, 2004) NASA #39;s Spirit rover has dug up plenty of evidence on slopes of  quot;Columbia Hills quot; that water once covered the area. Category (World, Sports, Business, Science): World\n",
    "Title: Indexes in Japan fall short of hype Description: Japanese stocks have failed to measure up to an assessment made in April by Merrill Lynch #39;s chief global strategist, David Bowers, who said Japan was  quot;very much everyone #39;s favorite equity market. Category (World, Sports, Business, Science): Business\n",
    "\"\"\"  # noqa\n",
    "prompt_template_postfix = \"Category (World, Sports, Business, Science):\"\n",
    "prompts = [\n",
    "    f\"{prompt_template_prefix}{model_input_text} {prompt_template_postfix}\" for model_input_text in model_input_texts\n",
    "]\n",
    "prompt_batches = split_prompts_into_batches(prompts)\n",
    "predicted_labels = []\n",
    "for batch_num, prompt_batch in enumerate(prompt_batches):\n",
    "    generation = model.generate(prompt_batch, short_generation_config)\n",
    "    print(f\"Batch number {batch_num+1} Complete\")\n",
    "    # We'll use tokens this time and consider just the first token\n",
    "    first_predicted_tokens = [tokens[0].strip().lower() for tokens in generation.generation[\"tokens\"]]\n",
    "    # If a token doesn't correspond to one of our labels, we'll randomly select one and count how many times that\n",
    "    # happens for reporting\n",
    "    n_no_match = 0\n",
    "    for potential_prediction in first_predicted_tokens:\n",
    "        if potential_prediction in lowercase_labels:\n",
    "            predicted_labels.append(potential_prediction)\n",
    "        else:\n",
    "            n_no_match += 1\n",
    "            print(f\"Potential Prediction: {potential_prediction} does not match any label\")\n",
    "            predicted_labels.append(random.choice(lowercase_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy: 0.65\n",
      "Confusion Matrix with ordering ['world', 'sports', 'business', 'science']\n",
      "[[15  0  1  2]\n",
      " [ 1 21  0  0]\n",
      " [ 9  0 21 18]\n",
      " [ 3  0  1  8]]\n",
      "========================================================\n",
      "Label: world, F1: 0.6521739130434783, Precision: 0.8333333333333334, Recall: 0.5357142857142857\n",
      "Label: sports, F1: 0.9767441860465117, Precision: 0.9545454545454546, Recall: 1.0\n",
      "Label: business, F1: 0.5915492957746479, Precision: 0.4375, Recall: 0.9130434782608695\n",
      "Label: science, F1: 0.4, Precision: 0.6666666666666666, Recall: 0.2857142857142857\n"
     ]
    }
   ],
   "source": [
    "report_metrics(predicted_labels, ag_news_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood No Few-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number 1 Complete\n",
      "Batch number 2 Complete\n",
      "Batch number 3 Complete\n",
      "Batch number 4 Complete\n",
      "Batch number 5 Complete\n",
      "Batch number 6 Complete\n",
      "Batch number 7 Complete\n",
      "Batch number 8 Complete\n",
      "Batch number 9 Complete\n",
      "Batch number 10 Complete\n"
     ]
    }
   ],
   "source": [
    "prompts = [f\"{model_input_text} {prompt_template} {label_words[0]}\" for model_input_text in model_input_texts]\n",
    "# For memory management, we split the prompts into batches of size 10\n",
    "predicted_labels = []\n",
    "prompt_batches = split_prompts_into_batches(prompts)\n",
    "for batch_num, prompt_batch in enumerate(prompt_batches):\n",
    "    activations = model.get_activations(prompt_batch, [last_layer_name], short_generation_config)\n",
    "    print(f\"Batch number {batch_num+1} Complete\")\n",
    "    for activations_single_prompt in activations.activations:\n",
    "        last_layer_matrix = activations_single_prompt[last_layer_name]\n",
    "        predicted_label = get_label_with_highest_likelihood(\n",
    "            last_layer_matrix, label_token_ids, int_to_label_map, right_shift=True\n",
    "        )\n",
    "        predicted_labels.append(predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy: 0.42\n",
      "Confusion Matrix with ordering ['world', 'sports', 'business', 'science']\n",
      "[[ 0  0  0  0]\n",
      " [ 7 21  2  2]\n",
      " [21  0 21 26]\n",
      " [ 0  0  0  0]]\n",
      "========================================================\n",
      "Label: world, F1: nan, Precision: nan, Recall: 0.0\n",
      "Label: sports, F1: 0.7924528301886793, Precision: 0.65625, Recall: 1.0\n",
      "Label: business, F1: 0.46153846153846156, Precision: 0.3088235294117647, Recall: 0.9130434782608695\n",
      "Label: science, F1: nan, Precision: nan, Recall: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/david/Desktop/VectorRepositories/PromptEngineering/src/reference_implementations/prompting_vector_llms/llm_prompting_examples/metrics.py:18: RuntimeWarning: invalid value encountered in divide\n",
      "  TP = np.diag(matrix)\n"
     ]
    }
   ],
   "source": [
    "report_metrics(predicted_labels, ag_news_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood with Few-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number 1 Complete\n",
      "Batch number 2 Complete\n",
      "Batch number 3 Complete\n",
      "Batch number 4 Complete\n",
      "Batch number 5 Complete\n",
      "Batch number 6 Complete\n",
      "Batch number 7 Complete\n",
      "Batch number 8 Complete\n",
      "Batch number 9 Complete\n",
      "Batch number 10 Complete\n"
     ]
    }
   ],
   "source": [
    "prompt_template_prefix = \"\"\"Title: Lane drives in winning run in ninth Description: Jason Lane took an unusual post-game batting practice with hitting coach Gary Gaetti after a disappointing performance Friday night. Category (World, Sports, Business, Science): Sports\n",
    "Title: Arson attack on Jewish centre in Paris (AFP) Description: AFP - A Jewish social centre in central Paris was destroyed by fire overnight in an anti-Semitic arson attack, city authorities said. Category (World, Sports, Business, Science): World\n",
    "Title: Oil prices look set to dominate Description: The price of oil looks set to grab headlines as analysts forecast that its record-breaking run may well continue. Category (World, Sports, Business, Science): Business\n",
    "Title: More Evidence for Past Water on Mars Description: Summary - (Aug 22, 2004) NASA #39;s Spirit rover has dug up plenty of evidence on slopes of  quot;Columbia Hills quot; that water once covered the area. Category (World, Sports, Business, Science): World\n",
    "Title: Indexes in Japan fall short of hype Description: Japanese stocks have failed to measure up to an assessment made in April by Merrill Lynch #39;s chief global strategist, David Bowers, who said Japan was  quot;very much everyone #39;s favorite equity market. Category (World, Sports, Business, Science): Business\n",
    "\"\"\"  # noqa\n",
    "prompt_template_postfix = \"Category (World, Sports, Business, Science):\"\n",
    "prompts = [\n",
    "    f\"{prompt_template_prefix}{model_input_text} {prompt_template_postfix}\" for model_input_text in model_input_texts\n",
    "]\n",
    "# For memory management, we split the prompts into batches of size 10\n",
    "predicted_labels = []\n",
    "prompt_batches = split_prompts_into_batches(prompts)\n",
    "for batch_num, prompt_batch in enumerate(prompt_batches):\n",
    "    activations = model.get_activations(prompt_batch, [last_layer_name], short_generation_config)\n",
    "    print(f\"Batch number {batch_num+1} Complete\")\n",
    "    for activations_single_prompt in activations.activations:\n",
    "        last_layer_matrix = activations_single_prompt[last_layer_name]\n",
    "        predicted_label = get_label_with_highest_likelihood(\n",
    "            last_layer_matrix, label_token_ids, int_to_label_map, right_shift=True\n",
    "        )\n",
    "        predicted_labels.append(predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy: 0.69\n",
      "Confusion Matrix with ordering ['world', 'sports', 'business', 'science']\n",
      "[[22  0  2  2]\n",
      " [ 1 21  0  0]\n",
      " [ 4  0 21 21]\n",
      " [ 1  0  0  5]]\n",
      "========================================================\n",
      "Label: world, F1: 0.8148148148148148, Precision: 0.8461538461538461, Recall: 0.7857142857142857\n",
      "Label: sports, F1: 0.9767441860465117, Precision: 0.9545454545454546, Recall: 1.0\n",
      "Label: business, F1: 0.608695652173913, Precision: 0.45652173913043476, Recall: 0.9130434782608695\n",
      "Label: science, F1: 0.29411764705882354, Precision: 0.8333333333333334, Recall: 0.17857142857142858\n"
     ]
    }
   ],
   "source": [
    "report_metrics(predicted_labels, ag_news_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opt_notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
