{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/opt_notebook/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import re\n",
    "import time\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import lingua\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from metrics import ag_news_metrics, map_ag_news_int_labels\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "There is a bit of documentation on how to interact with the large models [here](https://lingua-sdk.readthedocs.io/en/latest/getting_started.html). The relevant github links to the SDK are [here](https://github.com/VectorInstitute/lingua-sdk) and underlying code [here](https://github.com/VectorInstitute/lingua)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we connect to the service through which, we'll interact with the LLMs and see which models are avaiable to us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a client connection to the Lingua service\n",
    "client = lingua.Client(gateway_host=\"llm.cluster.local\", gateway_port=3001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show all supported models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OPT-175B', 'OPT-6.7B']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show all model instances that are currently active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'c402a90b-5867-476b-950d-9921585335ec',\n",
       "  'name': 'OPT-6.7B',\n",
       "  'state': 'ACTIVE'},\n",
       " {'id': 'af334811-a4fc-483d-91be-a65a3a98d34e',\n",
       "  'name': 'OPT-175B',\n",
       "  'state': 'ACTIVE'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.model_instances"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by querying the OPT-175B model. We'll try other models below. Get a handle to a model. In this example, let's use the OPT-175B model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = client.load_model(\"OPT-175B\")\n",
    "# If this model is not actively running, it will get launched in the background.\n",
    "# In this case, wait until it moves into an \"ACTIVE\" state before proceeding.\n",
    "while model.state != \"ACTIVE\":\n",
    "    time.sleep(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to configure the model to generate in the way we want it to. We set important parameters.\n",
    "\n",
    "*`max_tokens` sets the number the model generates before haulting generation.\n",
    "*`top_k`: Range: 0-Vocab size. At each generation step this is the number of tokens to select from with relative probabilities associated with their likliehoods. Setting this to 1 is \"Greedy decoding.\" If top_k is set to zero them we exclusively use nucleus sample (i.e. top_p below).\n",
    "*`top_p`: Range: 0.0-1.0, nucleus sampling. At each generation step, the tokens the largest probabilities, adding up to `top_p` are sampled from relative to their likliehoods.\n",
    "*`rep_penalty`: Range >= 1.0. This attempts to decrease the likelihood of tokens in a generation process if they have been generated before. A value of 1.0 means no penalty and larger values increasingly penalize repeated values. 1.2 has been reported as a good default value.\n",
    "*`temperature`: Range >=0.0. This value \"sharpens\" or flattens the softmax calculation done to produce probabilties over the vocab. As temperature goes to zero: only the largest probabilities will remain non-zero (approaches greedy decoding). As it approaches infinity, the distribution spreads out evenly over the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_generation_config = {\"max_tokens\": 5, \"top_k\": 4, \"top_p\": 3, \"rep_penalty\": 1.0, \"temperature\": 1.0}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a basic prompt for factual information.\n",
    "\n",
    "__Note__ that if you run the cell multiple times, you'll get different responses due to sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\" Isn't the US the\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation = model.generate(\"What is the capital of Canada?\", short_generation_config)\n",
    "# Extract the text from the returned generation\n",
    "generation.generation[\"text\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to have our model attempt to classify some news articles from the AG News Dataset. Articles have a single label 1-4\n",
    "\n",
    "1. World\n",
    "2. Sports\n",
    "3. Business\n",
    "4. Sci/Tech\n",
    "\n",
    "This is a constrained label space. We'll use the words World, Sports, Business, and Science as our targets for each of the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_markup(text: str) -> str:\n",
    "    text = re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text)\n",
    "    text = re.sub(r\"<.*?>+\", \"\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def ag_news_processor(path: str) -> Tuple[List[str], List[str], List[str]]:\n",
    "    ag_news_data = pd.read_csv(path)\n",
    "    labels = ag_news_data[\"Class Index\"].tolist()\n",
    "    titles = ag_news_data[\"Title\"].apply(lambda x: remove_markup(x)).tolist()\n",
    "    descriptions = ag_news_data[\"Description\"].apply(lambda x: remove_markup(x)).tolist()\n",
    "    return labels, titles, descriptions\n",
    "\n",
    "\n",
    "int_to_label_map = {1: \"world\", 2: \"sports\", 3: \"business\", 4: \"science\"}\n",
    "ag_news_labels, ag_news_titles, ag_news_descriptions = ag_news_processor(\"resources/ag_news_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ag_news_labels = map_ag_news_int_labels(ag_news_labels, int_to_label_map)\n",
    "ag_news_descriptions = [description.replace(\"\\\\\", \" \").strip() for description in ag_news_descriptions]\n",
    "ag_news_titles = [title.strip() for title in ag_news_titles]\n",
    "label_words = [\"World\", \"Sports\", \"Business\", \"Science\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input_texts = [\n",
    "    f\"Title: {ag_news_title} Description: {ag_news_description}\"\n",
    "    for ag_news_title, ag_news_description in zip(ag_news_titles, ag_news_descriptions)\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by trying out a basic instruction prompt to see what the model does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "On a phone?\n",
      "==================================\n",
      " (Choose up to three\n",
      "==================================\n",
      "\n",
      "==================================\n"
     ]
    }
   ],
   "source": [
    "prompt_template = \"To which category does this news article belong?\"\n",
    "sample_texts = [f\"{model_input_text} {prompt_template}\" for model_input_text in model_input_texts[0:3]]\n",
    "generation = model.generate(sample_texts, short_generation_config)\n",
    "for text in generation.generation[\"text\"]:\n",
    "    print(text)\n",
    "    print(\"==================================\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to constrain the model a bit by including the desired labels in the instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | What is it about\n",
      "==================================\n",
      " Multimedia Gallery TheStreet\n",
      "==================================\n",
      " 2) Which of the\n",
      "==================================\n"
     ]
    }
   ],
   "source": [
    "prompt_template = \"To which category does this news article belong (World, Sports, Business, Science)?\"\n",
    "sample_texts = [f\"{model_input_text} {prompt_template}\" for model_input_text in model_input_texts[0:3]]\n",
    "generation = model.generate(sample_texts, short_generation_config)\n",
    "for text in generation.generation[\"text\"]:\n",
    "    print(text)\n",
    "    print(\"==================================\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model doesn't really answer in the space that we want it to. Let's try with some few-shot examples to see if that helps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Business\n",
      "\n",
      "Title:\n",
      "==================================\n",
      " Business\n",
      "Title: Tiger\n",
      "==================================\n",
      " World\n",
      "Title: NASA\n",
      "==================================\n"
     ]
    }
   ],
   "source": [
    "prompt_template_prefix = \"\"\"Title: Lane drives in winning run in ninth Description: Jason Lane took an unusual post-game batting practice with hitting coach Gary Gaetti after a disappointing performance Friday night. Cateogry (World, Sports, Business, Science): Sports\n",
    "Title: Arson attack on Jewish centre in Paris (AFP) Description: AFP - A Jewish social centre in central Paris was destroyed by fire overnight in an anti-Semitic arson attack, city authorities said. Cateogry (World, Sports, Business, Science): World\n",
    "Title: Oil prices look set to dominate Description: The price of oil looks set to grab headlines as analysts forecast that its record-breaking run may well continue. Cateogry (World, Sports, Business, Science): Business\n",
    "Title: More Evidence for Past Water on Mars Description: Summary - (Aug 22, 2004) NASA #39;s Spirit rover has dug up plenty of evidence on slopes of  quot;Columbia Hills quot; that water once covered the area. Cateogry (World, Sports, Business, Science): World\n",
    "Title: Indexes in Japan fall short of hype Description: Japanese stocks have failed to measure up to an assessment made in April by Merrill Lynch #39;s chief global strategist, David Bowers, who said Japan was  quot;very much everyone #39;s favorite equity market. Cateogry (World, Sports, Business, Science): Business\n",
    "\"\"\"  # noqa\n",
    "prompt_template_postfix = \"Cateogry (World, Sports, Business, Science):\"\n",
    "sample_texts = [\n",
    "    f\"{prompt_template_prefix}{model_input_text} {prompt_template_postfix}\"\n",
    "    for model_input_text in model_input_texts[0:3]\n",
    "]\n",
    "generation = model.generate(sample_texts, short_generation_config)\n",
    "for text in generation.generation[\"text\"]:\n",
    "    # We'll limit ourselves to the single next token since we want it to respond that way\n",
    "    print(text)\n",
    "    print(\"==================================\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few-shot learning definitely helps a lot! We'll measure an accuracy sample below. However, there is nothing stoping the model from not selecting our labels. So can we do better? We can work around this by understanding the likliehood of our labels from the models perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'decoder.output_projection'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We're interested in the activations from the last layer of the model, because this will allow us to caculation the\n",
    "# likliehoods\n",
    "last_layer_name = model.module_names[-1]\n",
    "last_layer_name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to instantiate a tokenizer to obtain appropriate token indices for our labels. \n",
    "\n",
    "__NOTE__: All OPT models, regardless of size, used the same tokenizing. However, if you want to use a different type of model, a different tokenizer may be needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' World Sports Business Science'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "# Need to consider the token ids of our labels in the context of the prompt, as they may be different in context.\n",
    "tokenized_inputs = tokenizer([f\"{prompt_template} {label_word}\" for label_word in label_words], return_tensors=\"pt\")[\n",
    "    \"input_ids\"\n",
    "]\n",
    "\n",
    "label_token_ids = tokenized_inputs[:, -1]\n",
    "# If you ever need to move back from token ids, you can use tokenizer.decode or tokenizer.batch_decode\n",
    "tokenizer.decode(label_token_ids)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at how we can extract the likelihoods given the label tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_prompted_input = f\"{model_input_texts[0]} {prompt_template} {label_words[0]}\"\n",
    "# Create a prompt with one of the label words as a completion\n",
    "activations = model.get_activations(single_prompted_input, [last_layer_name], short_generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_with_highest_likelihood(\n",
    "    layer_matrix: torch.Tensor, label_token_ids: torch.Tensor, int_to_label_map: Dict[int, str]\n",
    ") -> str:\n",
    "    # The activations we can about are the last token (corresponding to our label token) and the values for our label\n",
    "    #  vocabulary\n",
    "    label_activations = layer_matrix[-1][label_token_ids].float()\n",
    "    softmax = nn.Softmax(dim=0)\n",
    "    label_distributions = softmax(label_activations)\n",
    "    # Plus one needed to correct into our label space of 1-4\n",
    "    max_label_index = torch.argmax(label_distributions) + 1\n",
    "    return int_to_label_map[max_label_index.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activations matrix shape: torch.Size([66, 50272])\n",
      "Predicted Label: sports\n"
     ]
    }
   ],
   "source": [
    "last_layer_matrix = activations.activations[0][last_layer_name]\n",
    "# The shape of this tensor should be number of input tokens by the vocabulary size (n x 50272)\n",
    "print(f\"Activations matrix shape: {last_layer_matrix.shape}\")\n",
    "predicted_label = get_label_with_highest_likelihood(last_layer_matrix, label_token_ids, int_to_label_map)\n",
    "print(f\"Predicted Label: {predicted_label}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "Time to compare our results across three methods. \n",
    "1. Measure the accuracy of our few-shot prompting approach.\n",
    "2. Measure the accuracy of our likelihood approach without few-shot.\n",
    "3. Measure the accuracy of our likelihood approach with few-shot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowercase_labels = [word.lower() for word in label_words]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_prefix = \"\"\"Title: Lane drives in winning run in ninth Description: Jason Lane took an unusual post-game batting practice with hitting coach Gary Gaetti after a disappointing performance Friday night. Cateogry (World, Sports, Business, Science): Sports\n",
    "Title: Arson attack on Jewish centre in Paris (AFP) Description: AFP - A Jewish social centre in central Paris was destroyed by fire overnight in an anti-Semitic arson attack, city authorities said. Cateogry (World, Sports, Business, Science): World\n",
    "Title: Oil prices look set to dominate Description: The price of oil looks set to grab headlines as analysts forecast that its record-breaking run may well continue. Cateogry (World, Sports, Business, Science): Business\n",
    "Title: More Evidence for Past Water on Mars Description: Summary - (Aug 22, 2004) NASA #39;s Spirit rover has dug up plenty of evidence on slopes of  quot;Columbia Hills quot; that water once covered the area. Cateogry (World, Sports, Business, Science): World\n",
    "Title: Indexes in Japan fall short of hype Description: Japanese stocks have failed to measure up to an assessment made in April by Merrill Lynch #39;s chief global strategist, David Bowers, who said Japan was  quot;very much everyone #39;s favorite equity market. Cateogry (World, Sports, Business, Science): Business\n",
    "\"\"\"  # noqa\n",
    "prompt_template_postfix = \"Cateogry (World, Sports, Business, Science):\"\n",
    "prompts = [\n",
    "    f\"{prompt_template_prefix}{model_input_text} {prompt_template_postfix}\" for model_input_text in model_input_texts\n",
    "]\n",
    "generation = model.generate(prompts, short_generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use tokens this time and consider just the first token\n",
    "first_predicted_tokens = [tokens[0].strip().lower() for tokens in generation.generation[\"tokens\"]]\n",
    "# If a token doesn't correspond to one of our labels, we'll randomly select one and count how many times that happens\n",
    "# for reporting\n",
    "predicted_labels = []\n",
    "n_no_match = 0\n",
    "for potential_prediction in first_predicted_tokens:\n",
    "    if potential_prediction in lowercase_labels:\n",
    "        predicted_labels.append(potential_prediction)\n",
    "    else:\n",
    "        n_no_match += 1\n",
    "        print(f\"Potential Prediction: {potential_prediction} does not match any label\")\n",
    "        predicted_labels.append(random.choice(lowercase_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy: 1.0\n",
      "Confusion Matrix with ordering ['world', 'sports', 'business', 'science']\n",
      "<function confusion_matrix at 0x7fc9e8567040>\n",
      "========================================================\n",
      "Label: world, F1: nan, Precision: nan,Recall: nan\n",
      "Label: sports, F1: 1.0, Precision: 1.0,Recall: 1.0\n",
      "Label: business, F1: 1.0, Precision: 1.0,Recall: 1.0\n",
      "Label: science, F1: nan, Precision: nan,Recall: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/david/Desktop/VectorRepositories/PromptEngineering/src/reference_implementations/prompting_vector_llms/llm_prompting_examples/metrics.py:18: RuntimeWarning: invalid value encountered in divide\n",
      "  recall = TP / (TP + FN)\n",
      "/Users/david/Desktop/VectorRepositories/PromptEngineering/src/reference_implementations/prompting_vector_llms/llm_prompting_examples/metrics.py:19: RuntimeWarning: invalid value encountered in divide\n",
      "  precision = TP / (TP + FP)\n"
     ]
    }
   ],
   "source": [
    "ag_news_metrics(predicted_labels, ag_news_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood No Few-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [f\"{model_input_text} {prompt_template} {label_words[0]}\" for model_input_text in model_input_texts]\n",
    "activations = model.get_activations(prompts, [last_layer_name], short_generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = []\n",
    "for activations_single_prompt in activations.activations:\n",
    "    last_layer_matrix = activations_single_prompt[last_layer_name]\n",
    "    predicted_label = get_label_with_highest_likelihood(last_layer_matrix, label_token_ids, int_to_label_map)\n",
    "    predicted_labels.append(predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy: 0.3333333333333333\n",
      "Confusion Matrix with ordering ['world', 'sports', 'business', 'science']\n",
      "<function confusion_matrix at 0x7fc9e8567040>\n",
      "========================================================\n",
      "Label: world, F1: nan, Precision: nan,Recall: nan\n",
      "Label: sports, F1: 0.5, Precision: 0.3333333333333333,Recall: 1.0\n",
      "Label: business, F1: nan, Precision: nan,Recall: 0.0\n",
      "Label: science, F1: nan, Precision: nan,Recall: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/david/Desktop/VectorRepositories/PromptEngineering/src/reference_implementations/prompting_vector_llms/llm_prompting_examples/metrics.py:18: RuntimeWarning: invalid value encountered in divide\n",
      "  recall = TP / (TP + FN)\n",
      "/Users/david/Desktop/VectorRepositories/PromptEngineering/src/reference_implementations/prompting_vector_llms/llm_prompting_examples/metrics.py:19: RuntimeWarning: invalid value encountered in divide\n",
      "  precision = TP / (TP + FP)\n"
     ]
    }
   ],
   "source": [
    "ag_news_metrics(predicted_labels, ag_news_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood with Few-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_prefix = \"\"\"Title: Lane drives in winning run in ninth Description: Jason Lane took an unusual post-game batting practice with hitting coach Gary Gaetti after a disappointing performance Friday night. Cateogry (World, Sports, Business, Science): Sports\n",
    "Title: Arson attack on Jewish centre in Paris (AFP) Description: AFP - A Jewish social centre in central Paris was destroyed by fire overnight in an anti-Semitic arson attack, city authorities said. Cateogry (World, Sports, Business, Science): World\n",
    "Title: Oil prices look set to dominate Description: The price of oil looks set to grab headlines as analysts forecast that its record-breaking run may well continue. Cateogry (World, Sports, Business, Science): Business\n",
    "Title: More Evidence for Past Water on Mars Description: Summary - (Aug 22, 2004) NASA #39;s Spirit rover has dug up plenty of evidence on slopes of  quot;Columbia Hills quot; that water once covered the area. Cateogry (World, Sports, Business, Science): World\n",
    "Title: Indexes in Japan fall short of hype Description: Japanese stocks have failed to measure up to an assessment made in April by Merrill Lynch #39;s chief global strategist, David Bowers, who said Japan was  quot;very much everyone #39;s favorite equity market. Cateogry (World, Sports, Business, Science): Business\n",
    "\"\"\"  # noqa\n",
    "prompt_template_postfix = \"Cateogry (World, Sports, Business, Science):\"\n",
    "prompts = [\n",
    "    f\"{prompt_template_prefix}{model_input_text} {prompt_template_postfix}\" for model_input_text in model_input_texts\n",
    "]\n",
    "activations = model.get_activations(prompts, [last_layer_name], short_generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = []\n",
    "for activations_single_prompt in activations.activations:\n",
    "    last_layer_matrix = activations_single_prompt[last_layer_name]\n",
    "    predicted_label = get_label_with_highest_likelihood(last_layer_matrix, label_token_ids, int_to_label_map)\n",
    "    predicted_labels.append(predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy: 1.0\n",
      "Confusion Matrix with ordering ['world', 'sports', 'business', 'science']\n",
      "<function confusion_matrix at 0x7fc9e8567040>\n",
      "========================================================\n",
      "Label: world, F1: nan, Precision: nan,Recall: nan\n",
      "Label: sports, F1: 1.0, Precision: 1.0,Recall: 1.0\n",
      "Label: business, F1: 1.0, Precision: 1.0,Recall: 1.0\n",
      "Label: science, F1: nan, Precision: nan,Recall: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/david/Desktop/VectorRepositories/PromptEngineering/src/reference_implementations/prompting_vector_llms/llm_prompting_examples/metrics.py:18: RuntimeWarning: invalid value encountered in divide\n",
      "  recall = TP / (TP + FN)\n",
      "/Users/david/Desktop/VectorRepositories/PromptEngineering/src/reference_implementations/prompting_vector_llms/llm_prompting_examples/metrics.py:19: RuntimeWarning: invalid value encountered in divide\n",
      "  precision = TP / (TP + FP)\n"
     ]
    }
   ],
   "source": [
    "ag_news_metrics(predicted_labels, ag_news_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opt_notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
