{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import time\n",
    "from typing import List, Tuple\n",
    "\n",
    "import kscope\n",
    "import pandas as pd\n",
    "from metrics import map_ag_news_int_labels, report_metrics\n",
    "from transformers import AutoTokenizer\n",
    "from utils import get_label_token_ids, get_label_with_highest_likelihood, split_prompts_into_batches"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "There is a bit of documentation on how to interact with the large models [here](https://kaleidoscope-sdk.readthedocs.io/en/latest/). The relevant github links to the SDK are [here](https://github.com/VectorInstitute/kaleidoscope-sdk) and underlying code [here](https://github.com/VectorInstitute/kaleidoscope)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we connect to the service through which we'll interact with the LLMs and see which models are avaiable to us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a client connection to the kscope service\n",
    "client = kscope.Client(gateway_host=\"llm.cluster.local\", gateway_port=6001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show all supported models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OPT-175B', 'OPT-6.7B']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show all model instances that are currently active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'f6e55e38-2c01-4def-aaf9-d4531f9a2598',\n",
       "  'name': 'OPT-175B',\n",
       "  'state': 'ACTIVE'},\n",
       " {'id': '4213e687-fdaf-4046-a03c-ca59b484ad14',\n",
       "  'name': 'OPT-6.7B',\n",
       "  'state': 'ACTIVE'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.model_instances"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we obtain a handle to a model. In this example, let's use the OPT-175B model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = client.load_model(\"OPT-175B\")\n",
    "# If this model is not actively running, it will get launched in the background.\n",
    "# In this case, wait until it moves into an \"ACTIVE\" state before proceeding.\n",
    "while model.state != \"ACTIVE\":\n",
    "    time.sleep(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to configure the model to generate in the way we want it to. So we set a number of important parameters. For a discussion of the configuration parameters see: `src/reference_implementations/prompting_vector_llms/CONFIG_README.md`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_generation_config = {\"max_tokens\": 2, \"top_k\": 4, \"top_p\": 1.0, \"rep_penalty\": 1.0, \"temperature\": 1.0}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a basic prompt for factual information.\n",
    "\n",
    "__Note__ that if you run the cell multiple times, you'll get different responses due to sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nMont']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation = model.generate(\"What is the capital of Canada?\", short_generation_config)\n",
    "# Extract the text from the returned generation\n",
    "generation.generation[\"text\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to have our model attempt to classify some news articles from the AG News Dataset. Articles have a single label 1-4\n",
    "\n",
    "1. World\n",
    "2. Sports\n",
    "3. Business\n",
    "4. Sci/Tech\n",
    "\n",
    "This is a constrained label space. We'll use the words \"World\", \"Sports\", \"Business\", and \"Technology\" as generative LM targets for each of the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_markup(text: str) -> str:\n",
    "    text = re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text)\n",
    "    text = re.sub(r\"<.*?>+\", \"\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def ag_news_processor(path: str) -> Tuple[List[str], List[str], List[str]]:\n",
    "    ag_news_data = pd.read_csv(path)\n",
    "    labels = ag_news_data[\"Class Index\"].tolist()\n",
    "    titles = ag_news_data[\"Title\"].apply(lambda x: remove_markup(x)).tolist()\n",
    "    descriptions = ag_news_data[\"Description\"].apply(lambda x: remove_markup(x)).tolist()\n",
    "    return labels, titles, descriptions\n",
    "\n",
    "\n",
    "int_to_label_map = {1: \"world\", 2: \"sports\", 3: \"business\", 4: \"technology\"}\n",
    "ag_news_labels, ag_news_titles, ag_news_descriptions = ag_news_processor(\n",
    "    \"resources/ag_news_datasets/ag_news_sample.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ag_news_labels = map_ag_news_int_labels(ag_news_labels, int_to_label_map)\n",
    "ag_news_descriptions = [description.replace(\"\\\\\", \" \").strip() for description in ag_news_descriptions]\n",
    "ag_news_titles = [title.strip() for title in ag_news_titles]\n",
    "label_words = [\"World\", \"Sports\", \"Business\", \"Technology\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input_texts = [\n",
    "    f\"Title: {ag_news_title} Description: {ag_news_description}\"\n",
    "    for ag_news_title, ag_news_description in zip(ag_news_titles, ag_news_descriptions)\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by trying out a basic instruction prompt to see what the model does. You might also try some prompts from [this paper](https://arxiv.org/pdf/2212.04037.pdf). See Table 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (Check\n",
      "==================================\n",
      " Desktop News\n",
      "==================================\n",
      "\n",
      "\n",
      "\n",
      "==================================\n"
     ]
    }
   ],
   "source": [
    "prompt_template = \"To which category does this news article belong?\"\n",
    "sample_texts = [f\"{model_input_text} {prompt_template}\" for model_input_text in model_input_texts[0:3]]\n",
    "generation = model.generate(sample_texts, short_generation_config)\n",
    "for text in generation.generation[\"text\"]:\n",
    "    print(text)\n",
    "    print(\"==================================\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not well...Now let's try to constrain the model a bit by including the desired labels in the instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Next\n",
      "\n",
      "==================================\n",
      " Law,\n",
      "==================================\n",
      "____?\n",
      "==================================\n"
     ]
    }
   ],
   "source": [
    "prompt_template = \"From World, Sports, Business, Technology, the category is \"\n",
    "sample_texts = [f\"{model_input_text} {prompt_template}\" for model_input_text in model_input_texts[0:3]]\n",
    "generation = model.generate(sample_texts, short_generation_config)\n",
    "for text in generation.generation[\"text\"]:\n",
    "    print(text)\n",
    "    print(\"==================================\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model doesn't really answer in the space that we want it to. Let's try with some few-shot examples to see if that helps.\n",
    "\n",
    "__NOTE__: We have simply randomly picked the examples used in the 5-shot prompt. Different choices might be made, including 4-shot or 8-shot prompts so that categories are evenly represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Business\n",
      "\n",
      "==================================\n",
      " Business\n",
      "\n",
      "==================================\n",
      " Sports\n",
      "==================================\n"
     ]
    }
   ],
   "source": [
    "prompt_template_prefix = \"\"\"Title: Lane drives in winning run in ninth Description: Jason Lane took an unusual post-game batting practice with hitting coach Gary Gaetti after a disappointing performance Friday night. Category (World, Sports, Business, Technology): Sports\n",
    "Title: Arson attack on Jewish centre in Paris (AFP) Description: AFP - A Jewish social centre in central Paris was destroyed by fire overnight in an anti-Semitic arson attack, city authorities said. Category (World, Sports, Business, Technology): World\n",
    "Title: Oil prices look set to dominate Description: The price of oil looks set to grab headlines as analysts forecast that its record-breaking run may well continue. Category (World, Sports, Business, Technology): Business\n",
    "Title: Indexes in Japan fall short of hype Description: Japanese stocks have failed to measure up to an assessment made in April by Merrill Lynch #39;s chief global strategist, David Bowers, who said Japan was  quot;very much everyone #39;s favorite equity market. Category (World, Sports, Business, Technology): Business\n",
    "Title: UK Scientists Allowed to Clone Human Embryos (Reuters) Description: Reuters - British scientists said on Wednesday they had received permission to clone human embryos for medical research, in what they believe to be the first such license to be granted in Europe. Category (World, Sports, Business, Technology): Technology\n",
    "\"\"\"  # noqa\n",
    "prompt_template_postfix = \"Category (World, Sports, Business, Technology):\"\n",
    "sample_texts = [\n",
    "    f\"{prompt_template_prefix}{model_input_text} {prompt_template_postfix}\"\n",
    "    for model_input_text in model_input_texts[0:3]\n",
    "]\n",
    "generation = model.generate(sample_texts, short_generation_config)\n",
    "for text in generation.generation[\"text\"]:\n",
    "    print(text)\n",
    "    print(\"==================================\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few-shot learning definitely helps a lot! We'll measure accuracy on a sample of the AG news dataset below. However, there is nothing stoping the model from not selecting our labels. So can we do better? We can work around this by understanding the likelihood of our labels from the models perspective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'decoder.output_projection'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We're interested in the activations from the last layer of the model, because this will allow us to calculate the\n",
    "# likelihoods\n",
    "last_layer_name = model.module_names[-1]\n",
    "last_layer_name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last layer of the model corresponds to the probabilities of each token in the model vocabulary. That is, it is the conditional probability\n",
    "$$\n",
    "P(y_t \\vert y_{<t}, x),\n",
    "$$\n",
    "The probability distribution over the vocabulary of the next token given the preceding tokens $y_{<t}$, and the prompt text $x$. Thus, for each token $y_{t}$ in our input, we get back a 50K vector corresponding to the probabilities over the vocabulary of $y_{t+1}$. We only care about the last token in our input, as it houses the probability of the, as yet, unseen token the model will generate.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to instantiate a tokenizer to obtain appropriate token indices for our labels. \n",
    "\n",
    "__NOTE__: All OPT models, regardless of size, used the same tokenizer. However, if you want to use a different type of model, a different tokenizer may be needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' World Sports Business Technology'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "label_token_ids = get_label_token_ids(tokenizer, prompt_template, label_words)\n",
    "# If you ever need to move back from token ids, you can use tokenizer.decode or tokenizer.batch_decode\n",
    "tokenizer.decode(label_token_ids)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need the token ids of our labels to exact the probabilties from the vocabulary of the model. The token id corresponds to the index of the token in the vocabulary matrix of the underlying model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at how we can extract the likelihoods given the label tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_prompted_input = f\"{model_input_texts[0]} {prompt_template}\"\n",
    "# Create a prompt with one of the label words as a completion\n",
    "activations = model.get_activations(single_prompted_input, [last_layer_name], short_generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activations matrix shape: torch.Size([61, 50272])\n",
      "Predicted Label: business\n"
     ]
    }
   ],
   "source": [
    "last_layer_matrix = activations.activations[0][last_layer_name]\n",
    "# The shape of this tensor should be number of input tokens by the vocabulary size (n x 50272)\n",
    "print(f\"Activations matrix shape: {last_layer_matrix.shape}\")\n",
    "predicted_label = get_label_with_highest_likelihood(\n",
    "    last_layer_matrix, label_token_ids, int_to_label_map, right_shift=True\n",
    ")\n",
    "print(f\"Predicted Label: {predicted_label}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "Time to compare our results across three methods. \n",
    "1. Measure the accuracy of our few-shot prompting approach.\n",
    "2. Measure the accuracy of our likelihood approach without few-shot.\n",
    "3. Measure the accuracy of our likelihood approach with few-shot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowercase_labels = [word.lower() for word in label_words]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot only\n",
    "\n",
    "In this example, we'll use a 5-shot prompt, as we did above and perform a \"exact match\" with our label space. That is, we parse out the first token that the model produces in its generation and simply try to string match it to one of our four label strings.\n",
    "\n",
    "__Note__: Our generation configuration uses a `top_k = 4` which means that it picks from the top 4 vocabulary words (scaled by their relative probabilities). You could, try changing this to 1 to get a bit better factual extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number 1 Complete\n",
      "Batch number 2 Complete\n",
      "Batch number 3 Complete\n",
      "Potential Prediction: science does not match any label\n",
      "Batch number 4 Complete\n",
      "Batch number 5 Complete\n",
      "Batch number 6 Complete\n",
      "Batch number 7 Complete\n",
      "Batch number 8 Complete\n",
      "Potential Prediction: politics does not match any label\n",
      "Potential Prediction: politics does not match any label\n",
      "Batch number 9 Complete\n",
      "Potential Prediction: politics does not match any label\n",
      "Batch number 10 Complete\n"
     ]
    }
   ],
   "source": [
    "prompt_template_prefix = \"\"\"Title: Lane drives in winning run in ninth Description: Jason Lane took an unusual post-game batting practice with hitting coach Gary Gaetti after a disappointing performance Friday night. Category (World, Sports, Business, Technology): Sports\n",
    "Title: Arson attack on Jewish centre in Paris (AFP) Description: AFP - A Jewish social centre in central Paris was destroyed by fire overnight in an anti-Semitic arson attack, city authorities said. Category (World, Sports, Business, Technology): World\n",
    "Title: Oil prices look set to dominate Description: The price of oil looks set to grab headlines as analysts forecast that its record-breaking run may well continue. Category (World, Sports, Business, Technology): Business\n",
    "Title: Indexes in Japan fall short of hype Description: Japanese stocks have failed to measure up to an assessment made in April by Merrill Lynch #39;s chief global strategist, David Bowers, who said Japan was  quot;very much everyone #39;s favorite equity market. Category (World, Sports, Business, Technology): Business\n",
    "Title: UK Scientists Allowed to Clone Human Embryos (Reuters) Description: Reuters - British scientists said on Wednesday they had received permission to clone human embryos for medical research, in what they believe to be the first such license to be granted in Europe. Category (World, Sports, Business, Technology): Technology\n",
    "\"\"\"  # noqa\n",
    "prompt_template_postfix = \"Category (World, Sports, Business, Technology):\"\n",
    "prompts = [\n",
    "    f\"{prompt_template_prefix}{model_input_text} {prompt_template_postfix}\" for model_input_text in model_input_texts\n",
    "]\n",
    "prompt_batches = split_prompts_into_batches(prompts)\n",
    "predicted_labels = []\n",
    "for batch_num, prompt_batch in enumerate(prompt_batches):\n",
    "    generation = model.generate(prompt_batch, short_generation_config)\n",
    "    print(f\"Batch number {batch_num+1} Complete\")\n",
    "    # We'll use tokens this time and consider just the first token\n",
    "    first_predicted_tokens = [tokens[0].strip().lower() for tokens in generation.generation[\"tokens\"]]\n",
    "    # If a token doesn't correspond to one of our labels, we'll randomly select one and count how many times that\n",
    "    # happens for reporting\n",
    "    n_no_match = 0\n",
    "    for potential_prediction in first_predicted_tokens:\n",
    "        if potential_prediction in lowercase_labels:\n",
    "            predicted_labels.append(potential_prediction)\n",
    "        else:\n",
    "            n_no_match += 1\n",
    "            print(f\"Potential Prediction: {potential_prediction} does not match any label\")\n",
    "            predicted_labels.append(random.choice(lowercase_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy: 0.67\n",
      "Confusion Matrix with ordering ['world', 'sports', 'business', 'technology']\n",
      "[[14  3  8  3]\n",
      " [ 4 15  2  0]\n",
      " [ 2  0 19  2]\n",
      " [ 2  0  7 19]]\n",
      "========================================================\n",
      "Label: world, F1: 0.56, Precision: 0.5, Recall: 0.6363636363636364\n",
      "Label: sports, F1: 0.7692307692307692, Precision: 0.7142857142857143, Recall: 0.8333333333333334\n",
      "Label: business, F1: 0.6440677966101694, Precision: 0.8260869565217391, Recall: 0.5277777777777778\n",
      "Label: technology, F1: 0.7307692307692307, Precision: 0.6785714285714286, Recall: 0.7916666666666666\n"
     ]
    }
   ],
   "source": [
    "report_metrics(predicted_labels, ag_news_labels, labels_order=[\"world\", \"sports\", \"business\", \"technology\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood No Few-shot\n",
    "\n",
    "In this example, we do not incorporate any demonstrations into the prompt (zero-shot prompt). From our experience above, the model does not do a good job generating responses that correspond to our label space. So rather than trying to match responses to our labels as strings, we extract the probabilties of our labels (see example above), as estimated by the model's verbalizer, and select the label with the highest probability as the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number 1 Complete\n",
      "Batch number 2 Complete\n",
      "Batch number 3 Complete\n",
      "Batch number 4 Complete\n",
      "Batch number 5 Complete\n",
      "Batch number 6 Complete\n",
      "Batch number 7 Complete\n",
      "Batch number 8 Complete\n",
      "Batch number 9 Complete\n",
      "Batch number 10 Complete\n"
     ]
    }
   ],
   "source": [
    "prompts = [f\"{model_input_text} {prompt_template}\" for model_input_text in model_input_texts]\n",
    "# For memory management, we split the prompts into batches of size 10\n",
    "predicted_labels = []\n",
    "prompt_batches = split_prompts_into_batches(prompts)\n",
    "for batch_num, prompt_batch in enumerate(prompt_batches):\n",
    "    activations = model.get_activations(prompt_batch, [last_layer_name], short_generation_config)\n",
    "    print(f\"Batch number {batch_num+1} Complete\")\n",
    "    for activations_single_prompt in activations.activations:\n",
    "        last_layer_matrix = activations_single_prompt[last_layer_name]\n",
    "        predicted_label = get_label_with_highest_likelihood(\n",
    "            last_layer_matrix, label_token_ids, int_to_label_map, right_shift=True\n",
    "        )\n",
    "        predicted_labels.append(predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy: 0.43\n",
      "Confusion Matrix with ordering ['world', 'sports', 'business', 'technology']\n",
      "[[26  1  1  0]\n",
      " [17  4  0  0]\n",
      " [13  0 10  0]\n",
      " [20  0  5  3]]\n",
      "========================================================\n",
      "Label: world, F1: 0.5, Precision: 0.9285714285714286, Recall: 0.34210526315789475\n",
      "Label: sports, F1: 0.3076923076923077, Precision: 0.19047619047619047, Recall: 0.8\n",
      "Label: business, F1: 0.5128205128205128, Precision: 0.43478260869565216, Recall: 0.625\n",
      "Label: technology, F1: 0.19354838709677416, Precision: 0.10714285714285714, Recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "report_metrics(predicted_labels, ag_news_labels, labels_order=[\"world\", \"sports\", \"business\", \"technology\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood with Few-Shot\n",
    "\n",
    "The zero-shot prompt combined with likelihood estimation for our label space doesn't do a great job, but it is likely a lot better than if we had tried to exact match the generation. Let's combine the two approaches. We'll use a 5-shot prompt, as we did in the exact match example above, but now we'll use likelihood over our labels as the prediction mechanism rather than exact matching the first generated token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number 1 Complete\n",
      "Batch number 2 Complete\n",
      "Batch number 3 Complete\n",
      "Batch number 4 Complete\n",
      "Batch number 5 Complete\n",
      "Batch number 6 Complete\n",
      "Batch number 7 Complete\n",
      "Batch number 8 Complete\n",
      "Batch number 9 Complete\n",
      "Batch number 10 Complete\n"
     ]
    }
   ],
   "source": [
    "prompt_template_prefix = \"\"\"Title: Lane drives in winning run in ninth Description: Jason Lane took an unusual post-game batting practice with hitting coach Gary Gaetti after a disappointing performance Friday night. Category (World, Sports, Business, Technology): Sports\n",
    "Title: Arson attack on Jewish centre in Paris (AFP) Description: AFP - A Jewish social centre in central Paris was destroyed by fire overnight in an anti-Semitic arson attack, city authorities said. Category (World, Sports, Business, Technology): World\n",
    "Title: Oil prices look set to dominate Description: The price of oil looks set to grab headlines as analysts forecast that its record-breaking run may well continue. Category (World, Sports, Business, Technology): Business\n",
    "Title: Indexes in Japan fall short of hype Description: Japanese stocks have failed to measure up to an assessment made in April by Merrill Lynch #39;s chief global strategist, David Bowers, who said Japan was  quot;very much everyone #39;s favorite equity market. Category (World, Sports, Business, Technology): Business\n",
    "Title: UK Scientists Allowed to Clone Human Embryos (Reuters) Description: Reuters - British scientists said on Wednesday they had received permission to clone human embryos for medical research, in what they believe to be the first such license to be granted in Europe. Category (World, Sports, Business, Technology): Technology\n",
    "\"\"\"  # noqa\n",
    "prompt_template_postfix = \"Category (World, Sports, Business, Technology):\"\n",
    "prompts = [\n",
    "    f\"{prompt_template_prefix}{model_input_text} {prompt_template_postfix}\" for model_input_text in model_input_texts\n",
    "]\n",
    "# For memory management, we split the prompts into batches of size 10\n",
    "predicted_labels = []\n",
    "prompt_batches = split_prompts_into_batches(prompts)\n",
    "for batch_num, prompt_batch in enumerate(prompt_batches):\n",
    "    activations = model.get_activations(prompt_batch, [last_layer_name], short_generation_config)\n",
    "    print(f\"Batch number {batch_num+1} Complete\")\n",
    "    for activations_single_prompt in activations.activations:\n",
    "        last_layer_matrix = activations_single_prompt[last_layer_name]\n",
    "        predicted_label = get_label_with_highest_likelihood(\n",
    "            last_layer_matrix, label_token_ids, int_to_label_map, right_shift=True\n",
    "        )\n",
    "        predicted_labels.append(predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy: 0.74\n",
      "Confusion Matrix with ordering ['world', 'sports', 'business', 'technology']\n",
      "[[17  1  8  2]\n",
      " [ 0 21  0  0]\n",
      " [ 1  0 22  0]\n",
      " [ 0  0 14 14]]\n",
      "========================================================\n",
      "Label: world, F1: 0.7391304347826088, Precision: 0.6071428571428571, Recall: 0.9444444444444444\n",
      "Label: sports, F1: 0.9767441860465117, Precision: 1.0, Recall: 0.9545454545454546\n",
      "Label: business, F1: 0.6567164179104478, Precision: 0.9565217391304348, Recall: 0.5\n",
      "Label: technology, F1: 0.6363636363636364, Precision: 0.5, Recall: 0.875\n"
     ]
    }
   ],
   "source": [
    "report_metrics(predicted_labels, ag_news_labels, labels_order=[\"world\", \"sports\", \"business\", \"technology\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opt_notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
