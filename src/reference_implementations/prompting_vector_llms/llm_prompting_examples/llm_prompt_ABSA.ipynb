{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.8.13 (default, Mar 28 2022, 11:38:47) \n",
      "[GCC 7.5.0]\n",
      "PyTorch version: 1.13.1+cu117\n",
      "Transformers version: 4.26.1\n"
     ]
    }
   ],
   "source": [
    "# Installing libraries required for this task\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import sklearn.metrics\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "\n",
    "# vector LLM toolkit \n",
    "import lingua\n",
    "\n",
    "# Print version information - check you are using correct environment\n",
    "print(\"Python version: \" + sys.version)\n",
    "print(\"PyTorch version: \" + torch.__version__)\n",
    "print(\"Transformers version: \" + transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "Next, We will be starting with connecting to the Lingua service through which we can connect to the large language model, OPT-175B. We will also be checking how many models are available to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a client connection to the Lingua service\n",
    "client = lingua.Client(gateway_host=\"llm.cluster.local\", gateway_port=3001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OPT-175B', 'OPT-6.7B']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking how many models are available for use\n",
    "client.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'b11f3264-9c03-4114-9d56-d39a0fa63640',\n",
       "  'name': 'OPT-175B',\n",
       "  'state': 'ACTIVE'},\n",
       " {'id': 'c5ea8da7-3384-4c7b-b47f-95ed1a485897',\n",
       "  'name': 'OPT-6.7B',\n",
       "  'state': 'ACTIVE'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking how many model instances are active\n",
    "client.model_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this notebook, we will be focusing on OPT-175B\n",
    "\n",
    "model = client.load_model(\"OPT-175B\")\n",
    "# If this model is not actively running, it will get launched in the background.\n",
    "# In this case, wait until it moves into an \"ACTIVE\" state before proceeding.\n",
    "while model.state != \"ACTIVE\":\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading model and tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to instantiate a tokenizer to obtain appropriate token indices for our labels.\n",
    "NOTE: All OPT models, regardless of size, used the same tokenizing. However, if you want to use a different type of model, a different tokenizer may be needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A class for the dataset\n",
    "    ...\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    df : pandas dataframe\n",
    "        Dataset in the format of a pandas dataframe. \n",
    "        Ensure it has columns named sentence_with_full_prompt and aspect_term_polarity\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    \"\"\"\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        text_prompt = row['sentence_with_full_prompt']\n",
    "        polarity = row['aspect_term_polarity']\n",
    "        text = row['text']\n",
    "#         if index == 100:\n",
    "        return text, text_prompt, polarity\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "def flatten(l):\n",
    "    '''\n",
    "    Flattens list of lists to list\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    l (list): \n",
    "        list of lists\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    list (list): \n",
    "        Flattened list\n",
    "    '''\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "# Stylistic function\n",
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown(string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some prompts\n",
    "\n",
    "#### Prompt Setup for Input\n",
    "* `df['sentence_with_prompt'] = 'Sentence: ' + df['text'] + ' ' + 'Sentiment on ' + df['aspect_term'] + ' is'`\n",
    "\n",
    "\n",
    "* `df['sentence_with_prompt'] = 'Sentence: ' + df['text'] + ' ' + 'Sentiment on ' + df['aspect_term'] + ' is positive or negative? It is'`\n",
    "\n",
    "\n",
    "* `df['sentence_with_prompt'] = 'Answer the question using the sentence provided. \\nQuestion: What is the sentiment on ' + df['aspect_term'] + ' - positive, negative, or neutral?' + '\\nSentence: ' + df['text'] + '\\nAnswer:'`\n",
    "\n",
    "\n",
    "* `df['sentence_with_prompt'] = 'Sentence: ' + df['text'] + ' ' + 'The sentiment associated with ' + df['aspect_term'] + ' is'`\n",
    "\n",
    "\n",
    "* `df['sentence_with_prompt'] = '\\nSentence: ' + df['text'] + ' ' + '\\nQuestion: What is the sentiment on ' + df['aspect_term'] + '? \\nAnswer:'`\n",
    "\n",
    "#### Few-shot Examples\n",
    "* `promptStarting = 'Sentence: Albert Einstein was one of the greatest intellects of his time. Sentiment on Albert Einstein is positive. \\nSentence: The sweet lassi was excellent as was the lamb chettinad and the garlic naan but the rasamalai was forgettable. Sentiment on rasmalai is negative. '`\n",
    "\n",
    "\n",
    "* `promptStarting = 'Sentence: Their pizza was good, but the service was bad. Sentiment on pizza is positive. \\nSentence: I charge it at night and skip taking the cord with me because its''s too heavy. Sentiment on cord is negative. \\nSentence: My suggestion is to eat family style because you''ll want to try the other dishes. Sentiment on dishes is neutral. '`\n",
    "\n",
    "\n",
    "* `promptStarting = 'Sentence: Albert Einstein was one of the greatest intellects of his time. Sentiment on Albert Einstein is positive. \\nSentence: The sweet lassi was excellent as was the lamb chettinad and the garlic naan but the rasamalai was forgettable. Sentiment on rasmalai is negative. \\nSentence: I had my software installed and ready to go, but the system crashed. Sentiment on software is neutral. '`\n",
    "\n",
    "\n",
    "* `promptStarting = 'Sentence: The sweet lassi was excellent as was the lamb chettinad and the garlic naan but the rasamalai was forgettable. Sentiment on rasmalai is negative. \\nSentence: I had my software installed and ready to go, but the system crashed. Sentiment on software is neutral. \\nSentence: Albert Einstein was one of the greatest intellects of his time. Sentiment on Albert Einstein is positive.'`\n",
    "\n",
    "\n",
    "* `promptStarting = 'Sentence: Albert Einstein was one of the greatest intellects of his time. Sentiment on Albert Einstein is positive or negative? It is positive. \\nSentence: The sweet lassi was excellent as was the lamb chettinad and the garlic naan but the rasamalai was forgettable. Sentiment on rasmalai is positive or negative? It is negative. '`\n",
    "\n",
    "\n",
    "* `promptStarting = 'Sentence: Albert Einstein was one of the greatest intellects of his time. The sentiment associated with Albert Einstein is positive. \\nSentence: The sweet lassi was excellent as was the lamb chettinad and the garlic naan but the rasamalai was forgettable. The sentiment associated with rasmalai is negative. '`\n",
    "\n",
    "\n",
    "* `promptStarting = 'Sentence: Albert Einstein was one of the greatest intellects of his time. The sentiment associated with Albert Einstein is positive. \\nSentence: The sweet lassi was excellent as was the lamb chettinad and the garlic naan but the rasamalai was forgettable. The sentiment associated with rasmalai is negative. \\nSentence: I had my software installed and ready to go, but the system crashed. The sentiment associated with software is neutral. '`\n",
    "\n",
    "\n",
    "* `promptStarting = 'Sentence: Albert Einstein was one of the greatest intellects of his time. \\nQuestion: What is the sentiment on Albert Einstein? \\nAnswer: postive \\n\\nSentence: The sweet lassi was excellent as was the lamb chettinad and the garlic naan but the rasamalai was forgettable. \\nQuestion: What is the sentiment on rasmalai? \\nAnswer: negative '`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will be showing accuracy on three different approaches, few-shot, zero-shot and zero-shot with Sentiment Classifier.\n",
    "\n",
    "* few-shot approach [`few-shot`] (i.e. give some example sentences for the model to determine what should come next for the input sentence) \n",
    "* zero-shot approach [`zero-shot`] (i.e. give the model the input sentence and ask for the sentiment) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_type = \"few-shot\" # other option here \"zero-shot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This csv file contains customer reviews of laptops collected in 2014 with size of around 469\n",
    "path=\"/scratch/ssd004/scratch/akshita/vector_intern23/PromptEngineering/src/reference_implementations/prompting_vector_llms/llm_prompting_examples/resources/absa_datasets/\"\n",
    "datasets_list = [path+'Laptops_Test_Gold.csv'] #Here you can add more datasets like 'Laptop_Train_v2.csv', 'Restaurants_Train_v2.csv', and 'Restaurants_Test_Gold.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba7f3c13e8df4d7da523f21e4b80f1f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1032 entries, 0 to 1031\n",
      "Data columns (total 6 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   id                    800 non-null    object \n",
      " 1   text                  1032 non-null   object \n",
      " 2   aspect_term           654 non-null    object \n",
      " 3   aspect_term_polarity  654 non-null    object \n",
      " 4   aspect_term_from      654 non-null    float64\n",
      " 5   aspect_term_to        654 non-null    float64\n",
      "dtypes: float64(2), object(4)\n",
      "memory usage: 48.5+ KB\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 469 entries, 0 to 1031\n",
      "Data columns (total 8 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   id                         314 non-null    object \n",
      " 1   text                       469 non-null    object \n",
      " 2   aspect_term                469 non-null    object \n",
      " 3   aspect_term_polarity       469 non-null    object \n",
      " 4   aspect_term_from           469 non-null    float64\n",
      " 5   aspect_term_to             469 non-null    float64\n",
      " 6   sentence_with_prompt       469 non-null    object \n",
      " 7   sentence_with_full_prompt  469 non-null    object \n",
      "dtypes: float64(2), object(6)\n",
      "memory usage: 33.0+ KB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba11f979b0b74dfe87bd104981ae8f74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for d in tqdm.notebook.tqdm(range(len(datasets_list))):\n",
    "    df = pd.read_csv(datasets_list[d])\n",
    "    \n",
    "    print('----------------------------------------------------------------')\n",
    "    df.info()\n",
    "    print()\n",
    "\n",
    "    # Delete any rows with null values\n",
    "    df = df.dropna(axis=0, how='any', subset=['aspect_term', 'aspect_term_polarity'])\n",
    "    \n",
    "    # Set the prompt format for the input sentence\n",
    "    df['sentence_with_prompt'] = 'Sentence: ' + df['text'] + ' ' + 'Sentiment on ' + df['aspect_term'] + ' is positive, negative, or neutral? It is'\n",
    "        \n",
    "    df = df.loc[df['aspect_term_polarity'].str.contains('positive') | df['aspect_term_polarity'].str.contains('negative')]\n",
    "    promptStarting = 'Sentence: Albert Einstein was one of the greatest intellects of his time. Sentiment on Albert Einstein is positive, negative, or neutral? It is positive. \\nSentence: The sweet lassi was excellent as was the lamb chettinad and the garlic naan but the rasamalai was forgettable. Sentiment on rasmalai is positive, negative, or neutral? It is negative. \\nSentence: I had my software installed and ready to go, but the system crashed. Sentiment on software is positive, negative, or neutral? It is neutral.'\n",
    "    ## zero-shot\n",
    "    \n",
    "    # for few-shot, we give more context to the model to improve the model generalizability. \n",
    "    if generation_type == 'few-shot': \n",
    "        df['sentence_with_full_prompt'] = promptStarting + '\\n' + df['sentence_with_prompt']\n",
    "    elif generation_type == 'zero-shot':\n",
    "        df['sentence_with_full_prompt'] = df['sentence_with_prompt']\n",
    "    else: \n",
    "        print('Do you want few-shot or zero-shot Set the generation_type correctly')    \n",
    "    \n",
    "    df.info()\n",
    "    \n",
    "    # Called the dataloader with custom created dataset as input\n",
    "    data = CustomDataset(df)\n",
    "    dataloader = DataLoader(data, batch_size=31)\n",
    "    \n",
    "    # initialize predictions and labels\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    ## parsing through the model alongth with vector hosted OPT-175B\n",
    "    for text, text_prompt, polarity in tqdm.notebook.tqdm(dataloader):\n",
    "        gen_text = model.generate(text_prompt, {\"max_tokens\": 1, \"top_k\": 4, \"top_p\": 3, \"rep_penalty\": 1.0, \"temperature\": 1.0}).generation[\"tokens\"]\n",
    "        first_predicted_tokens = [tokens[0].strip().lower() for tokens in gen_text]\n",
    "        sent = []\n",
    "        for i in range(len(first_predicted_tokens)):           \n",
    "            predictions.append(first_predicted_tokens[i])\n",
    "        labels.append(list(polarity))\n",
    "        \n",
    "    labels = flatten(labels)\n",
    "    # The labels associated with the dataset\n",
    "    labels_order = [\"positive\", \"negative\"]\n",
    "\n",
    "    cm = sklearn.metrics.confusion_matrix(np.array(labels), np.array(predictions), labels=labels_order)\n",
    "    \n",
    "    FP = cm.sum(axis=0) - np.diag(cm)\n",
    "    FN = cm.sum(axis=1) - np.diag(cm)\n",
    "    TP = np.diag(cm)\n",
    "\n",
    "    recall = TP / (TP + FN)\n",
    "    precision = TP / (TP + FP)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    print(f\"Prediction Accuracy: {TP.sum()/(cm.sum())}\")\n",
    "    \n",
    "    print(f\"Confusion Matrix with ordering {labels_order}\")\n",
    "    print(matrix)\n",
    "    print(\"========================================================\")\n",
    "    for label_index, label_name in enumerate(labels_order):\n",
    "        print(\n",
    "            f\"Label: {label_name}, F1: {f1[label_index]}, Precision: {recall[label_index]}, \"\n",
    "            f\"Recall: {precision[label_index]}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intern",
   "language": "python",
   "name": "py3-tf2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
