{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aspect Based Sentiment Analysis (ABSA)\n",
    "\n",
    "ABSA is used to identify the different aspects of a given target entity (such as a product or service) and the sentiment expressed towards each aspect in customer reviews or other text data. \n",
    "\n",
    "ABSA is further divided in two subtasks:\n",
    "\n",
    "Subtask 1 : It involves identifying aspect terms present in a given sentence containing pre-identified entities, such as restaurants. The goal is to extract the distinct aspect terms that refer to specific aspects of the target entity. Multi-word aspect terms should be considered as single terms.\n",
    "\n",
    "For example, \"I liked the service and the staff, but not the food”, “The food was nothing much, but I loved the staff”. Multi-word aspect terms (e.g., “hard disk”) should be treated as single terms (e.g., in “The hard disk is very noisy” the only aspect term is “hard disk”).\n",
    "\n",
    "\n",
    "Subtask 2 : It involves determining the polarity (positive, negative, neutral, or conflict) of each aspect term in a given sentence. For a set of aspect terms, the task is to identify their polarity based on the sentiment expressed towards them.\n",
    "\n",
    "For example:\n",
    "\n",
    "“I loved their fajitas” → {fajitas: positive}\n",
    "“I hated their fajitas, but their salads were great” → {fajitas: negative, salads: positive}\n",
    "“The fajitas are their first plate” → {fajitas: neutral}\n",
    "“The fajitas were great to taste, but not to see” → {fajitas: conflict}\n",
    "\n",
    "Subtask 3: It involves identifying the aspect categories discussed in a given sentence from a predefined set of categories such as price, food, service, ambience, and anecdotes/miscellaneous. The aspect categories are coarser than the aspect terms of subtask 1 and may not necessarily occur as terms in the sentence.\n",
    "\n",
    "For example, given the set of aspect categories {food, service, price, ambience, anecdotes/miscellaneous}:\n",
    "\n",
    "“The restaurant was too expensive” → {price}\n",
    "“The restaurant was expensive, but the menu was great” → {price, food}\n",
    "\n",
    "Subtask 4: It involves determining the polarity of each pre-identified aspect category (e.g., food, price). The goal is to identify the sentiment polarity of each category based on the sentiment expressed towards them in a given sentence.\n",
    "\n",
    "For example:\n",
    "\n",
    "“The restaurant was too expensive” → {price: negative}\n",
    "“The restaurant was expensive, but the menu was great” → {price: negative, food: positive}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.9.15 (main, Nov 24 2022, 08:29:02) \n",
      "[Clang 14.0.6 ]\n",
      "PyTorch version: 1.13.1\n",
      "Transformers version: 4.36.2\n"
     ]
    }
   ],
   "source": [
    "# Installing libraries required for this task\n",
    "\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "from enum import Enum\n",
    "from typing import List, Tuple\n",
    "\n",
    "# vector LLM toolkit\n",
    "import kscope\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics\n",
    "import torch\n",
    "import tqdm\n",
    "import transformers\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Print version information - check you are using correct environment\n",
    "print(\"Python version: \" + sys.version)\n",
    "print(\"PyTorch version: \" + torch.__version__)\n",
    "print(\"Transformers version: \" + transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "Next, we will be starting with connecting to the Kaleidoscope service through which we can connect to the large language model, LLaMA2-7B or other models available on the service. We will also be checking how many models are available to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a client connection to the Kaleidoscope service\n",
    "client = kscope.Client(gateway_host=\"llm.cluster.local\", gateway_port=3001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gpt2',\n",
       " 'llama2-7b',\n",
       " 'llama2-7b_chat',\n",
       " 'llama2-13b',\n",
       " 'llama2-13b_chat',\n",
       " 'llama2-70b',\n",
       " 'llama2-70b_chat',\n",
       " 'falcon-7b',\n",
       " 'falcon-40b',\n",
       " 'sdxl-turbo']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking what models are available in Kaleidoscope\n",
    "client.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '9690cf13-9b39-43d0-8ffb-70d8f765591d',\n",
       "  'name': 'llama2-7b',\n",
       "  'state': 'ACTIVE'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking how many model instances are active\n",
    "client.model_instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading model and setting up the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is active!\n"
     ]
    }
   ],
   "source": [
    "# For this notebook, we will be focusing on OPT-175B\n",
    "\n",
    "model = client.load_model(\"llama2-7b\")\n",
    "# If this model is not actively running, it will get launched in the background.\n",
    "# In this case, wait until it moves into an \"ACTIVE\" state before proceeding.\n",
    "while model.state != \"ACTIVE\":\n",
    "    time.sleep(1)\n",
    "\n",
    "print(\"The model is active!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A class for the dataset\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    df : pandas dataframe\n",
    "        Dataset in the format of a pandas dataframe.\n",
    "        Ensure it has columns named sentence_with_full_prompt and aspect_term_polarity\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame) -> None:\n",
    "        self.df = df\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[str, str, str]:\n",
    "        row = self.df.iloc[index]\n",
    "        text_prompt = row[\"sentence_with_full_prompt\"]\n",
    "        polarity = row[\"aspect_term_polarity\"]\n",
    "        text = row[\"text\"]\n",
    "        return text, text_prompt, polarity\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Examples\n",
    "\n",
    "In next section, some prompt examples are given which gives a demonstration on how to setup a prompt for the task of zero-shot and few-shot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt Setup for Input and Zero-shot examples\n",
    "\n",
    "* `df['sentence_with_prompt'] = 'Sentence: ' + df['text'] + ' ' + 'Sentiment on ' + df['aspect_term'] + ' is'`\n",
    "\n",
    "\n",
    "* `df['sentence_with_prompt'] = 'Sentence: ' + df['text'] + ' ' + 'Sentiment on ' + df['aspect_term'] + ' is positive or negative? It is'`\n",
    "\n",
    "\n",
    "* `df['sentence_with_prompt'] = 'Answer the question using the sentence provided. \\nQuestion: What is the sentiment on ' + df['aspect_term'] + ' - positive, negative, or neutral?' + '\\nSentence: ' + df['text'] + '\\nAnswer:'`\n",
    "\n",
    "\n",
    "* `df['sentence_with_prompt'] = 'Sentence: ' + df['text'] + ' ' + 'The sentiment associated with ' + df['aspect_term'] + ' is'`\n",
    "\n",
    "\n",
    "* `df['sentence_with_prompt'] = '\\nSentence: ' + df['text'] + ' ' + '\\nQuestion: What is the sentiment on ' + df['aspect_term'] + '? \\nAnswer:'`\n",
    "\n",
    "#### Few-shot Examples\n",
    "\n",
    "The examples below show few-shot demonstrations. These are to be prepended to the input and final question for the model to answer following the examples above. The first two examples below are two- and three-shot examples where the format is completion of \"Sentiment on `<aspect term>` is\" is used to produce a model response.\n",
    "\n",
    "* `demonstrations = 'Sentence: Albert Einstein was one of the greatest intellects of his time. Sentiment on Albert Einstein is positive. \\nSentence: The sweet lassi was excellent as was the lamb chettinad and the garlic naan but the rasamalai was forgettable. Sentiment on rasmalai is negative. '`\n",
    "\n",
    "\n",
    "* `demonstrations = 'Sentence: Their pizza was good, but the service was bad. Sentiment on pizza is positive. \\nSentence: I charge it at night and skip taking the cord with me because its''s too heavy. Sentiment on cord is negative. \\nSentence: My suggestion is to eat family style because you''ll want to try the other dishes. Sentiment on dishes is neutral. '`\n",
    "\n",
    "The examples below offer several other alternatives for formatting of the prompt to produce a response to the ABSA task from the model in a few-shot setting. These include both sentence completion and question-based forms.\n",
    "\n",
    "* `demonstrations = 'Sentence: Albert Einstein was one of the greatest intellects of his time. Sentiment on Albert Einstein is positive or negative? It is positive. \\nSentence: The sweet lassi was excellent as was the lamb chettinad and the garlic naan but the rasamalai was forgettable. Sentiment on rasmalai is positive or negative? It is negative. '`\n",
    "\n",
    "\n",
    "* `promptStarting = 'Sentence: Albert Einstein was one of the greatest intellects of his time. The sentiment associated with Albert Einstein is positive. \\nSentence: The sweet lassi was excellent as was the lamb chettinad and the garlic naan but the rasamalai was forgettable. The sentiment associated with rasmalai is negative. '`\n",
    "\n",
    "\n",
    "* `demonstrations = 'Sentence: Albert Einstein was one of the greatest intellects of his time. The sentiment associated with Albert Einstein is positive. \\nSentence: The sweet lassi was excellent as was the lamb chettinad and the garlic naan but the rasamalai was forgettable. The sentiment associated with rasmalai is negative. \\nSentence: I had my software installed and ready to go, but the system crashed. The sentiment associated with software is neutral. '`\n",
    "\n",
    "\n",
    "* `demonstrations = 'Sentence: Albert Einstein was one of the greatest intellects of his time. \\nQuestion: What is the sentiment on Albert Einstein? \\nAnswer: postive \\n\\nSentence: The sweet lassi was excellent as was the lamb chettinad and the garlic naan but the rasamalai was forgettable. \\nQuestion: What is the sentiment on rasmalai? \\nAnswer: negative '`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will be showing accuracy on two different approaches: Few-shot and Zero-shot prompting.\n",
    "\n",
    "* Zero-shot approach [\"`zero-shot`\"] (i.e. give the model the input sentence and ask for the sentiment).\n",
    "* Few-shot approach [\"`few-shot`\"] (i.e. give some example sentences for the model to determine what should come next for the input sentence, then ask for the sentiment of a new example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Types of prompt to be used.\n",
    "class PromptType(Enum):\n",
    "    FEW_SHOT = \"few-shot\"\n",
    "    ZERO_SHOT = \"zero-shot\"\n",
    "\n",
    "\n",
    "# Datasets available to use with this notebook.\n",
    "class AbsaDataset(Enum):\n",
    "    LAPTOPS_TRAIN_GOLD = \"Laptops_Train_v2.csv\"\n",
    "    # This csv file contains customer reviews of laptops collected in 2014 with size of around 469\n",
    "    LAPTOPS_TEST_GOLD = \"Laptops_Test_Gold.csv\"\n",
    "\n",
    "\n",
    "def get_dataset_path(dataset: AbsaDataset) -> str:\n",
    "    # location of all available datasets.\n",
    "    path_stub = \"resources/absa_datasets/\"\n",
    "    return os.path.join(path_stub, dataset.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_type = PromptType.FEW_SHOT\n",
    "absa_dataset = AbsaDataset.LAPTOPS_TEST_GOLD\n",
    "\n",
    "dataset_path = get_dataset_path(absa_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preprocessing and Setting up the Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_labels(label: str, filter_by: List[str] = [\"positive\", \"negative\", \"neutral\"]) -> bool:\n",
    "    label_matches = [label_filter in label for label_filter in filter_by]\n",
    "    return any(label_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1032 entries, 0 to 1031\n",
      "Data columns (total 6 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   id                    800 non-null    object \n",
      " 1   text                  1032 non-null   object \n",
      " 2   aspect_term           654 non-null    object \n",
      " 3   aspect_term_polarity  654 non-null    object \n",
      " 4   aspect_term_from      654 non-null    float64\n",
      " 5   aspect_term_to        654 non-null    float64\n",
      "dtypes: float64(2), object(4)\n",
      "memory usage: 48.5+ KB\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 638 entries, 0 to 1031\n",
      "Data columns (total 8 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   id                         410 non-null    object \n",
      " 1   text                       638 non-null    object \n",
      " 2   aspect_term                638 non-null    object \n",
      " 3   aspect_term_polarity       638 non-null    object \n",
      " 4   aspect_term_from           638 non-null    float64\n",
      " 5   aspect_term_to             638 non-null    float64\n",
      " 6   sentence_with_prompt       638 non-null    object \n",
      " 7   sentence_with_full_prompt  638 non-null    object \n",
      "dtypes: float64(2), object(6)\n",
      "memory usage: 44.9+ KB\n",
      "Unique Labels: ['positive' 'negative' 'neutral']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "print(\"----------------------------------------------------------------\")\n",
    "df.info()\n",
    "print()\n",
    "\n",
    "# Delete any rows with null values\n",
    "df = df.dropna(axis=0, how=\"any\", subset=[\"aspect_term\", \"aspect_term_polarity\"])\n",
    "\n",
    "# Set the prompt format for the input sentence (drawn from one of the example from above)\n",
    "df[\"sentence_with_prompt\"] = (\n",
    "    \"Sentence: \"\n",
    "    + df[\"text\"]\n",
    "    + \" \"\n",
    "    + \"Is the sentiment on \"\n",
    "    + df[\"aspect_term\"]\n",
    "    + \" positive, negative, or neutral? It is\"\n",
    ")\n",
    "\n",
    "# Make sure to index instances with positive, negative, neutral as polarity\n",
    "df = df.loc[df[\"aspect_term_polarity\"].apply(lambda x: filter_by_labels(x))]\n",
    "\n",
    "# Three shot demonstrations to include if we're doing a few-shot prompt\n",
    "demonstrations = (\n",
    "    \"Sentence: Albert Einstein was one of the greatest intellects of his time. Is the sentiment on Albert Einstein \"\n",
    "    \"positive, negative, or neutral? It is positive. \\nSentence: The sweet lassi was excellent as was the lamb \"\n",
    "    \"chettinad and the garlic naan but the rasamalai was forgettable. Is the sentiment on rasmalai positive, \"\n",
    "    \"negative, or neutral? It is negative. \\nSentence: I had my software installed and ready to go, but the system \"\n",
    "    \"crashed. Is the sentiment on the software positive, negative, or neutral? It is neutral.\"\n",
    ")\n",
    "# for few-shot, we give more context to the model to improve the model performance and generalizability.\n",
    "if generation_type is PromptType.FEW_SHOT:\n",
    "    df[\"sentence_with_full_prompt\"] = demonstrations + \"\\n\" + df[\"sentence_with_prompt\"]\n",
    "elif generation_type is PromptType.ZERO_SHOT:\n",
    "    df[\"sentence_with_full_prompt\"] = df[\"sentence_with_prompt\"]\n",
    "else:\n",
    "    raise ValueError(\"Invalid generation type: Please select from zero-shot or few-shot.\")\n",
    "\n",
    "df.info()\n",
    "print(f\"Unique Labels: {df['aspect_term_polarity'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt the language model for the full dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first take a look at an example of the prompts that we have created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL TEXT: Boot time is super fast, around anywhere from 35 seconds to 1 minute.\n",
      "\n",
      "PROMPT: Sentence: Albert Einstein was one of the greatest intellects of his time. Is the sentiment on Albert Einstein positive, negative, or neutral? It is positive. \n",
      "Sentence: The sweet lassi was excellent as was the lamb chettinad and the garlic naan but the rasamalai was forgettable. Is the sentiment on rasmalai positive, negative, or neutral? It is negative. \n",
      "Sentence: I had my software installed and ready to go, but the system crashed. Is the sentiment on the software positive, negative, or neutral? It is neutral.\n",
      "Sentence: Boot time is super fast, around anywhere from 35 seconds to 1 minute. Is the sentiment on Boot time positive, negative, or neutral? It is\n",
      "\n",
      "LABEL: positive\n"
     ]
    }
   ],
   "source": [
    "# Construct the dataloader with custom created dataset as input\n",
    "data = CustomDataset(df)\n",
    "dataloader = DataLoader(data, batch_size=2)\n",
    "\n",
    "# Grab the first example from the dataloader for inspection\n",
    "text, text_prompt, polarity = next(iter(dataloader))\n",
    "print(f\"ORIGINAL TEXT: {text[0]}\\n\")\n",
    "print(f\"PROMPT: {text_prompt[0]}\\n\")\n",
    "print(f\"LABEL: {polarity[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create configuration for the model. We're only looking for a short response. So we set the max tokens to be\n",
    "# generated to 1. For a discussion of the configuration parameters see:\n",
    "# src/reference_implementations/prompting_vector_llms/CONFIG_README.md\n",
    "generation_config = {\"max_tokens\": 1, \"top_k\": 1, \"top_p\": 1.0, \"temperature\": 1.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll first consider an example to see what the output looks like. Note that we sent the model a batch of prompts of size 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT TEXT: Sentence: Albert Einstein was one of the greatest intellects of his time. Is the sentiment on Albert Einstein positive, negative, or neutral? It is positive. \n",
      "Sentence: The sweet lassi was excellent as was the lamb chettinad and the garlic naan but the rasamalai was forgettable. Is the sentiment on rasmalai positive, negative, or neutral? It is negative. \n",
      "Sentence: I had my software installed and ready to go, but the system crashed. Is the sentiment on the software positive, negative, or neutral? It is neutral.\n",
      "Sentence: Boot time is super fast, around anywhere from 35 seconds to 1 minute. Is the sentiment on Boot time positive, negative, or neutral? It is\n",
      "Prompt 1 GENERATED TOKENS: ['positive']\n",
      "\n",
      "PROMPT TEXT: Sentence: Albert Einstein was one of the greatest intellects of his time. Is the sentiment on Albert Einstein positive, negative, or neutral? It is positive. \n",
      "Sentence: The sweet lassi was excellent as was the lamb chettinad and the garlic naan but the rasamalai was forgettable. Is the sentiment on rasmalai positive, negative, or neutral? It is negative. \n",
      "Sentence: I had my software installed and ready to go, but the system crashed. Is the sentiment on the software positive, negative, or neutral? It is neutral.\n",
      "Sentence: tech support would not fix the problem unless I bought your plan for $150 plus. Is the sentiment on tech support positive, negative, or neutral? It is\n",
      "Prompt 2 GENERATED TOKENS: ['negative']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text, text_prompt, polarity = next(iter(dataloader))\n",
    "generated_tokens_batch = model.generate(text_prompt, generation_config).generation[\"tokens\"]\n",
    "for index, prompt_tokens in enumerate(generated_tokens_batch):\n",
    "    print(f\"PROMPT TEXT: {text_prompt[index]}\")\n",
    "    print(f\"Prompt {index + 1} GENERATED TOKENS: {prompt_tokens}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7dda7adc2f8448bb60c3c4c76fa11f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a dataloader with a larger batch size to process full dataset.\n",
    "dataloader = DataLoader(data, batch_size=10)\n",
    "# initialize predictions and labels.\n",
    "raw_predictions = []\n",
    "labels = []\n",
    "\n",
    "for _, text_prompt, polarity in tqdm.notebook.tqdm(dataloader):\n",
    "    generated_tokens = model.generate(text_prompt, generation_config).generation[\"tokens\"]\n",
    "    # Note that we are looking at the models generated response and attempting to match it to one of the labels in\n",
    "    # our labels space. If the model produces a different token it is considered wrong.\n",
    "    first_predicted_tokens = [tokens[0].strip().lower() for tokens in generated_tokens]\n",
    "    raw_predictions.extend(first_predicted_tokens)\n",
    "    labels.extend(list(polarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction nothing does not match one of positive, negative, neutral\n",
      "Prediction pos does not match one of positive, negative, neutral\n",
      "Prediction pos does not match one of positive, negative, neutral\n",
      "Prediction definitely does not match one of positive, negative, neutral\n",
      "Prediction np does not match one of positive, negative, neutral\n",
      "Prediction not does not match one of positive, negative, neutral\n",
      "Prediction slightly does not match one of positive, negative, neutral\n",
      "Prediction mixed does not match one of positive, negative, neutral\n",
      "Prediction still does not match one of positive, negative, neutral\n",
      "Prediction a does not match one of positive, negative, neutral\n"
     ]
    }
   ],
   "source": [
    "# Postprocess the predictions. If any of the predictions are not the strings \"positive\", \"negative\", or \"neutral\" then\n",
    "# we will assign them to one of them randomly.\n",
    "label_strings = [\"positive\", \"negative\", \"neutral\"]\n",
    "predictions = []\n",
    "for prediction in raw_predictions:\n",
    "    if prediction not in label_strings:\n",
    "        print(f\"Prediction {prediction} does not match one of {', '.join(label_strings)}\")\n",
    "        predictions.append(random.choice(label_strings))\n",
    "    else:\n",
    "        predictions.append(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure the accuracy and construct the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy: 0.5031347962382445\n",
      "Confusion Matrix with ordering ['positive', 'neutral', 'negative']\n",
      "[[203 104  34]\n",
      " [ 67  66  36]\n",
      " [ 40  36  52]]\n",
      "========================================================\n",
      "Label: positive, F1: 0.6236559139784946, Precision: 0.6548387096774193, Recall: 0.5953079178885631\n",
      "Label: neutral, F1: 0.352, Precision: 0.32038834951456313, Recall: 0.3905325443786982\n",
      "Label: negative, F1: 0.4159999999999999, Precision: 0.4262295081967213, Recall: 0.40625\n"
     ]
    }
   ],
   "source": [
    "# The labels associated with the dataset\n",
    "labels_order = [\"positive\", \"neutral\", \"negative\"]\n",
    "\n",
    "cm = sklearn.metrics.confusion_matrix(np.array(labels), np.array(predictions), labels=labels_order)\n",
    "\n",
    "FP = cm.sum(axis=0) - np.diag(cm)\n",
    "FN = cm.sum(axis=1) - np.diag(cm)\n",
    "TP = np.diag(cm)\n",
    "\n",
    "recall = TP / (TP + FN)\n",
    "precision = TP / (TP + FP)\n",
    "f1 = 2 * (precision * recall) / (precision + recall)\n",
    "print(f\"Prediction Accuracy: {TP.sum()/(cm.sum())}\")\n",
    "\n",
    "print(f\"Confusion Matrix with ordering {labels_order}\")\n",
    "print(cm)\n",
    "print(\"========================================================\")\n",
    "for label_index, label_name in enumerate(labels_order):\n",
    "    print(\n",
    "        f\"Label: {label_name}, F1: {f1[label_index]}, Precision: {precision[label_index]}, \"\n",
    "        f\"Recall: {recall[label_index]}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model performs the task well above random guesses (33%) and seems to generate responses in the space of labels we are targetting (as most answers are in the desired label space). However, there is room for improvement, especially since accuracy associated with predicting positive only is around 53% given the label balance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompt_engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
