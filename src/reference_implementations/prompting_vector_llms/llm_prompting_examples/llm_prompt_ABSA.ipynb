{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aspect Based Sentiment Analysis ( ABSA )\n",
    "\n",
    "ABSA is used to identify the different aspects of a given target entity (such as a product or service) and the sentiment expressed towards each aspect in customer reviews or other text data. \n",
    "\n",
    "ABSA is further divided in two subtasks:\n",
    "\n",
    "Subtask 1 : It involves identifying aspect terms present in a given sentence containing pre-identified entities, such as restaurants. The goal is to extract the distinct aspect terms that refer to specific aspects of the target entity. Multi-word aspect terms should be considered as single terms.\n",
    "\n",
    "For example, \"I liked the service and the staff, but not the food”, “The food was nothing much, but I loved the staff”. Multi-word aspect terms (e.g., “hard disk”) should be treated as single terms (e.g., in “The hard disk is very noisy” the only aspect term is “hard disk”).\n",
    "\n",
    "\n",
    "Subtask 2 : It involves determining the polarity (positive, negative, neutral, or conflict) of each aspect term in a given sentence. For a set of aspect terms, the task is to identify their polarity based on the sentiment expressed towards them.\n",
    "\n",
    "For example:\n",
    "\n",
    "“I loved their fajitas” → {fajitas: positive}\n",
    "“I hated their fajitas, but their salads were great” → {fajitas: negative, salads: positive}\n",
    "“The fajitas are their first plate” → {fajitas: neutral}\n",
    "“The fajitas were great to taste, but not to see” → {fajitas: conflict}\n",
    "\n",
    "Subtask 3: It involves identifying the aspect categories discussed in a given sentence from a predefined set of categories such as price, food, service, ambience, and anecdotes/miscellaneous. The aspect categories are coarser than the aspect terms of subtask 1 and may not necessarily occur as terms in the sentence.\n",
    "\n",
    "For example, given the set of aspect categories {food, service, price, ambience, anecdotes/miscellaneous}:\n",
    "\n",
    "“The restaurant was too expensive” → {price}\n",
    "“The restaurant was expensive, but the menu was great” → {price, food}\n",
    "\n",
    "Subtask 4: It involves determining the polarity of each pre-identified aspect category (e.g., food, price). The goal is to identify the sentiment polarity of each category based on the sentiment expressed towards them in a given sentence.\n",
    "\n",
    "For example:\n",
    "\n",
    "“The restaurant was too expensive” → {price: negative}\n",
    "“The restaurant was expensive, but the menu was great” → {price: negative, food: positive}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.9.15 (main, Nov 24 2022, 08:29:02) \n",
      "[Clang 14.0.6 ]\n",
      "PyTorch version: 1.13.1\n",
      "Transformers version: 4.27.3\n"
     ]
    }
   ],
   "source": [
    "# Installing libraries required for this task\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from typing import Tuple\n",
    "\n",
    "# vector LLM toolkit\n",
    "import kscope\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics\n",
    "import torch\n",
    "import tqdm\n",
    "import transformers\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Print version information - check you are using correct environment\n",
    "print(\"Python version: \" + sys.version)\n",
    "print(\"PyTorch version: \" + torch.__version__)\n",
    "print(\"Transformers version: \" + transformers.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "Next, We will be starting with connecting to the Kaleidoscope service through which we can connect to the large language model, OPT-175B. We will also be checking how many models are available to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a client connection to the Kaleidoscope service\n",
    "client = kscope.Client(gateway_host=\"llm.cluster.local\", gateway_port=3001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OPT-175B', 'OPT-6.7B']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking how many models are available for use\n",
    "client.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'b11f3264-9c03-4114-9d56-d39a0fa63640',\n",
       "  'name': 'OPT-175B',\n",
       "  'state': 'ACTIVE'},\n",
       " {'id': 'c5ea8da7-3384-4c7b-b47f-95ed1a485897',\n",
       "  'name': 'OPT-6.7B',\n",
       "  'state': 'ACTIVE'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking how many model instances are active\n",
    "client.model_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this notebook, we will be focusing on OPT-175B\n",
    "\n",
    "model = client.load_model(\"OPT-175B\")\n",
    "# If this model is not actively running, it will get launched in the background.\n",
    "# In this case, wait until it moves into an \"ACTIVE\" state before proceeding.\n",
    "while model.state != \"ACTIVE\":\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading model and tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to instantiate a tokenizer to obtain appropriate token indices for our labels.\n",
    "NOTE: All OPT models, regardless of size, used the same tokenizing. However, if you want to use a different type of model, a different tokenizer may be needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A class for the dataset\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    df : pandas dataframe\n",
    "        Dataset in the format of a pandas dataframe.\n",
    "        Ensure it has columns named sentence_with_full_prompt and aspect_term_polarity\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame) -> None:\n",
    "        self.df = df\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[str, str, str]:\n",
    "        row = self.df.iloc[index]\n",
    "        text_prompt = row[\"sentence_with_full_prompt\"]\n",
    "        polarity = row[\"aspect_term_polarity\"]\n",
    "        text = row[\"text\"]\n",
    "        return text, text_prompt, polarity\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Examples\n",
    "\n",
    "In next section, some prompt examples are given which gives a demonstration on how to setup a prompt for the task of zero-shot and few-shot.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt Setup for Input and Zero-shot examples\n",
    "\n",
    "* `df['sentence_with_prompt'] = 'Sentence: ' + df['text'] + ' ' + 'Sentiment on ' + df['aspect_term'] + ' is'`\n",
    "\n",
    "\n",
    "* `df['sentence_with_prompt'] = 'Sentence: ' + df['text'] + ' ' + 'Sentiment on ' + df['aspect_term'] + ' is positive or negative? It is'`\n",
    "\n",
    "\n",
    "* `df['sentence_with_prompt'] = 'Answer the question using the sentence provided. \\nQuestion: What is the sentiment on ' + df['aspect_term'] + ' - positive, negative, or neutral?' + '\\nSentence: ' + df['text'] + '\\nAnswer:'`\n",
    "\n",
    "\n",
    "* `df['sentence_with_prompt'] = 'Sentence: ' + df['text'] + ' ' + 'The sentiment associated with ' + df['aspect_term'] + ' is'`\n",
    "\n",
    "\n",
    "* `df['sentence_with_prompt'] = '\\nSentence: ' + df['text'] + ' ' + '\\nQuestion: What is the sentiment on ' + df['aspect_term'] + '? \\nAnswer:'`\n",
    "\n",
    "#### Few-shot Examples\n",
    "\n",
    "The examples below show few-shot demonstrations. These are to be prepended to the input and final question for the model to answer following the examples above. The first two examples below are two- and three-shot examples where the format is completion of \"Sentiment on `<aspect term>` is\" is used to produce a model response.\n",
    "\n",
    "* `demonstrations = 'Sentence: Albert Einstein was one of the greatest intellects of his time. Sentiment on Albert Einstein is positive. \\nSentence: The sweet lassi was excellent as was the lamb chettinad and the garlic naan but the rasamalai was forgettable. Sentiment on rasmalai is negative. '`\n",
    "\n",
    "\n",
    "* `demonstrations = 'Sentence: Their pizza was good, but the service was bad. Sentiment on pizza is positive. \\nSentence: I charge it at night and skip taking the cord with me because its''s too heavy. Sentiment on cord is negative. \\nSentence: My suggestion is to eat family style because you''ll want to try the other dishes. Sentiment on dishes is neutral. '`\n",
    "\n",
    "The examples below offer several other alternatives for formatting of the prompt to produce a response to the ABSA task from the model in a few-shot setting. These include both sentence completion and question-based forms.\n",
    "\n",
    "* `demonstrations = 'Sentence: Albert Einstein was one of the greatest intellects of his time. Sentiment on Albert Einstein is positive or negative? It is positive. \\nSentence: The sweet lassi was excellent as was the lamb chettinad and the garlic naan but the rasamalai was forgettable. Sentiment on rasmalai is positive or negative? It is negative. '`\n",
    "\n",
    "\n",
    "* `promptStarting = 'Sentence: Albert Einstein was one of the greatest intellects of his time. The sentiment associated with Albert Einstein is positive. \\nSentence: The sweet lassi was excellent as was the lamb chettinad and the garlic naan but the rasamalai was forgettable. The sentiment associated with rasmalai is negative. '`\n",
    "\n",
    "\n",
    "* `demonstrations = 'Sentence: Albert Einstein was one of the greatest intellects of his time. The sentiment associated with Albert Einstein is positive. \\nSentence: The sweet lassi was excellent as was the lamb chettinad and the garlic naan but the rasamalai was forgettable. The sentiment associated with rasmalai is negative. \\nSentence: I had my software installed and ready to go, but the system crashed. The sentiment associated with software is neutral. '`\n",
    "\n",
    "\n",
    "* `demonstrations = 'Sentence: Albert Einstein was one of the greatest intellects of his time. \\nQuestion: What is the sentiment on Albert Einstein? \\nAnswer: postive \\n\\nSentence: The sweet lassi was excellent as was the lamb chettinad and the garlic naan but the rasamalai was forgettable. \\nQuestion: What is the sentiment on rasmalai? \\nAnswer: negative '`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will be showing accuracy on two different approaches: Few-shot and Zero-shot prompting.\n",
    "\n",
    "* zero-shot approach [`zero-shot`] (i.e. give the model the input sentence and ask for the sentiment).\n",
    "* few-shot approach [`few-shot`] (i.e. give some example sentences for the model to determine what should come next for the input sentence, then ask for the sentiment of a new example) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_type = \"few-shot\"  # other option here \"zero-shot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This csv file contains customer reviews of laptops collected in 2014 with size of around 469\n",
    "path = \"resources/absa_datasets/\"\n",
    "# Below you can add more datasets like 'Laptop_Train_v2.csv', 'Restaurants_Train_v2.csv', and\n",
    "# 'Restaurants_Test_Gold.csv'\n",
    "datasets_list = [os.path.join(path, \"Laptops_Test_Gold.csv\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8899d5aab30d4107a00384552505f00e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1032 entries, 0 to 1031\n",
      "Data columns (total 6 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   id                    800 non-null    object \n",
      " 1   text                  1032 non-null   object \n",
      " 2   aspect_term           654 non-null    object \n",
      " 3   aspect_term_polarity  654 non-null    object \n",
      " 4   aspect_term_from      654 non-null    float64\n",
      " 5   aspect_term_to        654 non-null    float64\n",
      "dtypes: float64(2), object(4)\n",
      "memory usage: 48.5+ KB\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 469 entries, 0 to 1031\n",
      "Data columns (total 8 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   id                         314 non-null    object \n",
      " 1   text                       469 non-null    object \n",
      " 2   aspect_term                469 non-null    object \n",
      " 3   aspect_term_polarity       469 non-null    object \n",
      " 4   aspect_term_from           469 non-null    float64\n",
      " 5   aspect_term_to             469 non-null    float64\n",
      " 6   sentence_with_prompt       469 non-null    object \n",
      " 7   sentence_with_full_prompt  469 non-null    object \n",
      "dtypes: float64(2), object(6)\n",
      "memory usage: 33.0+ KB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5e123c5fe134be38ab0dd28ba4ae896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy: 0.6535211267605634\n",
      "Confusion Matrix with ordering ['positive', 'negative']\n",
      "[[169  95]\n",
      " [ 28  63]]\n",
      "========================================================\n",
      "Label: positive, F1: 0.7331887201735358, Precision: 0.6401515151515151, Recall: 0.8578680203045685\n",
      "Label: negative, F1: 0.5060240963855421, Precision: 0.6923076923076923, Recall: 0.3987341772151899\n"
     ]
    }
   ],
   "source": [
    "for d in tqdm.notebook.tqdm(range(len(datasets_list))):\n",
    "    df = pd.read_csv(datasets_list[d])\n",
    "\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    df.info()\n",
    "    print()\n",
    "\n",
    "    # Delete any rows with null values\n",
    "    df = df.dropna(axis=0, how=\"any\", subset=[\"aspect_term\", \"aspect_term_polarity\"])\n",
    "\n",
    "    # Set the prompt format for the input sentence (drawn from one of the example from above)\n",
    "    df[\"sentence_with_prompt\"] = (\n",
    "        \"Sentence: \"\n",
    "        + df[\"text\"]\n",
    "        + \" \"\n",
    "        + \"Sentiment on \"\n",
    "        + df[\"aspect_term\"]\n",
    "        + \" is positive, negative, or neutral? It is\"\n",
    "    )\n",
    "\n",
    "    # Make sure to index instances with positive and negative as polarity\n",
    "    df = df.loc[\n",
    "        df[\"aspect_term_polarity\"].str.contains(\"positive\") | df[\"aspect_term_polarity\"].str.contains(\"negative\")\n",
    "    ]\n",
    "\n",
    "    demonstrations = \"Sentence: Albert Einstein was one of the greatest intellects of his time. Sentiment on Albert Einstein is positive, negative, or neutral? It is positive. \\nSentence: The sweet lassi was excellent as was the lamb chettinad and the garlic naan but the rasamalai was forgettable. Sentiment on rasmalai is positive, negative, or neutral? It is negative. \\nSentence: I had my software installed and ready to go, but the system crashed. Sentiment on software is positive, negative, or neutral? It is neutral.\"  # noqa E501\n",
    "\n",
    "    # for few-shot, we give more context to the model to improve the model performance and generalizability.\n",
    "    if generation_type == \"few-shot\":\n",
    "        df[\"sentence_with_full_prompt\"] = demonstrations + \"\\n\" + df[\"sentence_with_prompt\"]\n",
    "    elif generation_type == \"zero-shot\":\n",
    "        df[\"sentence_with_full_prompt\"] = df[\"sentence_with_prompt\"]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid generation type: Please select from zero-shot or few-shot.\")\n",
    "\n",
    "    df.info()\n",
    "\n",
    "    # Construct the dataloader with custom created dataset as input\n",
    "    data = CustomDataset(df)\n",
    "    dataloader = DataLoader(data, batch_size=20)\n",
    "\n",
    "    # initialize predictions and labels\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    # parsing through the model along with vector hosted OPT-175B\n",
    "    # For a discussion of the configuration parameters see:\n",
    "    # src/reference_implementations/prompting_vector_llms/CONFIG_README.md\n",
    "    for text, text_prompt, polarity in tqdm.notebook.tqdm(dataloader):\n",
    "        gen_text = model.generate(\n",
    "            text_prompt, {\"max_tokens\": 1, \"top_k\": 4, \"top_p\": 1.0, \"rep_penalty\": 1.0, \"temperature\": 1.0}\n",
    "        ).generation[\"tokens\"]\n",
    "        # Note that we are looking at the models generated response and attempting to match it to on of the labels in\n",
    "        # our labels space. If the model produces a different token it is considered wrong.\n",
    "        first_predicted_tokens = [tokens[0].strip().lower() for tokens in gen_text]\n",
    "        predictions.extend(first_predicted_tokens)\n",
    "        labels.extend(list(polarity))\n",
    "\n",
    "    # The labels associated with the dataset\n",
    "    labels_order = [\"positive\", \"negative\"]\n",
    "\n",
    "    cm = sklearn.metrics.confusion_matrix(np.array(labels), np.array(predictions), labels=labels_order)\n",
    "\n",
    "    FP = cm.sum(axis=0) - np.diag(cm)\n",
    "    FN = cm.sum(axis=1) - np.diag(cm)\n",
    "    TP = np.diag(cm)\n",
    "\n",
    "    recall = TP / (TP + FN)\n",
    "    precision = TP / (TP + FP)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    print(f\"Prediction Accuracy: {TP.sum()/(cm.sum())}\")\n",
    "\n",
    "    print(f\"Confusion Matrix with ordering {labels_order}\")\n",
    "    print(cm)\n",
    "    print(\"========================================================\")\n",
    "    for label_index, label_name in enumerate(labels_order):\n",
    "        print(\n",
    "            f\"Label: {label_name}, F1: {f1[label_index]}, Precision: {recall[label_index]}, \"\n",
    "            f\"Recall: {precision[label_index]}\"\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompt_engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
