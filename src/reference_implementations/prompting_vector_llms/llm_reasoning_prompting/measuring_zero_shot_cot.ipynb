{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "from enum import Enum\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import kscope\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conecting to the Service\n",
    "First we connect to the Kaleidoscope service through which we'll interact with the LLMs and see which models are avaiable to us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a client connection to the kscope service\n",
    "client = kscope.Client(gateway_host=\"llm.cluster.local\", gateway_port=3001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show all model instances that are currently active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '6d599738-d7b0-4277-83f8-1de47854f9f5',\n",
       "  'name': 'falcon-40b',\n",
       "  'state': 'ACTIVE'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.model_instances"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we obtain a handle to a model. In this example, let's use the Falcon-40B model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = client.load_model(\"falcon-40b\")\n",
    "# If this model is not actively running, it will get launched in the background.\n",
    "# In this case, wait until it moves into an \"ACTIVE\" state before proceeding.\n",
    "while model.state != \"ACTIVE\":\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we setup the configurations that the model uses for generation and decoding. We are going to use **GREEDY** decoding, as done in the original CoT papers. Greedy decoding means that the model always selects the token with the highest probability as its next word. Falcon requires a `do_sample` argument to be `True` in the configuration to perform anything other than greedy decoding and it defaults to False. So the configurations below are both greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_generation_config = {\"max_tokens\": 20, \"top_k\": 1, \"top_p\": 1.0, \"temperature\": 0.8}\n",
    "moderate_generation_config = {\"max_tokens\": 128, \"top_k\": 1, \"top_p\": 1.0, \"temperature\": 0.8}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's ask the model a simple question to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ottawa\n",
      "What is the capital of Canada?\n",
      "Ottawa\n",
      "What is the capital of\n"
     ]
    }
   ],
   "source": [
    "generation = model.generate(\"What is the capital of Canada?\", small_generation_config)\n",
    "# Extract the text from the returned generation\n",
    "print(generation.generation[\"sequences\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** The model generates until the `max_tokens` threshold is met, which was 20 tokens in this case. Many LLMs struggle to \"terminate\" their generation before reaching the `max_tokens` threshold. This means that they will often produce more than just a simple answer to the question. In this case, the model answered the question, started a new line, and began repeating the input. \n",
    "\n",
    "Consider the case where you were trying to parse out the answer to send to a database. You would need to have some rules for extracting just the answer to the question and not the extra things the model generates. This becomes important below"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by loading a sampling of 100 examples and parsing them into a test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoTDataset(Enum):\n",
    "    GSM8K = \"gsm8k\"\n",
    "    MULTI_ARITH = \"multi_arith\"\n",
    "\n",
    "\n",
    "dataset_name = CoTDataset.MULTI_ARITH\n",
    "\n",
    "dataset = load_dataset(\"gsm8k\", \"main\") if dataset_name is CoTDataset.GSM8K else load_dataset(\"ChilleD/MultiArith\")\n",
    "# Setting the manual seed so that the shuffle is deterministic.\n",
    "torch.manual_seed(1776)\n",
    "# Loading with a batch size of one so we can process them 1 at a time.\n",
    "dataloader = DataLoader(dataset[\"train\"], shuffle=True, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_gsm8k_answers(answer_str: str) -> float:\n",
    "    processed_answer = answer_str.split(\"####\")[-1].strip()\n",
    "    return float(processed_answer.replace(\",\", \"\"))\n",
    "\n",
    "\n",
    "def process_multi_arith_answers(answer_str: str) -> float:\n",
    "    return float(answer_str.strip().replace(\",\", \"\"))\n",
    "\n",
    "\n",
    "def process_dataset_point(datapoint: Dict[str, List[str]], dataset_name: CoTDataset) -> Tuple[str, float]:\n",
    "    if dataset_name is CoTDataset.GSM8K:\n",
    "        word_problem = datapoint[\"question\"][0]\n",
    "        answer = process_gsm8k_answers(datapoint[\"answer\"][0])\n",
    "        return word_problem, answer\n",
    "    elif dataset_name is CoTDataset.MULTI_ARITH:\n",
    "        word_problem = datapoint[\"question\"][0].strip()\n",
    "        answer = process_multi_arith_answers(datapoint[\"final_ans\"][0])\n",
    "        return word_problem, answer\n",
    "    else:\n",
    "        raise ValueError(\"Dataset not supported...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we construct a dataset of 100 Word Problems and the associated answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Problem: Rachel was organizing her book case making sure each of the shelves had exactly 9 books on it. If she had 6 shelves of mystery books and 2 shelves of picture books, how many books did she have total?\n",
      "Answer: 72.0\n"
     ]
    }
   ],
   "source": [
    "total_examples = 100\n",
    "data_iterator = iter(dataloader)\n",
    "word_problems: List[str] = []\n",
    "answers: List[float] = []\n",
    "for i in range(total_examples):\n",
    "    example = next(data_iterator)\n",
    "    word_problem, answer = process_dataset_point(example, dataset_name)\n",
    "    word_problems.append(word_problem)\n",
    "    answers.append(answer)\n",
    "\n",
    "print(f\"Word Problem: {word_problems[0]}\")\n",
    "print(f\"Answer: {answers[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Zero-shot Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's measure the performance of a zero-shot prompt in solving the word problems in this task. \n",
    "\n",
    "**Note**: This is a two stage process. First, we need to have the model generate the correct response. Next, we need to parse that response to get a final answer and compare it to the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_from_template(word_problem: str) -> str:\n",
    "    return f\"Q: {word_problem}\\nA: The answer is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Rachel was organizing her book case making sure each of the shelves had exactly 9 books on it. If she had 6 shelves of mystery books and 2 shelves of picture books, how many books did she have total?\n",
      "A: The answer is\n"
     ]
    }
   ],
   "source": [
    "zero_shot_prompts = [create_prompt_from_template(word_problem) for word_problem in word_problems]\n",
    "print(zero_shot_prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_answer_to_float(raw_answer: str) -> float:\n",
    "    remove_leading_symbols = re.sub(r\"^([^\\d])*\", \"\", raw_answer.strip())\n",
    "    remove_commas = re.sub(r\",\\s*\", \"\", remove_leading_symbols)\n",
    "    return float(remove_commas.rstrip(\".\"))\n",
    "\n",
    "\n",
    "def parse_predicted_answer(full_answer: str) -> float:\n",
    "    # Attempt to parse the answer string into a number\n",
    "    answer_match = re.search(r\"-?(\\d[,.]*)+\", full_answer)\n",
    "    if not answer_match:\n",
    "        print(f\"Failed to match to number: {full_answer}\")\n",
    "        return 0.0\n",
    "    else:\n",
    "        split_answer = answer_match.group()\n",
    "        try:\n",
    "            return parse_answer_to_float(split_answer)\n",
    "        except Exception:\n",
    "            print(f\"Failed to parse: {full_answer}\\nMatched: {split_answer}\")\n",
    "            return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 prompts...\n",
      "Processed 20 prompts...\n",
      "Processed 30 prompts...\n",
      "Processed 40 prompts...\n",
      "Processed 50 prompts...\n",
      "Processed 60 prompts...\n",
      "Processed 70 prompts...\n",
      "Processed 80 prompts...\n",
      "Processed 90 prompts...\n",
      "Processed 100 prompts...\n"
     ]
    }
   ],
   "source": [
    "predicted_answers = []\n",
    "for prompt_num, zero_shot_prompt in enumerate(zero_shot_prompts):\n",
    "    generation_example = model.generate(zero_shot_prompt, generation_config=small_generation_config)\n",
    "    full_answer = generation_example.generation[\"sequences\"][0]\n",
    "    predicted_answers.append(parse_predicted_answer(full_answer))\n",
    "\n",
    "    if (prompt_num + 1) % 10 == 0:\n",
    "        print(f\"Processed {prompt_num + 1} prompts...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's measure the accuracy of the parsed predictions from the model compared with the true answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-shot Prompt Accuracy: 0.11\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for predicted_answer, true_answer in zip(predicted_answers, answers):\n",
    "    if true_answer == predicted_answer:\n",
    "        correct += 1\n",
    "print(f\"Zero-shot Prompt Accuracy: {correct/total_examples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly the model struggles to produce the correct answer for these problems. Let's see if we can improve the performance with zero-shot CoT!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot Chain-of-Thought Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try performing some zero-shot CoT Prompting to see if we can get better performance. Remember that the zero-shot CoT prompt process has two stages. In the first stage, we ask the model to \"think step by step\" about how to solve the problem. In the second stage, we include that logic in the prompt and ask the model to provide a final answer.\n",
    "\n",
    "**NOTE**: CoT Queries take a lot longer to run due to the significantly longer context involved. The 100 queries will take at least 50 minutes to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_first_stage_prompt(word_problem: str) -> str:\n",
    "    return f\"Q: {word_problem}\\nA: Let's think step by step.\"\n",
    "\n",
    "\n",
    "def construct_second_stage_prompt(prompt: str, logic_generation: str) -> str:\n",
    "    return f\"{prompt}{logic_generation}\\nTherefore, the final answer is\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try out the two stage process to see what it looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logic Prompt:\n",
      "Q: Rachel was organizing her book case making sure each of the shelves had exactly 9 books on it. If she had 6 shelves of mystery books and 2 shelves of picture books, how many books did she have total?\n",
      "A: Let's think step by step.\n",
      "\n",
      "Generated Logic:\n",
      "\n",
      "1. How many books are on the mystery shelves?\n",
      "6 shelves x 9 books per shelf = 54 mystery books\n",
      "2. How many books are on the picture shelves?\n",
      "2 shelves x 9 books per shelf = 18 picture books\n",
      "3. How many books are there total?\n",
      "54 mystery books + 18 picture books = 72 books total\n",
      "4. How many books are on the mystery shelves?\n",
      "6 shelves x 9 books per\n",
      "Answer Prompt:\n",
      "Q: Rachel was organizing her book case making sure each of the shelves had exactly 9 books on it. If she had 6 shelves of mystery books and 2 shelves of picture books, how many books did she have total?\n",
      "A: Let's think step by step.\n",
      "1. How many books are on the mystery shelves?\n",
      "6 shelves x 9 books per shelf = 54 mystery books\n",
      "2. How many books are on the picture shelves?\n",
      "2 shelves x 9 books per shelf = 18 picture books\n",
      "3. How many books are there total?\n",
      "54 mystery books + 18 picture books = 72 books total\n",
      "4. How many books are on the mystery shelves?\n",
      "6 shelves x 9 books per\n",
      "Therefore, the final answer is\n",
      "\n",
      "Generated Answer:\n",
      " 72 books total.\n",
      "Q: A man has $1,000 to spend on a\n"
     ]
    }
   ],
   "source": [
    "logic_prompt = construct_first_stage_prompt(word_problems[0])\n",
    "print(f\"Logic Prompt:\\n{logic_prompt}\\n\")\n",
    "\n",
    "# First stage prompt to generate logic\n",
    "logic_generation = model.generate(logic_prompt, generation_config=moderate_generation_config)\n",
    "generated_logic = logic_generation.generation[\"sequences\"][0]\n",
    "print(f\"Generated Logic:\\n{generated_logic}\")\n",
    "\n",
    "answer_prompt = construct_second_stage_prompt(logic_prompt, generated_logic)\n",
    "print(f\"Answer Prompt:\\n{answer_prompt}\\n\")\n",
    "\n",
    "# Second stage prompt to generate answer\n",
    "answer_generation = model.generate(answer_prompt, generation_config=small_generation_config)\n",
    "generated_answer = answer_generation.generation[\"sequences\"][0]\n",
    "print(f\"Generated Answer:\\n{generated_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 prompts...\n",
      "Processed 20 prompts...\n",
      "Processed 30 prompts...\n",
      "Processed 40 prompts...\n",
      "Processed 50 prompts...\n",
      "Processed 60 prompts...\n",
      "Processed 70 prompts...\n",
      "Processed 80 prompts...\n",
      "Processed 90 prompts...\n",
      "Failed to match to number:  \"He was a chicken.\"\n",
      "Q: A man was driving his car when he saw a\n",
      "Processed 100 prompts...\n"
     ]
    }
   ],
   "source": [
    "predicted_answers = []\n",
    "for prompt_num, word_problem in enumerate(word_problems):\n",
    "    logic_prompt = construct_first_stage_prompt(word_problem)\n",
    "    logic_generation = model.generate(logic_prompt, generation_config=moderate_generation_config)\n",
    "    generated_logic = logic_generation.generation[\"sequences\"][0]\n",
    "\n",
    "    answer_prompt = construct_second_stage_prompt(logic_prompt, generated_logic)\n",
    "    answer_generation = model.generate(answer_prompt, generation_config=small_generation_config)\n",
    "    full_answer = answer_generation.generation[\"sequences\"][0]\n",
    "    predicted_answers.append(parse_predicted_answer(full_answer))\n",
    "\n",
    "    if (prompt_num + 1) % 10 == 0:\n",
    "        print(f\"Processed {prompt_num + 1} prompts...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-shot CoT Prompt Accuracy: 0.32\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for predicted_answer, true_answer in zip(predicted_answers, answers):\n",
    "    if true_answer == predicted_answer:\n",
    "        correct += 1\n",
    "print(f\"Zero-shot CoT Prompt Accuracy: {correct/total_examples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the models ability to perform the task has improved significantly. However, it takes quite a lot longer for the model to respond due to the extra computation associated with logic generation and then processing for the second generation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opt_notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
