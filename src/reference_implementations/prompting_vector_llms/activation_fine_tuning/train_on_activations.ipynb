{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/prompt_fine_tune_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import cuda\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning\n",
    "\n",
    "This notebook builds on the activations produced by the `compute_activations.ipynb` notebook. The cached activations are loaded from disk to faciliate the fine-tuning of a classification model on the sentiment analysis task. We have precomputed a set of activations in the resources folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations_path = \"./resources/\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define an Activation Dataset which will load our activations from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationDataset(Dataset):\n",
    "    def __init__(self, activations_path: str) -> None:\n",
    "        self._load_activations(activations_path)\n",
    "\n",
    "    def _load_activations(self, path: str) -> None:\n",
    "        with open(path, \"rb\") as handle:\n",
    "            cached_activations = pickle.load(handle)\n",
    "        self.activations = cached_activations[\"activations\"]\n",
    "        self.labels = cached_activations[\"labels\"]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.activations)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[List[float], int]:\n",
    "        return self.activations[idx], self.labels[idx]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be performing classification on the activations of the last (non-pad) token of the sequence, common practice for autoregressive models (e.g. GPT-3, OPT). These activations have already been formed and only the last non-pad token activations have been stored. We stack these activation tensors and extract the sentiment labels associated with the input movie review that generated the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_last_token(batch: List[Tuple[torch.Tensor, int]]) -> Tuple[torch.Tensor, int]:\n",
    "    last_token_activations: List[torch.Tensor] = []\n",
    "    labels: List[int] = []\n",
    "    for activations, label in batch:\n",
    "        last_token_activations.append(activations)\n",
    "        labels.append(label)\n",
    "\n",
    "    activation_batch = torch.stack(last_token_activations)\n",
    "\n",
    "    return activation_batch, labels  # type: ignore"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We construct a very small, two-layer, MLP that we will train on just 100 training samples to perform the sentiment analysis task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, cfg: Dict[str, int]) -> None:\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(cfg[\"embedding_dim\"], cfg[\"hidden_dim\"], bias=False)\n",
    "        self.out = nn.Linear(cfg[\"hidden_dim\"], cfg[\"label_dim\"])\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.relu(self.linear(x))\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Test Model for Activations without Prompts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load the activations associated with a small training set of 100 samples and a test set with 300 samples. These activations were not generated using any prompts, just the raw text of the movie review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ActivationDataset(os.path.join(activations_path, \"train_activations_demo.pkl\"))\n",
    "test_dataset = ActivationDataset(os.path.join(activations_path, \"test_activations_demo.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True, collate_fn=batch_last_token)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=True, collate_fn=batch_last_token)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now write a relatively simple script to train and evaluate our model. The model has an embedding dimension of 12,288 corresponding to the size of the intermediate activations for OPT-175B. The hidden dimension is small (128) and the final dimension corresponds to our label space (positive, negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 24: 100%|██████████| 25/25 [00:00<00:00, 31.38it/s, Train-Loss=0.0312, Test-Accuracy=0.763]\n"
     ]
    }
   ],
   "source": [
    "model = MLP({\"embedding_dim\": 12288, \"hidden_dim\": 128, \"label_dim\": 2})\n",
    "device = \"cuda\" if cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.001)\n",
    "\n",
    "NUM_EPOCHS = 25\n",
    "pbar = tqdm(range(NUM_EPOCHS))\n",
    "for epoch_idx in pbar:\n",
    "    pbar.set_description(\"Epoch: %s\" % epoch_idx)\n",
    "    training_params = {\"Train-Loss\": 0.0, \"Test-Accuracy\": 0.0}\n",
    "    pbar.set_postfix(training_params)\n",
    "\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        activations, labels = batch\n",
    "        activations = activations.to(device)\n",
    "        labels = torch.tensor(labels).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(activations)\n",
    "        loss = loss_fn(logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        training_params[\"Train-Loss\"] = loss.detach().item()\n",
    "        pbar.set_postfix(training_params)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = []\n",
    "        for batch in test_dataloader:\n",
    "            activations, labels = batch\n",
    "            activations = activations.float().to(device)\n",
    "            labels = torch.tensor(labels).to(device)\n",
    "\n",
    "            logits = model(activations)\n",
    "            predictions.extend((logits.argmax(dim=1) == labels))\n",
    "\n",
    "        accuracy = torch.stack(predictions).sum() / len(predictions)\n",
    "\n",
    "        training_params[\"Test-Accuracy\"] = accuracy.detach().item()\n",
    "        pbar.set_postfix(training_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Test Model for Activations with Prompts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now load the activations associated with a small training set of 100 samples and a test set with 300 samples that were generated using prompts as part of the input to the OPT model. The prompt structure can be seen in the `compute_activations.ipynb` notebook, but they incorporate 5-shot examples and an instruction prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ActivationDataset(os.path.join(activations_path, \"train_activations_with_prompts_demo.pkl\"))\n",
    "test_dataset = ActivationDataset(os.path.join(activations_path, \"test_activations_with_prompts_demo.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True, collate_fn=batch_last_token)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=True, collate_fn=batch_last_token)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now write a relatively simple script to train and evaluate our model. The model has an embedding dimension of 12,288 corresponding to the size of the intermediate activations for OPT-175B. The hidden dimension is small (128) and the final dimension corresponds to our label space (positive, negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 24: 100%|██████████| 25/25 [00:00<00:00, 33.77it/s, Train-Loss=0.0813, Test-Accuracy=0.96] \n"
     ]
    }
   ],
   "source": [
    "model = MLP({\"embedding_dim\": 12288, \"hidden_dim\": 128, \"label_dim\": 2})\n",
    "device = \"cuda\" if cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.001)\n",
    "\n",
    "NUM_EPOCHS = 25\n",
    "pbar = tqdm(range(NUM_EPOCHS))\n",
    "for epoch_idx in pbar:\n",
    "    pbar.set_description(\"Epoch: %s\" % epoch_idx)\n",
    "    training_params = {\"Train-Loss\": 0.0, \"Test-Accuracy\": 0.0}\n",
    "    pbar.set_postfix(training_params)\n",
    "\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        activations, labels = batch\n",
    "        activations = activations.to(device)\n",
    "        labels = torch.tensor(labels).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(activations)\n",
    "        loss = loss_fn(logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        training_params[\"Train-Loss\"] = loss.detach().item()\n",
    "        pbar.set_postfix(training_params)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = []\n",
    "        for batch in test_dataloader:\n",
    "            activations, labels = batch\n",
    "            activations = activations.float().to(device)\n",
    "            labels = torch.tensor(labels).to(device)\n",
    "\n",
    "            logits = model(activations)\n",
    "            predictions.extend((logits.argmax(dim=1) == labels))\n",
    "\n",
    "        accuracy = torch.stack(predictions).sum() / len(predictions)\n",
    "\n",
    "        training_params[\"Test-Accuracy\"] = accuracy.detach().item()\n",
    "        pbar.set_postfix(training_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is quite an amazing result. Simply by including a few-shot prompt when computing the activations, we have __significantly__ increased the sampling efficiency of training this small classifier and induced an immense jump in performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompt_fine_tune_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
