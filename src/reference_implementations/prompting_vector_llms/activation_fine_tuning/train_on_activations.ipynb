{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning\n",
    "\n",
    "The cached activations can be loaded from disk to faciliate the fine-tuning of a classification model on the sentiment analysis task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations_path = \"./resources/\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define an Activation Dataset which will load our activations from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationDataset(Dataset):\n",
    "    def __init__(self, activations_path: str) -> None:\n",
    "        self._load_activations(activations_path)\n",
    "\n",
    "    def _load_activations(self, path: str) -> None:\n",
    "        with open(path, \"rb\") as handle:\n",
    "            cached_activations = pickle.load(handle)\n",
    "        self.activations = cached_activations[\"activations\"]\n",
    "        self.labels = cached_activations[\"labels\"]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.activations)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[List[float], int]:\n",
    "        return self.activations[idx], self.labels[idx]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be performing classification on the last token of the sequence, common practice for autoregressive models (e.g. GPT-3). The following batch_last_token collate function will be passed into the dataloader to extract the last token activation from each sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_last_token(batch: List[Tuple[List[List[float]], int]]) -> Tuple[torch.Tensor, int]:\n",
    "    last_token_activations: List[List[float]] = []\n",
    "    labels: List[int] = []\n",
    "    for activations, label in batch:\n",
    "        last_token_activations.append(activations[-1])\n",
    "        labels.append(label)\n",
    "\n",
    "    activation_batch = torch.stack(last_token_activations)\n",
    "\n",
    "    return activation_batch, labels  # type: ignore"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And an MLP to perform the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, cfg: Dict[str, int]) -> None:\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(cfg[\"embedding_dim\"], cfg[\"hidden_dim\"], bias=False)\n",
    "        self.out = nn.Linear(cfg[\"hidden_dim\"], cfg[\"label_dim\"])\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.relu(self.linear(x))\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Test Model for Activations without Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ActivationDataset(os.path.join(activations_path, \"train_activations.pkl\"))\n",
    "test_dataset = ActivationDataset(os.path.join(activations_path, \"test_activations.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True, collate_fn=batch_last_token)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=True, collate_fn=batch_last_token)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now write a relatively simple script to train and evaluate our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP({\"embedding_dim\": 768, \"hidden_dim\": 128, \"label_dim\": 2})\n",
    "model.cuda()\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "pbar = tqdm(range(NUM_EPOCHS))\n",
    "for epoch_idx in pbar:\n",
    "    pbar.set_description(\"Epoch: %s\" % epoch_idx)\n",
    "    training_params = {\"Train-Loss\": 0.0, \"Test-Accuracy\": 0.0}\n",
    "    pbar.set_postfix(training_params)\n",
    "\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        activations, labels = batch\n",
    "        activations = activations.float().cuda()\n",
    "        labels = torch.tensor(labels).cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(activations)\n",
    "        loss = loss_fn(logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        training_params[\"Train-Loss\"] = loss.detach().item()\n",
    "        pbar.set_postfix(training_params)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = []\n",
    "        for batch in test_dataloader:\n",
    "            activations, labels = batch\n",
    "            activations = activations.float().cuda()\n",
    "            labels = torch.tensor(labels).cuda()\n",
    "\n",
    "            logits = model(activations)\n",
    "            predictions.extend((logits.argmax(dim=1) == labels))\n",
    "\n",
    "        accuracy = torch.stack(predictions).sum() / len(predictions)\n",
    "\n",
    "        training_params[\"Test-Accuracy\"] = accuracy.detach().item()\n",
    "        pbar.set_postfix(training_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Test Model for Activations with Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ActivationDataset(os.path.join(activations_path, \"train_activations_with_prompts_demo.pkl\"))\n",
    "test_dataset = ActivationDataset(os.path.join(activations_path, \"test_activations_with_prompts_demo.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True, collate_fn=batch_last_token)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=True, collate_fn=batch_last_token)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now write a relatively simple script to train and evaluate our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP({\"embedding_dim\": 768, \"hidden_dim\": 128, \"label_dim\": 2})\n",
    "model.cuda()\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "pbar = tqdm(range(NUM_EPOCHS))\n",
    "for epoch_idx in pbar:\n",
    "    pbar.set_description(\"Epoch: %s\" % epoch_idx)\n",
    "    training_params = {\"Train-Loss\": 0.0, \"Test-Accuracy\": 0.0}\n",
    "    pbar.set_postfix(training_params)\n",
    "\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        activations, labels = batch\n",
    "        activations = activations.float().cuda()\n",
    "        labels = torch.tensor(labels).cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(activations)\n",
    "        loss = loss_fn(logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        training_params[\"Train-Loss\"] = loss.detach().item()\n",
    "        pbar.set_postfix(training_params)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = []\n",
    "        for batch in test_dataloader:\n",
    "            activations, labels = batch\n",
    "            activations = activations.float().cuda()\n",
    "            labels = torch.tensor(labels).cuda()\n",
    "\n",
    "            logits = model(activations)\n",
    "            predictions.extend((logits.argmax(dim=1) == labels))\n",
    "\n",
    "        accuracy = torch.stack(predictions).sum() / len(predictions)\n",
    "\n",
    "        training_params[\"Test-Accuracy\"] = accuracy.detach().item()\n",
    "        pbar.set_postfix(training_params)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
