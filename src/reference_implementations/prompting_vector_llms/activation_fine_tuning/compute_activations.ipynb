{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from pprint import pprint\n",
    "from typing import List\n",
    "\n",
    "import datasets\n",
    "import kscope\n",
    "from datasets import Dataset\n",
    "from kscope import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'b11f3264-9c03-4114-9d56-d39a0fa63640',\n",
       "  'name': 'OPT-175B',\n",
       "  'state': 'ACTIVE'},\n",
       " {'id': 'c5ea8da7-3384-4c7b-b47f-95ed1a485897',\n",
       "  'name': 'OPT-6.7B',\n",
       "  'state': 'ACTIVE'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Establish a client connection to the kscope service\n",
    "client = kscope.Client(gateway_host=\"llm.cluster.local\", gateway_port=3001)\n",
    "client.model_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = client.load_model(\"OPT-175B\")\n",
    "# If this model is not actively running, it will get launched in the background.\n",
    "# In this case, wait until it moves into an \"ACTIVE\" state before proceeding.\n",
    "while model.state != \"ACTIVE\":\n",
    "    time.sleep(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to configure the model to generate in the way we want it to. However, because we only care about the activations of our input, the configuration is less important. We need a configuration but the parameters don't really matter. For a discussion of the configuration parameters see: `src/reference_implementations/prompting_vector_llms/CONFIG_README.md`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_generation_config = {\"max_tokens\": 1, \"top_k\": 4, \"top_p\": 1.0, \"rep_penalty\": 1.0, \"temperature\": 1.0}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Generation \n",
    "\n",
    "Activation generation is quite easy. We can use the client to query the remote model and explore the various modules. Here, we are listing only the last 10 layers of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['decoder.layers.95.self_attn',\n",
       " 'decoder.layers.95.self_attn.dropout_module',\n",
       " 'decoder.layers.95.self_attn.qkv_proj',\n",
       " 'decoder.layers.95.self_attn.out_proj',\n",
       " 'decoder.layers.95.self_attn_layer_norm',\n",
       " 'decoder.layers.95.fc1',\n",
       " 'decoder.layers.95.fc2',\n",
       " 'decoder.layers.95.final_layer_norm',\n",
       " 'decoder.layer_norm',\n",
       " 'decoder.output_projection']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.module_names[-10:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can select the module names of interest and pass them into a `get_activations` function alongside our set of prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activations(activations=[{'decoder.layers.95.fc2': tensor([[ 0.0062, -0.4624,  0.4702,  ...,  0.0985,  1.2588, -0.7002],\n",
      "        [-0.2197,  0.3364, -0.1965,  ..., -0.2651,  0.0475, -0.1638]],\n",
      "       dtype=torch.float16)}, {'decoder.layers.95.fc2': tensor([[ 2.5312, -1.4609, -1.7266,  ...,  1.9453, -0.8262, -1.5234],\n",
      "        [-0.2294, -0.6396,  0.9043,  ..., -0.8140,  0.1183,  0.2437],\n",
      "        [-0.8062,  0.8320, -0.8003,  ..., -0.8530, -0.7656,  0.5718]],\n",
      "       dtype=torch.float16)}], logprobs=[[None, -7.15677547454834, -6.544065475463867], [None, -5.25247859954834, -6.673819065093994, -5.898566246032715]], text=['Hello World', 'Fizz Buzz'], tokens=[['</s>', 'Hello', ' World'], ['</s>', 'F', 'izz', ' Buzz']])\n",
      "Tensor Shape: torch.Size([2, 12288])\n",
      "Tensor Shape: torch.Size([3, 12288])\n"
     ]
    }
   ],
   "source": [
    "prompts = [\"Hello World\", \"Fizz Buzz\"]\n",
    "\n",
    "module_name = \"decoder.layers.95.fc2\"\n",
    "\n",
    "activations = model.get_activations(prompts, [module_name], short_generation_config)\n",
    "pprint(activations)\n",
    "\n",
    "# We sent a batch of 2 prompts to the model. So there is a list of length two activations returned\n",
    "for activations_single_prompt in activations.activations:\n",
    "    # For each prompt we extract the activations and calculate which label had the high likelihood.\n",
    "    raw_activations = activations_single_prompt[module_name]\n",
    "    # The activations should have shape (number of tokens) x (activation size)\n",
    "    # For example, OPT-175 has an embedding dimension for the layer requested of 12288\n",
    "    print(\"Tensor Shape:\", raw_activations.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second above example, \"Fizz Buzz\" is tokenized as [\"F\", \"izz\", \" Buzz\"]. So we receive a tensor with 3 rows (one for each token) and 12,288 columns (the hidden dimension of OPT-175B)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a proof of concept of the few-shot abilities of LLMs, we'll only use a small training dataset and will only perform validation using a small test subset for compute efficiency.\n",
    "\n",
    "* Training set: 100 randomly sampled training examples\n",
    "* Test set: 300 randomly sample test examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/Users/david/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n",
      "100%|██████████| 3/3 [00:00<00:00, 244.08it/s]\n",
      "Loading cached shuffled indices for dataset at /Users/david/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-9c48ce5d173413c7.arrow\n",
      "Loading cached shuffled indices for dataset at /Users/david/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-c1eaa46e94dfbfd3.arrow\n",
      "Loading cached shuffled indices for dataset at /Users/david/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-9c48ce5d173413c7.arrow\n"
     ]
    }
   ],
   "source": [
    "imdb = datasets.load_dataset(\"imdb\")\n",
    "train_size = 100\n",
    "test_size = 300\n",
    "n_demonstrations = 5\n",
    "\n",
    "activation_save_path = \"./resources/\"\n",
    "\n",
    "small_train_dataset = imdb[\"train\"].shuffle(seed=42).select([i for i in list(range(train_size))])\n",
    "small_test_dataset = imdb[\"test\"].shuffle(seed=42).select([i for i in list(range(test_size))])\n",
    "# We're going to be experimenting with the affect that prompting the model for the task we care about affects a\n",
    "# classifier trained on the activations performs. So we will construct demonstrations by randomly selecting a set of\n",
    "# 5 examples from the training set to serve this purpose.\n",
    "small_demonstration_set = imdb[\"train\"].shuffle(seed=42).select([i for i in list(range(n_demonstrations))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split a list into a tuple of lists of a fixed size\n",
    "def batcher(seq: List[str], size: int) -> Dataset:\n",
    "    return (seq[pos : pos + size] for pos in range(0, len(seq), size))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Text Activations\n",
    "\n",
    "Let's start by getting the activations associated with the raw review text. We'll do activations for the text coupled with a prompt below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset_activations(\n",
    "    split: str,\n",
    "    inputs: List[str],\n",
    "    labels: List[int],\n",
    "    model: Model,\n",
    "    module_name: str,\n",
    "    pickle_name: str,\n",
    "    batch_size: int = 16,\n",
    ") -> None:\n",
    "    print(\"Generating Activations with Prompts: \" + split)\n",
    "\n",
    "    activations = []\n",
    "    for batch_number, input_batch in enumerate(batcher(inputs, batch_size)):\n",
    "        # Getting activations for each input batch. For an example of how get_activations works, see beginning of this\n",
    "        # notebook.\n",
    "        activations.append(model.get_activations(input_batch, [module_name], short_generation_config))\n",
    "        print(f\"Batch Number {batch_number} Complete\")\n",
    "\n",
    "    parsed_activations = []\n",
    "    for batch in activations:\n",
    "        for input_activation in batch.activations:\n",
    "            # We will be performing classification on the last token non-pad token of the sequence a common practice\n",
    "            # for autoregressive models (e.g. GPT-3, OPT). So we only keep the last row of the activation outputs.\n",
    "            parsed_activations.append(input_activation[module_name][-1].float())\n",
    "\n",
    "    cached_activations = {\"activations\": parsed_activations, \"labels\": labels}\n",
    "\n",
    "    with open(os.path.join(activation_save_path, f\"{split}{pickle_name}.pkl\"), \"wb\") as handle:\n",
    "        pickle.dump(cached_activations, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Activations with Prompts: train\n",
      "Batch Number 0 Complete\n",
      "Batch Number 1 Complete\n",
      "Batch Number 2 Complete\n",
      "Batch Number 3 Complete\n",
      "Batch Number 4 Complete\n",
      "Batch Number 5 Complete\n",
      "Batch Number 6 Complete\n",
      "Generating Activations with Prompts: test\n",
      "Batch Number 0 Complete\n",
      "Batch Number 1 Complete\n",
      "Batch Number 2 Complete\n",
      "Batch Number 3 Complete\n",
      "Batch Number 4 Complete\n",
      "Batch Number 5 Complete\n",
      "Batch Number 6 Complete\n",
      "Batch Number 7 Complete\n",
      "Batch Number 8 Complete\n",
      "Batch Number 9 Complete\n",
      "Batch Number 10 Complete\n",
      "Batch Number 11 Complete\n",
      "Batch Number 12 Complete\n",
      "Batch Number 13 Complete\n",
      "Batch Number 14 Complete\n",
      "Batch Number 15 Complete\n",
      "Batch Number 16 Complete\n",
      "Batch Number 17 Complete\n",
      "Batch Number 18 Complete\n"
     ]
    }
   ],
   "source": [
    "module_name = \"decoder.layers.95.fc2\"\n",
    "\n",
    "train_labels = small_train_dataset[\"label\"]\n",
    "test_labels = small_test_dataset[\"label\"]\n",
    "\n",
    "generate_dataset_activations(\n",
    "    \"train\", small_train_dataset[\"text\"], train_labels, model, module_name, \"_activations_demo\"\n",
    ")\n",
    "generate_dataset_activations(\"test\", small_test_dataset[\"text\"], test_labels, model, module_name, \"_activations_demo\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Conditioned Activations\n",
    "\n",
    "Now let's generate activations pre-conditioned with an instruction and a few demonstrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_demonstrations(instruction: str, demonstration_set: Dataset) -> str:\n",
    "    label_int_to_str = {0: \"negative\", 1: \"positive\"}\n",
    "    demonstration = f\"{instruction}\"\n",
    "    demo_texts = demonstration_set[\"text\"]\n",
    "    demo_labels = demonstration_set[\"label\"]\n",
    "    for text, label in zip(demo_texts, demo_labels):\n",
    "        # truncate the text in case it is very long\n",
    "        split_text = text.split(\" \")\n",
    "        if len(split_text) > 128:\n",
    "            text = \" \".join(split_text[-128:])\n",
    "        demonstration = f\"{demonstration}\\n\\nText: {text} The sentiment is {label_int_to_str[label]}.\"\n",
    "    return f\"{demonstration}\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompts(texts: List[str], demonstration: str) -> List[str]:\n",
    "    return [f\"{demonstration}Text: {text} The sentiment is\" for text in texts]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we show the demonstration structure (based on 5 examples) and what each prompt passed to OPT looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demonstration:\n",
      "Classify the sentiment of the text.\n",
      "\n",
      "Text: There is no relation at all between Fortier and Profiler but the fact that both are police series about violent crimes. Profiler looks crispy, Fortier looks classic. Profiler plots are quite simple. Fortier's plot are far more complicated... Fortier looks more like Prime Suspect, if we have to spot similarities... The main character is weak and weirdo, but have \"clairvoyance\". People like to compare, to judge, to evaluate. How about just enjoying? Funny thing too, people writing Fortier looks American but, on the other hand, arguing they prefer American series (!!!). Maybe it's the language, or the spirit, but I think this series is more English than American. By the way, the actors are really good and funny. The acting is not superficial at all... The sentiment is positive.\n",
      "\n",
      "Text: a great. The plot is very true to the book which is a classic written by Mark Twain. The movie starts of with a scene where Hank sings a song with a bunch of kids called \"when you stub your toe on the moon\" It reminds me of Sinatra's song High Hopes, it is fun and inspirational. The Music is great throughout and my favorite song is sung by the King, Hank (bing Crosby) and Sir \"Saggy\" Sagamore. OVerall a great family movie or even a great Date movie. This is a movie you can watch over and over again. The princess played by Rhonda Fleming is gorgeous. I love this movie!! If you liked Danny Kaye in the Court Jester then you will definitely like this movie. The sentiment is positive.\n",
      "\n",
      "Text: of the nation, who made this war happen. The character of Rambo is perfect to notice this. He is extremely patriotic, bemoans that US-Americans didn't appreciate and celebrate the achievements of the single soldier, but has nothing but distrust for leading officers and politicians. Like every film that defends the war (e.g. \"We Were Soldiers\") also this one avoids the need to give a comprehensible reason for the engagement in South Asia. And for that matter also the reason for every single US-American soldier that was there. Instead, Rambo gets to take revenge for the wounds of a whole nation. It would have been better to work on how to deal with the memories, rather than suppressing them. \"Do we get to win this time?\" Yes, you do. The sentiment is negative.\n",
      "\n",
      "Text: In the process of trying to establish the audiences' empathy with Jake Roedel (Tobey Maguire) the filmmakers slander the North and the Jayhawkers. Missouri never withdrew from the Union and the Union Army was not an invading force. The Southerners fought for State's Rights: the right to own slaves, elect crooked legislatures and judges, and employ a political spoils system. There's nothing noble in that. The Missourians could have easily traveled east and joined the Confederate Army.<br /><br />It seems to me that the story has nothing to do with ambiguity. When Jake leaves the Bushwhackers, it's not because he saw error in his way, he certainly doesn't give himself over to the virtue of the cause of abolition. The sentiment is positive.\n",
      "\n",
      "Text: having to do with verisimilitude. However, it's dragged down from greatness by its insistence on trendy distractions, which culminate in a long scene where a horrible five-day stomach flu makes the rounds in the household. We must endure pointless fantasy sequences, initiated by the imaginary ringleader Leary. Whose existence, by the way, is finally reminiscent of the Brad Pitt character in *Fight Club*. And this finally drives home the film's other big flaw: lack of originality. In this review, I realize it's been far too easy to reference many other films. Granted, this film is an improvement on most of them, but still. *The Secret Lives of Dentists* is worth seeing, but don't get too excited about it. (Not that you were all that excited, anyway. I guess.) The sentiment is negative.\n",
      "\n",
      "\n",
      "Prompt Example:\n",
      "Classify the sentiment of the text.\n",
      "\n",
      "Text: There is no relation at all between Fortier and Profiler but the fact that both are police series about violent crimes. Profiler looks crispy, Fortier looks classic. Profiler plots are quite simple. Fortier's plot are far more complicated... Fortier looks more like Prime Suspect, if we have to spot similarities... The main character is weak and weirdo, but have \"clairvoyance\". People like to compare, to judge, to evaluate. How about just enjoying? Funny thing too, people writing Fortier looks American but, on the other hand, arguing they prefer American series (!!!). Maybe it's the language, or the spirit, but I think this series is more English than American. By the way, the actors are really good and funny. The acting is not superficial at all... The sentiment is positive.\n",
      "\n",
      "Text: a great. The plot is very true to the book which is a classic written by Mark Twain. The movie starts of with a scene where Hank sings a song with a bunch of kids called \"when you stub your toe on the moon\" It reminds me of Sinatra's song High Hopes, it is fun and inspirational. The Music is great throughout and my favorite song is sung by the King, Hank (bing Crosby) and Sir \"Saggy\" Sagamore. OVerall a great family movie or even a great Date movie. This is a movie you can watch over and over again. The princess played by Rhonda Fleming is gorgeous. I love this movie!! If you liked Danny Kaye in the Court Jester then you will definitely like this movie. The sentiment is positive.\n",
      "\n",
      "Text: of the nation, who made this war happen. The character of Rambo is perfect to notice this. He is extremely patriotic, bemoans that US-Americans didn't appreciate and celebrate the achievements of the single soldier, but has nothing but distrust for leading officers and politicians. Like every film that defends the war (e.g. \"We Were Soldiers\") also this one avoids the need to give a comprehensible reason for the engagement in South Asia. And for that matter also the reason for every single US-American soldier that was there. Instead, Rambo gets to take revenge for the wounds of a whole nation. It would have been better to work on how to deal with the memories, rather than suppressing them. \"Do we get to win this time?\" Yes, you do. The sentiment is negative.\n",
      "\n",
      "Text: In the process of trying to establish the audiences' empathy with Jake Roedel (Tobey Maguire) the filmmakers slander the North and the Jayhawkers. Missouri never withdrew from the Union and the Union Army was not an invading force. The Southerners fought for State's Rights: the right to own slaves, elect crooked legislatures and judges, and employ a political spoils system. There's nothing noble in that. The Missourians could have easily traveled east and joined the Confederate Army.<br /><br />It seems to me that the story has nothing to do with ambiguity. When Jake leaves the Bushwhackers, it's not because he saw error in his way, he certainly doesn't give himself over to the virtue of the cause of abolition. The sentiment is positive.\n",
      "\n",
      "Text: having to do with verisimilitude. However, it's dragged down from greatness by its insistence on trendy distractions, which culminate in a long scene where a horrible five-day stomach flu makes the rounds in the household. We must endure pointless fantasy sequences, initiated by the imaginary ringleader Leary. Whose existence, by the way, is finally reminiscent of the Brad Pitt character in *Fight Club*. And this finally drives home the film's other big flaw: lack of originality. In this review, I realize it's been far too easy to reference many other films. Granted, this film is an improvement on most of them, but still. *The Secret Lives of Dentists* is worth seeing, but don't get too excited about it. (Not that you were all that excited, anyway. I guess.) The sentiment is negative.\n",
      "\n",
      "Text: There is no relation at all between Fortier and Profiler but the fact that both are police series about violent crimes. Profiler looks crispy, Fortier looks classic. Profiler plots are quite simple. Fortier's plot are far more complicated... Fortier looks more like Prime Suspect, if we have to spot similarities... The main character is weak and weirdo, but have \"clairvoyance\". People like to compare, to judge, to evaluate. How about just enjoying? Funny thing too, people writing Fortier looks American but, on the other hand, arguing they prefer American series (!!!). Maybe it's the language, or the spirit, but I think this series is more English than American. By the way, the actors are really good and funny. The acting is not superficial at all... The sentiment is\n"
     ]
    }
   ],
   "source": [
    "demonstration = create_demonstrations(\"Classify the sentiment of the text.\", small_demonstration_set)\n",
    "print(f\"Demonstration:\\n{demonstration}\")\n",
    "\n",
    "train_prompts = create_prompts(small_train_dataset[\"text\"], demonstration)\n",
    "test_prompts = create_prompts(small_test_dataset[\"text\"], demonstration)\n",
    "print(f\"Prompt Example:\\n{train_prompts[0]}\")\n",
    "\n",
    "train_labels = small_train_dataset[\"label\"]\n",
    "test_labels = small_test_dataset[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Activations with Prompts: train\n",
      "Batch Number 0 Complete\n",
      "Batch Number 1 Complete\n",
      "Batch Number 2 Complete\n",
      "Batch Number 3 Complete\n",
      "Batch Number 4 Complete\n",
      "Batch Number 5 Complete\n",
      "Batch Number 6 Complete\n",
      "Generating Activations with Prompts: test\n",
      "Batch Number 0 Complete\n",
      "Batch Number 1 Complete\n",
      "Batch Number 2 Complete\n",
      "Batch Number 3 Complete\n",
      "Batch Number 4 Complete\n",
      "Batch Number 5 Complete\n",
      "Batch Number 6 Complete\n",
      "Batch Number 7 Complete\n",
      "Batch Number 8 Complete\n",
      "Batch Number 9 Complete\n",
      "Batch Number 10 Complete\n",
      "Batch Number 11 Complete\n",
      "Batch Number 12 Complete\n",
      "Batch Number 13 Complete\n",
      "Batch Number 14 Complete\n",
      "Batch Number 15 Complete\n",
      "Batch Number 16 Complete\n",
      "Batch Number 17 Complete\n",
      "Batch Number 18 Complete\n"
     ]
    }
   ],
   "source": [
    "module_name = \"decoder.layers.95.fc2\"\n",
    "generate_dataset_activations(\n",
    "    \"train\", train_prompts, train_labels, model, module_name, \"_activations_with_prompts_demo\"\n",
    ")\n",
    "generate_dataset_activations(\n",
    "    \"test\",\n",
    "    test_prompts,\n",
    "    test_labels,\n",
    "    model,\n",
    "    module_name,\n",
    "    \"_activations_with_prompts_demo\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these activations saved, the next step is to train a simple classifier on top of them in order to perform the sentiment classification. This is done in the `train_on_activations.ipynb` notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompt_fine_tune_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
