{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from pprint import pprint\n",
    "from typing import List\n",
    "\n",
    "import datasets\n",
    "import lingua\n",
    "from datasets import Dataset\n",
    "from lingua import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a client connection to the Lingua service\n",
    "client = lingua.Client(gateway_host=\"llm.cluster.local\", gateway_port=3001)\n",
    "client.model_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = client.load_model(\"OPT-175B\")\n",
    "# If this model is not actively running, it will get launched in the background.\n",
    "# In this case, wait until it moves into an \"ACTIVE\" state before proceeding.\n",
    "while model.state != \"ACTIVE\":\n",
    "    time.sleep(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to configure the model to generate in the way we want it to. However, because we only care about the activations of our input, the configuration is less important. We need one but the parameters don\"t really matter. If you\"re curious about the values, please see other "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_generation_config = {\"max_tokens\": 1, \"top_k\": 4, \"top_p\": 3, \"rep_penalty\": 1.0, \"temperature\": 1.0}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Generation \n",
    "\n",
    "Activation generation is quite easy. We can use the client to query the remote model and explore the various modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.module_names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can select the module names of interest and pass them into a `get_activations` function alongside our set of prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\"Hello World\", \"Fizz Buzz\"]\n",
    "\n",
    "module_name = \"decoder.layers.95.fc2\"\n",
    "\n",
    "activations = model.get_activations(prompts, [module_name], short_generation_config)\n",
    "pprint(activations)\n",
    "\n",
    "# We sent a batch of 2 prompts to the model. So there is a list of length two activations returned\n",
    "for activations_single_prompt in activations.activations:\n",
    "    # For each prompt we extract the activations and calculate which label had the high likelihood.\n",
    "    raw_activations = activations_single_prompt[module_name]\n",
    "    # The activations should have shape (number of tokens) x (activation size)\n",
    "    # For example, OPT-175 has an embedding dimension for the layer requested of 12288\n",
    "    print(\"Tensor Shape:\", raw_activations.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a proof of concept of the few-shot abilities of LLMs, we\"ll only use a small training dataset and will only perform validation using a small test subset for compute efficiency.\n",
    "\n",
    "* Training set: 100 randomly sampled training examples\n",
    "* Test set: 300 randomly sample test examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = datasets.load_dataset(\"imdb\")\n",
    "train_size = 100\n",
    "test_size = 300\n",
    "n_demonstrations = 5\n",
    "\n",
    "activation_save_path = \"./resources/\"\n",
    "\n",
    "small_train_dataset = imdb[\"train\"].shuffle(seed=42).select([i for i in list(range(train_size))])\n",
    "small_test_dataset = imdb[\"test\"].shuffle(seed=42).select([i for i in list(range(test_size))])\n",
    "# We\"re going to be experimenting with the affect that prompting the model for the task we envision affects the\n",
    "# classifiers downstream performance. So we construct demonstrations here.\n",
    "small_demonstration_set = imdb[\"train\"].shuffle(seed=42).select([i for i in list(range(n_demonstrations))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batcher(seq: List[str], size: int) -> Dataset:\n",
    "    return (seq[pos : pos + size] for pos in range(0, len(seq), size))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let\"s start by getting the activations associated with the raw review text. We\"ll do activations for the text coupled with a prompt below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset_activations(\n",
    "    split: str,\n",
    "    inputs: List[str],\n",
    "    labels: List[int],\n",
    "    model: Model,\n",
    "    module_name: str,\n",
    "    pickle_name: str,\n",
    "    batch_size: int = 16,\n",
    ") -> None:\n",
    "    print(\"Generating Activations with Prompts: \" + split)\n",
    "\n",
    "    activations = []\n",
    "    for batch_number, input_batch in enumerate(batcher(inputs, batch_size)):\n",
    "        activations.append(model.get_activations(input_batch, [module_name], short_generation_config))\n",
    "        print(f\"Batch Number {batch_number} Complete\")\n",
    "\n",
    "    parsed_activations = []\n",
    "    for batch in activations:\n",
    "        for input_activation in batch.activations:\n",
    "            # We will be performing classification on the last token non-pad token of the sequence\n",
    "            # common practice for autoregressive models (e.g. GPT-3). So we only keep the last row of the activation\n",
    "            # outputs\n",
    "            parsed_activations.append(input_activation[module_name][-1].float())\n",
    "\n",
    "    cached_activations = {\"activations\": parsed_activations, \"labels\": labels}\n",
    "\n",
    "    with open(os.path.join(activation_save_path, f\"{split}{pickle_name}.pkl\"), \"wb\") as handle:\n",
    "        pickle.dump(cached_activations, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_name = \"decoder.layers.95.fc2\"\n",
    "\n",
    "train_labels = small_train_dataset[\"label\"]\n",
    "test_labels = small_test_dataset[\"label\"]\n",
    "\n",
    "generate_dataset_activations(\n",
    "    \"train\", small_train_dataset[\"text\"], train_labels, model, module_name, \"_activations_demo\"\n",
    ")\n",
    "generate_dataset_activations(\"test\", small_test_dataset[\"text\"], test_labels, model, module_name, \"_activations_demo\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let\"s generate activations pre-conditioned with an instruction and a few demonstrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_demonstrations(instruction: str, demonstration_set: Dataset) -> str:\n",
    "    label_int_to_str = {0: \"negative\", 1: \"positive\"}\n",
    "    demonstration = f\"{instruction}\"\n",
    "    demo_texts = demonstration_set[\"text\"]\n",
    "    demo_labels = demonstration_set[\"label\"]\n",
    "    for text, label in zip(demo_texts, demo_labels):\n",
    "        # truncate the text in case it is very long\n",
    "        split_text = text.split(\" \")\n",
    "        if len(split_text) > 128:\n",
    "            text = \" \".join(split_text[-128:])\n",
    "        demonstration = f\"{demonstration}\\n\\nText: {text} The sentiment is {label_int_to_str[label]}.\"\n",
    "    return f\"{demonstration}\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompts(texts: List[str], demonstration: str) -> List[str]:\n",
    "    return [f\"{demonstration}Text: {text} The sentiment is\" for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demonstration = create_demonstrations(\"Classify the sentiment of the text.\", small_demonstration_set)\n",
    "print(f\"Demonstration:\\n{demonstration}\")\n",
    "\n",
    "train_prompts = create_prompts(small_train_dataset[\"text\"], demonstration)\n",
    "test_prompts = create_prompts(small_test_dataset[\"text\"], demonstration)\n",
    "print(f\"Prompt Example:\\n{train_prompts[0]}\")\n",
    "\n",
    "train_labels = small_train_dataset[\"label\"]\n",
    "test_labels = small_test_dataset[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_name = \"decoder.layers.95.fc2\"\n",
    "generate_dataset_activations(\n",
    "    \"train\", train_prompts, train_labels, model, module_name, \"_activations_with_prompts_demo\"\n",
    ")\n",
    "generate_dataset_activations(\n",
    "    \"test\",\n",
    "    test_prompts,\n",
    "    test_labels,\n",
    "    model,\n",
    "    module_name,\n",
    "    \"_activations_with_prompts_demo\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these activations saved, the next step is to train a simple classifier on top of them in order to perform the sentiment classification. This is done in the `train_on_activations.ipynb` notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompt_fine_tune_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
