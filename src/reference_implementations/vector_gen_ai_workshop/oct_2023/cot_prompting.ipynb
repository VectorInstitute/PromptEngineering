{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import kscope"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conecting to the Service\n",
    "First we connect to the Kaleidoscope service through which we'll interact with the LLMs and see which models are avaiable to us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a client connection to the kscope service\n",
    "client = kscope.Client(gateway_host=\"llm.cluster.local\", gateway_port=3001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show all model instances that are currently active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.model_instances"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we obtain a handle to a model. In this example, let's use the LLaMA2-7B model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/david/Desktop/VectorRepositories/PromptEngineering/src/reference_implementations/vector_gen_ai_workshop/cot_prompting.ipynb Cell 8\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/david/Desktop/VectorRepositories/PromptEngineering/src/reference_implementations/vector_gen_ai_workshop/cot_prompting.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# If this model is not actively running, it will get launched in the background.\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/david/Desktop/VectorRepositories/PromptEngineering/src/reference_implementations/vector_gen_ai_workshop/cot_prompting.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# In this case, wait until it moves into an \"ACTIVE\" state before proceeding.\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/david/Desktop/VectorRepositories/PromptEngineering/src/reference_implementations/vector_gen_ai_workshop/cot_prompting.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mwhile\u001b[39;00m model\u001b[39m.\u001b[39mstate \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mACTIVE\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/david/Desktop/VectorRepositories/PromptEngineering/src/reference_implementations/vector_gen_ai_workshop/cot_prompting.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m1\u001b[39;49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = client.load_model(\"llama2-7b\")\n",
    "# If this model is not actively running, it will get launched in the background.\n",
    "# In this case, wait until it moves into an \"ACTIVE\" state before proceeding.\n",
    "while model.state != \"ACTIVE\":\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "moderate_generation_config = {\"max_tokens\": 50, \"top_k\": 4, \"top_p\": 1.0, \"rep_penalty\": 1.2, \"temperature\": 0.9}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's ask the model some questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"\\nI don't know and I don't care about the capitals of other countries.\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation = model.generate(\"What is the capital of Canada?\", moderate_generation_config)\n",
    "# Extract the text from the returned generation\n",
    "generation.generation[\"text\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few-Shot Chain of Thought Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by prompting LLaMA2-7B to solve some word problems using the Few-Shot CoT method proposed in [\"Chain-of-Thought Prompting Elicits Reasoning\n",
    "in Large Language Models\"](https://arxiv.org/pdf/2201.11903.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's see what happens if we try to solve some word problems with a zero-shot prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n"
     ]
    }
   ],
   "source": [
    "zero_shot_prompt = (\n",
    "    \"The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do \" \"they have?\"\n",
    ")\n",
    "\n",
    "print(zero_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_example = model.generate(zero_shot_prompt, generation_config=moderate_generation_config)\n",
    "print(generation_example.generation[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct answer to this word problem is 6.\n",
    "\n",
    "Now let's try performing a few-shot prompt to see if that helps the model provide the correct answer in a format that we can extract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n",
      "\n",
      "A: The answer is 11.\n",
      "\n",
      "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
      "\n",
      "A: \n"
     ]
    }
   ],
   "source": [
    "few_shot_prompt = (\n",
    "    \"Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis \"\n",
    "    \"balls does he have now?\\n\\nA: The answer is 11.\\n\\nQ: The cafeteria had 23 apples. If they used 20 to make lunch \"\n",
    "    \"and bought 6 more, how many apples do they have?\\n\\nA: \"\n",
    ")\n",
    "print(few_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_example = model.generate(few_shot_prompt, generation_config=moderate_generation_config)\n",
    "print(generation_example.generation[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try prompting the model with a few-shot CoT prompt, where we provide an example of the kind of reasoning required to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n",
      "\n",
      "A: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.\n",
      "\n",
      "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
      "\n",
      "A: \n"
     ]
    }
   ],
   "source": [
    "few_shot_cot_prompt = (\n",
    "    \"Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis \"\n",
    "    \"balls does he have now?\\n\\nA: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. \"\n",
    "    \"5 + 6 = 11. The answer is 11.\\n\\nQ: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 \"\n",
    "    \"more, how many apples do they have?\\n\\nA: \"\n",
    ")\n",
    "print(few_shot_cot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_example = model.generate(few_shot_cot_prompt, generation_config=moderate_generation_config)\n",
    "print(generation_example.generation[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try compare few-shot prompting with few-shot CoT for slightly different kind of problem. This example is drawn from the AQuA: Algebraic Word Problems task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: John found that the average of 15 numbers is 40. If 10 is added to each number then the mean of the numbers is? Answer Choices: (a) 50 (b) 45 (c) 65 (d) 78 (e) 64\n",
      "\n",
      "A: The answer is (a).\n",
      "\n",
      "Q: If a / b = 3/4 and 8a + 5b = 22, then find the value of a. Answer Choices: (a) 1/2 (b) 3/2 (c) 5/2 (d) 4/2 (e) 7/2\n",
      "\n",
      "A: \n"
     ]
    }
   ],
   "source": [
    "few_shot_prompt = (\n",
    "    \"Q: John found that the average of 15 numbers is 40. If 10 is added to each number then the mean of \"\n",
    "    \"the numbers is? Answer Choices: (a) 50 (b) 45 (c) 65 (d) 78 (e) 64\\n\\nA: The answer is (a).\\n\\nQ: If a / b = 3/4 \"\n",
    "    \"and 8a + 5b = 22, then find the value of a. Answer Choices: (a) 1/2 (b) 3/2 (c) 5/2 (d) 4/2 (e) 7/2\\n\\nA: \"\n",
    ")\n",
    "print(few_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_example = model.generate(few_shot_prompt, generation_config=moderate_generation_config)\n",
    "print(generation_example.generation[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct choice for this problem is (b)\n",
    "\n",
    "Let's see if we can extract the correct answer with few-shot CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: John found that the average of 15 numbers is 40. If 10 is added to each number then the mean of the numbers is? Answer Choices: (a) 50 (b) 45 (c) 65 (d) 78 (e) 64\n",
      "\n",
      "A: If 10 is added to each number, then the mean of the numbers also increases by 10. So the new mean would be 50. The answer is (a).\n",
      "\n",
      "Q: If a / b = 3/4 and 8a + 5b = 22,then find the value of a. Answer Choices: (a) 1/2 (b) 3/2 (c) 5/2 (d) 4/2 (e) 7/2 \n",
      "\n",
      "A: \n"
     ]
    }
   ],
   "source": [
    "few_shot_cot_prompt = (\n",
    "    \"Q: John found that the average of 15 numbers is 40. If 10 is added to each number then the mean of the numbers \"\n",
    "    \"is? Answer Choices: (a) 50 (b) 45 (c) 65 (d) 78 (e) 64\\n\\nA: If 10 is added to each number, then the mean of the \"\n",
    "    \"numbers also increases by 10. So the new mean would be 50. The answer is (a).\\n\\nQ: If a / b = 3/4 and 8a + 5b \"\n",
    "    \"= 22,then find the value of a. Answer Choices: (a) 1/2 (b) 3/2 (c) 5/2 (d) 4/2 (e) 7/2 \\n\\nA: \"\n",
    ")\n",
    "print(few_shot_cot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_example = model.generate(few_shot_cot_prompt, generation_config=moderate_generation_config)\n",
    "print(generation_example.generation[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-Shot Chain of Thought Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be tedious and tricky to form useful and effective reasoning examples. Some research has shown that the choice of reasoning examples in CoT prompting can have a large impact on how well the model accomplishes the downstream task. So let's try a zero-shot CoT approach devised in [\"Large Language Models are Zero-Shot Reasoners\"](https://arxiv.org/pdf/2205.11916.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n",
      "\n",
      "A: The answer is 11.\n",
      "\n",
      "Q: A juggler can juggle 16 balls. Half of the balls are golf balls, and half of the golf balls are blue. How many blue golf balls are there?\n",
      "\n",
      "A: \n"
     ]
    }
   ],
   "source": [
    "few_shot_prompt = (\n",
    "    \"Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis \"\n",
    "    \"balls does he have now?\\n\\nA: The answer is 11.\\n\\nQ: A juggler can juggle 16 balls. Half of the balls are golf \"\n",
    "    \"balls, and half of the golf balls are blue. How many blue golf balls are there?\\n\\nA: \"\n",
    ")\n",
    "print(few_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_example = model.generate(few_shot_prompt, generation_config=moderate_generation_config)\n",
    "print(generation_example.generation[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct answer to this problem is 4.\n",
    "\n",
    "Perhaps we can extract the correct answer with zero-Shot CoT is split into two stages:\n",
    "1) Reasoning Generation\n",
    "2) Answer Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: A juggler can juggle 16 balls. Half of the balls are golf balls, and half of the golf balls are blue. How many blue golf balls are there?\n",
      "\n",
      "A: Let’s think step by step.\n"
     ]
    }
   ],
   "source": [
    "reasoning_generation_prompt = (\n",
    "    \"Q: A juggler can juggle 16 balls. Half of the balls are golf balls, and half of the golf balls are blue. How \"\n",
    "    \"many blue golf balls are there?\\n\\nA: Let’s think step by step.\"\n",
    ")\n",
    "print(reasoning_generation_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_generation = model.generate(\n",
    "    reasoning_generation_prompt, generation_config=moderate_generation_config\n",
    ").generation[\"text\"]\n",
    "print(reasoning_generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_extraction_prompt = f\"{reasoning_generation_prompt}\\n{reasoning_generation} Therefore, the answer is \"\n",
    "print(answer_extraction_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_generation = model.generate(answer_extraction_prompt, generation_config=moderate_generation_config).generation[\n",
    "    \"text\"\n",
    "]\n",
    "print(answer_generation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opt_notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
