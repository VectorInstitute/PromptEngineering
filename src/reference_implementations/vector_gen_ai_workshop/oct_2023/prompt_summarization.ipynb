{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "\n",
    "import evaluate\n",
    "import kscope\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conecting to the Service\n",
    "First we connect to the Kaleidoscope service through which we'll interact with the LLMs and see which models are avaiable to us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a client connection to the kscope service\n",
    "client = kscope.Client(gateway_host=\"llm.cluster.local\", gateway_port=3001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show all model instances that are currently active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.model_instances"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we obtain a handle to a model. In this example, let's use the OPT-175B model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = client.load_model(\"falcon-7b\")\n",
    "# If this model is not actively running, it will get launched in the background.\n",
    "# In this case, wait until it moves into an \"ACTIVE\" state before proceeding.\n",
    "while model.state != \"ACTIVE\":\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_generation_config = {\"max_tokens\": 128, \"top_k\": 4, \"top_p\": 1.0, \"temperature\": 0.5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_generations(generation_text: str) -> str:\n",
    "    # This simply attempts to extract the first three \"sentences\" within a generated string\n",
    "    split_text = re.findall(r\".*?[.!\\?]\", generation_text)[0:3]\n",
    "    split_text = [text.strip() for text in split_text]\n",
    "    return \"\\n\".join(split_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Prompts\n",
    "\n",
    "Now let's create a basic prompt template that we can reuse for multiple text inputs. This will be an instruction prompt with an unconstrained answer space as we're going to try to get OPT to summarize texts. We'll try several different templates and examine performance for each. Note that this section simply considers \"manual\" or \"human-level\" inspection to determine the quality of the summary. At the bottom of this notebook, we consider measuring the quality of two prompts on a sample of the CNN Daily Mail task using a ROUGE 1 Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_summary_1 = \"Summarize the preceding text.\"\n",
    "prompt_template_summary_2 = \"TLDR;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"news_summary_datasets/examples_news.txt\", \"r\") as file:\n",
    "    news_stories = [line.strip() for line in file.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_with_template_1 = [f\"{news_story} {prompt_template_summary_1}\" for news_story in news_stories]\n",
    "prompts_with_template_2 = [f\"{news_story} {prompt_template_summary_2}\" for news_story in news_stories]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these examples, we use the prompt structures\n",
    "\n",
    "* (text) Summarize the preceeding text.\n",
    "* (text) TLDR;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Russia has been capturing some of the US and NATO-provided weapons and equipment left on the battlefield in Ukraine and sending them to Iran, where the US believes Tehran will try to reverse-engineer the systems, four sources familiar with the matter told CNN. Over the last year, US, NATO and other Western officials have seen several instances of Russian forces seizing smaller, shoulder-fired weapons equipment including Javelin anti-tank and Stinger anti-aircraft systems that Ukrainian forces have at times been forced to leave behind on the battlefield, the sources told CNN. In many of those cases, Russia has then flown the equipment to Iran to dismantle and analyze, likely so the Iranian military can attempt to make their own version of the weapons, sources said. Russia believes that continuing to provide captured Western weapons to Iran will incentivize Tehran to maintain its support for Russia’s war in Ukraine, the sources said. US officials don’t believe that the issue is widespread or systematic, and the Ukrainian military has made it a habit since the beginning of the war to report to the Pentagon any losses of US-provided equipment to Russian forces, officials said. Still, US officials acknowledge that the issue is difficult to track. Summarize the preceding text.\n",
      "\n",
      "Russia has been capturing some of the US and NATO-provided weapons and equipment left on the battlefield in Ukraine and sending them to Iran, where the US believes Tehran will try to reverse-engineer the systems, four sources familiar with the matter told CNN. Over the last year, US, NATO and other Western officials have seen several instances of Russian forces seizing smaller, shoulder-fired weapons equipment including Javelin anti-tank and Stinger anti-aircraft systems that Ukrainian forces have at times been forced to leave behind on the battlefield, the sources told CNN. In many of those cases, Russia has then flown the equipment to Iran to dismantle and analyze, likely so the Iranian military can attempt to make their own version of the weapons, sources said. Russia believes that continuing to provide captured Western weapons to Iran will incentivize Tehran to maintain its support for Russia’s war in Ukraine, the sources said. US officials don’t believe that the issue is widespread or systematic, and the Ukrainian military has made it a habit since the beginning of the war to report to the Pentagon any losses of US-provided equipment to Russian forces, officials said. Still, US officials acknowledge that the issue is difficult to track. TLDR;\n"
     ]
    }
   ],
   "source": [
    "print(f\"{prompts_with_template_1[0]}\\n\")\n",
    "print(prompts_with_template_2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Summarize the preceding text.\n",
      "Original Length: 1261, Summary Length: 423\n",
      "The US has been providing Ukraine with weapons and equipment since the beginning of the war, and the Ukrainian military has been reporting to the US any losses of US-provided equipment to Russian forces.\n",
      "The US believes that continuing to provide captured Western weapons to Iran will incentivize Tehran to maintain its support for Russia’s war in Ukraine.\n",
      "The US doesn’t believe that the issue is widespread or systematic.\n",
      "====================================================================================\n",
      "\n",
      "Original Length: 1180, Summary Length: 151\n",
      "The National Weather Service issued a flash flood warning for the San Francisco Bay Area, including the city of San Francisco, until 8:30 a.\n",
      "m.\n",
      "Friday.\n",
      "====================================================================================\n",
      "\n",
      "Original Length: 1259, Summary Length: 243\n",
      "The state’s request comes as the Supreme Court is considering a case that could have a major impact on the rights of transgender people.\n",
      "The justices are scheduled to hear arguments in the case, Bostock v.\n",
      "Clayton County, Georgia, on April 28.\n",
      "====================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summaries_1 = [\n",
    "    model.generate(prompt, long_generation_config).generation[\"sequences\"][0] for prompt in prompts_with_template_1\n",
    "]\n",
    "print(f\"Prompt: {prompt_template_summary_1}\")\n",
    "for summary, original_story in zip(summaries_1, news_stories):\n",
    "    # Let's just take the first 3 sentences, split by periods\n",
    "    summary = post_process_generations(summary)\n",
    "    print(f\"Original Length: {len(original_story)}, Summary Length: {len(summary)}\")\n",
    "    print(summary)\n",
    "    print(\"====================================================================================\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Summary fairly good, though a bit unclear. \n",
    "2) The second is captures the flash flood warning but hallucinates that it is for San Francisco. It is not.\n",
    "3) The first sentence is a bit vague, but is relevant. The second sentence is a hallucination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: TLDR;\n",
      "Original Length: 1261, Summary Length: 345\n",
      "Russia is sending captured US weapons to Iran to reverse engineer.\n",
      "The US is concerned that Iran will use the captured weapons to attack US forces in the Middle East.\n",
      "Source: Russia is sending captured US weapons to Iran to reverse engineer – CNNThe US is concerned that Iran will use the captured weapons to attack US forces in the Middle East.\n",
      "====================================================================================\n",
      "\n",
      "Original Length: 1180, Summary Length: 248\n",
      "California is in the middle of a massive storm that could cause widespread flooding and mudslides.\n",
      "The post California braces for ‘historic’ storm appeared first on TheGrio.\n",
      "The post California braces for ‘historic’ storm appeared first on TheGrio.\n",
      "====================================================================================\n",
      "\n",
      "Original Length: 1259, Summary Length: 382\n",
      "The state is asking the Supreme Court to allow it to enforce a law that prohibits transgender women and girls from participating in public school sports.\n",
      "The law was temporarily blocked by a federal judge earlier this year.\n",
      "The state is asking the Supreme Court to vacate the injunction and allow the law to continue protecting West Virginia student athletes this spring and beyond.\n",
      "====================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summaries_2 = [\n",
    "    model.generate(prompt, long_generation_config).generation[\"sequences\"][0] for prompt in prompts_with_template_2\n",
    "]\n",
    "print(f\"Prompt: {prompt_template_summary_2}\")\n",
    "for summary, original_story in zip(summaries_2, news_stories):\n",
    "    # Let's just take the first 3 sentences, split by periods\n",
    "    summary = post_process_generations(summary)\n",
    "    print(f\"Original Length: {len(original_story)}, Summary Length: {len(summary)}\")\n",
    "    print(summary)\n",
    "    print(\"====================================================================================\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) The first sentence is a good gist. The second sentence is hallucination. The third is a non- correct citation.\n",
    "2) The first is a bit vague, but true given the story, the rest is not useful and likely hallucinated\n",
    "3) This is a good summary."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we improve the results by providing additional context to our instructions?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we prompt the model to provide a summary and try to do so in a compact way. We still post-process the text to grab the first three sentences, but hopefully the model tries to pack more information into those first sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Summarize the text in as few words as possible:\n",
      "Original Length: 1261, Summary Length: 460\n",
      "“The US believes that continuing to provide captured Western weapons to Iran will incentivize Tehran to maintain its support for Russia’s war in Ukraine.\n",
      "”The US believes that continuing to provide captured Western weapons to Iran will incentivize Tehran to maintain its support for Russia’s war in Ukraine.\n",
      "The US believes that continuing to provide captured Western weapons to Iran will incentivize Tehran to maintain its support for Russia’s war in Ukraine.\n",
      "====================================================================================\n",
      "\n",
      "Prompt: Summarize the text in as few words as possible:\n",
      "Original Length: 1180, Summary Length: 537\n",
      "“The most dangerous amount of rain could impact nearly 70,000 people along the central California coast, stretching from Salinas southward to San Luis Obispo and including parts of Ventura and Monterey counties.\n",
      "The most dangerous amount of rain could impact nearly 70,000 people along the central California coast, stretching from Salinas southward to San Luis Obispo and including parts of Ventura and Monterey counties, according to the Weather Prediction Center, which issued a Level 4 of 4 warning of excessive rainfall in the area.\n",
      "====================================================================================\n",
      "\n",
      "Prompt: Summarize the text in as few words as possible:\n",
      "Original Length: 1259, Summary Length: 336\n",
      "The state is asking the Supreme Court to allow it to enforce a law that prohibits transgender women and girls from participating in public school sports.\n",
      "The law was enacted in 2021.\n",
      "The state is asking the Supreme Court to allow it to enforce a law that prohibits transgender women and girls from participating in public school sports.\n",
      "====================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_template_summary_3 = \"Summarize the text in as few words as possible:\"\n",
    "prompts_with_template_3 = [f\"{news_story} {prompt_template_summary_3}\" for news_story in news_stories]\n",
    "summaries_3 = [\n",
    "    model.generate(prompt, long_generation_config).generation[\"sequences\"][0] for prompt in prompts_with_template_3\n",
    "]\n",
    "for summary, original_story in zip(summaries_3, news_stories):\n",
    "    print(f\"Prompt: {prompt_template_summary_3}\")\n",
    "    # Let's just take the first 3 sentences, split by periods\n",
    "    summary = post_process_generations(summary)\n",
    "    print(f\"Original Length: {len(original_story)}, Summary Length: {len(summary)}\")\n",
    "    print(summary)\n",
    "    print(\"====================================================================================\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignoring the repetition:\n",
    "1) The first is relevant to the article, but not necessarily a great summary\n",
    "2) Again, relevant to the article, but not a great summary.\n",
    "3) A decent summary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPT, and generative models in general, have been reported to perform better when not prompted with \"declarative\" instructions or direct interogatives (See the [OPT Paper](https://arxiv.org/abs/2205.01068)). As such, let's ask for the summary as a question!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Briefly, what is this story about?\n",
      "Original Length: 1261, Summary Length: 277\n",
      "The story is about the US and NATO-provided weapons and equipment left on the battlefield in Ukraine and sent to Iran.\n",
      "What is the significance of this story?\n",
      "The story is about the US and NATO-provided weapons and equipment left on the battlefield in Ukraine and sent to Iran.\n",
      "====================================================================================\n",
      "\n",
      "Prompt: Briefly, what is this story about?\n",
      "Original Length: 1180, Summary Length: 154\n",
      "The story is about the California weather.\n",
      "What is the main point of this story?\n",
      "The main point of this story is that California is getting a lot of rain.\n",
      "====================================================================================\n",
      "\n",
      "Prompt: Briefly, what is this story about?\n",
      "Original Length: 1259, Summary Length: 361\n",
      "The West Virginia law, which was enacted in 2021, prohibits transgender women and girls from participating in public school sports.\n",
      "The law was challenged by a transgender student athlete who sued the state, and a district court temporarily blocked the law three months after it was enacted.\n",
      "But earlier this year the district court ruled in favor of the state.\n",
      "====================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_template_summary_4 = \"Briefly, what is this story about?\"\n",
    "prompts_with_template_4 = [f\"{news_story}\\n{prompt_template_summary_4}\" for news_story in news_stories]\n",
    "summaries_4 = [\n",
    "    model.generate(prompt, long_generation_config).generation[\"sequences\"][0] for prompt in prompts_with_template_4\n",
    "]\n",
    "for summary, original_story in zip(summaries_4, news_stories):\n",
    "    print(f\"Prompt: {prompt_template_summary_4}\")\n",
    "    # Let's just take the first 3 sentences, split by periods\n",
    "    summary = post_process_generations(summary)\n",
    "    print(f\"Original Length: {len(original_story)}, Summary Length: {len(summary)}\")\n",
    "    print(summary)\n",
    "    print(\"====================================================================================\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignoring the repetition:\n",
    "1) More of a preview than a summary\n",
    "2) Very vague\n",
    "3) A good summary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final example, rather than asking a question, we putting the task in a context that might be more natural for a generative model. That is, we ask it to \"sum up\" the article with a natural phrase prefix to be completed in a \"conversational\" way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: In short,\n",
      "Original Length: 1261, Summary Length: 411\n",
      "the US is concerned that Russia is trying to use captured Western weapons to help Iran reverse-engineer the systems, the sources said.\n",
      "The US has been providing Ukraine with Javelin anti-tank missiles, Stinger anti-aircraft missiles and other weapons since the beginning of the war.\n",
      "The US has also provided Ukraine with a variety of other weapons, including small arms, ammunition, drones, and other equipment.\n",
      "====================================================================================\n",
      "\n",
      "Prompt: In short,\n",
      "Original Length: 1180, Summary Length: 279\n",
      "the storm is expected to bring “life-threatening” flooding to the region, according to the National Weather Service.\n",
      "“The combination of heavy rain and melting snow will result in widespread and potentially historic flooding,” the NWS said.\n",
      "“This is a life-threatening situation.\n",
      "====================================================================================\n",
      "\n",
      "Prompt: In short,\n",
      "Original Length: 1259, Summary Length: 385\n",
      "the state is asking the Supreme Court to allow the law to go into effect while the case is pending.\n",
      "The state’s request comes as the Supreme Court is considering a case that could have a major impact on the rights of transgender people.\n",
      "The justices are scheduled to hear arguments in a case involving a Virginia student who was barred from using the boys’ bathroom at his high school.\n",
      "====================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_template_summary_5 = \"In short,\"\n",
    "prompts_with_template_5 = [f\"{news_story} {prompt_template_summary_5}\" for news_story in news_stories]\n",
    "summaries_5 = [\n",
    "    model.generate(prompt, long_generation_config).generation[\"sequences\"][0] for prompt in prompts_with_template_5\n",
    "]\n",
    "for summary, original_story in zip(summaries_5, news_stories):\n",
    "    print(f\"Prompt: {prompt_template_summary_5}\")\n",
    "    # Let's just take the first 3 sentences, split by periods\n",
    "    summary = post_process_generations(summary)\n",
    "    print(f\"Original Length: {len(original_story)}, Summary Length: {len(summary)}\")\n",
    "    print(summary)\n",
    "    print(\"====================================================================================\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) A pretty good summary, but missing a few major points.\n",
    "2) A good summary, except that the location of interest is not mentioned.\n",
    "3) Sort of a good summary, but the final point is not true, it is about *West* Virgina transgendered athletes participating in sports"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring Performance on CNN Daily Mail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cnn_dailymail (/Users/david/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b0bef463f1940f5a2aa8423cb1f8fef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data from the CNN Daily Mail Test set, the ROUGE metric scorer from Hugging Face, and a Tokenizer from OPT. The tokenizer is used to truncate the text such that it fits nicely into the OPT model context. We truncate the text to 1023, so that it is of length 1024 when the start-of-sentence token (`<s>`) is added.\n",
    "\n",
    "__NOTE__: All OPT models, regardless of size, used the same tokenizer. However, if you want to use a different type of model, a different tokenizer may be needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "dataloader = DataLoader(dataset[\"test\"], shuffle=False, batch_size=1)\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "prompt_template_summary_1 = \"How would you briefly summarize the text?\"\n",
    "prompt_template_summary_2 = \"In short,\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_article_text(article_text: str, tokenizer: AutoTokenizer, max_sequence_length: int = 1023) -> str:\n",
    "    tokenized_article = tokenizer.encode(article_text, truncation=True, max_length=max_sequence_length)\n",
    "    return tokenizer.decode(tokenized_article, skip_special_tokens=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll try two different prompts from the examples above and consider how well they each do in terms of rouge score against reference summaries on the CNN Daily Mail task, which is a common summarization benchmark. You can see a discussion of this dataset here: [CNN Daily Mail](https://huggingface.co/datasets/cnn_dailymail). \n",
    "\n",
    "__Note__: On a big model, like OPT-175, this process will likely take a bit of time, given the length of the articles and the fact that we are asking for 50 summaries."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the First prompt structure\n",
    "\n",
    "(text) How would you briefly summarize the text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Batch: 1\n",
      "Processed Batch: 2\n",
      "Processed Batch: 3\n",
      "Processed Batch: 4\n",
      "Processed Batch: 5\n",
      "Processed Batch: 6\n",
      "Processed Batch: 7\n",
      "Processed Batch: 8\n",
      "Processed Batch: 9\n",
      "Processed Batch: 10\n",
      "Processed Batch: 11\n",
      "Processed Batch: 12\n",
      "Processed Batch: 13\n",
      "Processed Batch: 14\n",
      "Processed Batch: 15\n",
      "Processed Batch: 16\n",
      "Processed Batch: 17\n",
      "Processed Batch: 18\n",
      "Processed Batch: 19\n",
      "Processed Batch: 20\n",
      "Processed Batch: 21\n",
      "Processed Batch: 22\n",
      "Processed Batch: 23\n",
      "Processed Batch: 24\n",
      "Processed Batch: 25\n",
      "Processed Batch: 26\n",
      "Processed Batch: 27\n",
      "Processed Batch: 28\n",
      "Processed Batch: 29\n",
      "Processed Batch: 30\n",
      "Processed Batch: 31\n",
      "Processed Batch: 32\n",
      "Processed Batch: 33\n",
      "Processed Batch: 34\n",
      "Processed Batch: 35\n",
      "Processed Batch: 36\n",
      "Processed Batch: 37\n",
      "Processed Batch: 38\n",
      "Processed Batch: 39\n",
      "Processed Batch: 40\n",
      "Processed Batch: 41\n",
      "Processed Batch: 42\n",
      "Processed Batch: 43\n",
      "Processed Batch: 44\n",
      "Processed Batch: 45\n",
      "Processed Batch: 46\n",
      "Processed Batch: 47\n",
      "Processed Batch: 48\n",
      "Processed Batch: 49\n",
      "Processed Batch: 50\n",
      "Final Rouge Score: 0.23631473596801175\n"
     ]
    }
   ],
   "source": [
    "max_batches = 50\n",
    "batch_rouge_scores = []\n",
    "for batch_number, batch in enumerate(dataloader, 1):\n",
    "    if batch_number > max_batches:\n",
    "        break\n",
    "    truncated_articles = [truncate_article_text(text, opt_tokenizer) for text in batch[\"article\"]]\n",
    "    prompts = [f\"{article_text}\\n{prompt_template_summary_1}\" for article_text in truncated_articles]\n",
    "    summaries = model.generate(prompts, long_generation_config).generation[\"sequences\"]\n",
    "    # Let's just take the first 3 sentences, split by periods\n",
    "    summaries = [post_process_generations(summary) for summary in summaries]\n",
    "    # References for the metric need to be in the form of list of lists\n",
    "    # (ROUGE can admit multiple references per prediction)\n",
    "    highlights = [[highlight] for highlight in batch[\"highlights\"]]\n",
    "    results = rouge.compute(\n",
    "        predictions=summaries,\n",
    "        references=highlights,\n",
    "        rouge_types=[\"rouge1\"],\n",
    "    )\n",
    "    batch_rouge_scores.append(results[\"rouge1\"])\n",
    "    print(f\"Processed Batch: {batch_number}\")\n",
    "# Average all the ROUGE 1 scores together for the final one\n",
    "print(f\"Final Rouge Score: {sum(batch_rouge_scores)/len(batch_rouge_scores)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the second prompt structure\n",
    "\n",
    "(text) In short,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Batch: 1\n",
      "Processed Batch: 2\n",
      "Processed Batch: 3\n",
      "Processed Batch: 4\n",
      "Processed Batch: 5\n",
      "Processed Batch: 6\n",
      "Processed Batch: 7\n",
      "Processed Batch: 8\n",
      "Processed Batch: 9\n",
      "Processed Batch: 10\n",
      "Processed Batch: 11\n",
      "Processed Batch: 12\n",
      "Processed Batch: 13\n",
      "Processed Batch: 14\n",
      "Processed Batch: 15\n",
      "Processed Batch: 16\n",
      "Processed Batch: 17\n",
      "Processed Batch: 18\n",
      "Processed Batch: 19\n",
      "Processed Batch: 20\n",
      "Processed Batch: 21\n",
      "Processed Batch: 22\n",
      "Processed Batch: 23\n",
      "Processed Batch: 24\n",
      "Processed Batch: 25\n",
      "Processed Batch: 26\n",
      "Processed Batch: 27\n",
      "Processed Batch: 28\n",
      "Processed Batch: 29\n",
      "Processed Batch: 30\n",
      "Processed Batch: 31\n",
      "Processed Batch: 32\n",
      "Processed Batch: 33\n",
      "Processed Batch: 34\n",
      "Processed Batch: 35\n",
      "Processed Batch: 36\n",
      "Processed Batch: 37\n",
      "Processed Batch: 38\n",
      "Processed Batch: 39\n",
      "Processed Batch: 40\n",
      "Processed Batch: 41\n",
      "Processed Batch: 42\n",
      "Processed Batch: 43\n",
      "Processed Batch: 44\n",
      "Processed Batch: 45\n",
      "Processed Batch: 46\n",
      "Processed Batch: 47\n",
      "Processed Batch: 48\n",
      "Processed Batch: 49\n",
      "Processed Batch: 50\n",
      "Final Rouge Score: 0.20484075484793451\n"
     ]
    }
   ],
   "source": [
    "max_batches = 50\n",
    "batch_rouge_scores = []\n",
    "for batch_number, batch in enumerate(dataloader, 1):\n",
    "    if batch_number > max_batches:\n",
    "        break\n",
    "    truncated_articles = [truncate_article_text(text, opt_tokenizer) for text in batch[\"article\"]]\n",
    "    prompts = [f\"{article_text}\\n{prompt_template_summary_2}\" for article_text in truncated_articles]\n",
    "    summaries = model.generate(prompts, long_generation_config).generation[\"sequences\"]\n",
    "    # Let's just take the first 3 sentences, split by periods\n",
    "    summaries = [post_process_generations(summary) for summary in summaries]\n",
    "    # References for the metric need to be in the form of list of lists\n",
    "    # (ROUGE can admit multiple references per prediction)\n",
    "    highlights = [[highlight] for highlight in batch[\"highlights\"]]\n",
    "    results = rouge.compute(\n",
    "        predictions=summaries,\n",
    "        references=highlights,\n",
    "        rouge_types=[\"rouge1\"],\n",
    "    )\n",
    "    batch_rouge_scores.append(results[\"rouge1\"])\n",
    "    print(f\"Processed Batch: {batch_number}\")\n",
    "# Average all the ROUGE 1 scores together for the final one\n",
    "print(f\"Final Rouge Score: {sum(batch_rouge_scores)/len(batch_rouge_scores)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second prompt, as measured by ROUGE 1 scores, appears to produce summaries of higher quality than the first prompt. This is likely due to the way it is structured. It fits into the \"generative\" training setting a bit better than asking a point blank question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opt_notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
