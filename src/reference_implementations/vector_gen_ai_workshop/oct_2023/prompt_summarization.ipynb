{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "\n",
    "import evaluate\n",
    "import kscope\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from utils import split_prompts_into_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conecting to the Service\n",
    "First we connect to the Kaleidoscope service through which we'll interact with the LLMs and see which models are avaiable to us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a client connection to the kscope service\n",
    "client = kscope.Client(gateway_host=\"llm.cluster.local\", gateway_port=6001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show all model instances that are currently active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'f6e55e38-2c01-4def-aaf9-d4531f9a2598',\n",
       "  'name': 'OPT-175B',\n",
       "  'state': 'ACTIVE'},\n",
       " {'id': '4213e687-fdaf-4046-a03c-ca59b484ad14',\n",
       "  'name': 'OPT-6.7B',\n",
       "  'state': 'ACTIVE'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.model_instances"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we obtain a handle to a model. In this example, let's use the OPT-175B model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = client.load_model(\"OPT-175B\")\n",
    "# If this model is not actively running, it will get launched in the background.\n",
    "# In this case, wait until it moves into an \"ACTIVE\" state before proceeding.\n",
    "while model.state != \"ACTIVE\":\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_generation_config = {\"max_tokens\": 128, \"top_k\": 4, \"top_p\": 1.0, \"rep_penalty\": 1.2, \"temperature\": 0.5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_generations(generation_text: str) -> str:\n",
    "    # This simply attempts to extract the first three \"sentences\" within a generated string\n",
    "    split_text = re.findall(r\".*?[.!\\?]\", generation_text)[0:3]\n",
    "    split_text = [text.strip() for text in split_text]\n",
    "    return \"\\n\".join(split_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Prompts\n",
    "\n",
    "Now let's create a basic prompt template that we can reuse for multiple text inputs. This will be an instruction prompt with an unconstrained answer space as we're going to try to get OPT to summarize texts. We'll try several different templates and examine performance for each. Note that this section simply considers \"manual\" or \"human-level\" inspection to determine the quality of the summary. At the bottom of this notebook, we consider measuring the quality of two prompts on a sample of the CNN Daily Mail task using a ROUGE 1 Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_summary_1 = \"Summarize the preceding text.\"\n",
    "prompt_template_summary_2 = \"TLDR;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"news_summary_datasets/examples_news.txt\", \"r\") as file:\n",
    "    news_stories = [line.strip() for line in file.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_with_template_1 = [f\"{news_story}\\n{prompt_template_summary_1}\" for news_story in news_stories]\n",
    "prompts_with_template_2 = [f\"{news_story}\\n{prompt_template_summary_2}\" for news_story in news_stories]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these examples, we use the prompt structures\n",
    "\n",
    "* (text) Summarize the preceding text.\n",
    "* (text) TLDR;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Russia has been capturing some of the US and NATO-provided weapons and equipment left on the battlefield in Ukraine and sending them to Iran, where the US believes Tehran will try to reverse-engineer the systems, four sources familiar with the matter told CNN. Over the last year, US, NATO and other Western officials have seen several instances of Russian forces seizing smaller, shoulder-fired weapons equipment including Javelin anti-tank and Stinger anti-aircraft systems that Ukrainian forces have at times been forced to leave behind on the battlefield, the sources told CNN. In many of those cases, Russia has then flown the equipment to Iran to dismantle and analyze, likely so the Iranian military can attempt to make their own version of the weapons, sources said. Russia believes that continuing to provide captured Western weapons to Iran will incentivize Tehran to maintain its support for Russia’s war in Ukraine, the sources said. US officials don’t believe that the issue is widespread or systematic, and the Ukrainian military has made it a habit since the beginning of the war to report to the Pentagon any losses of US-provided equipment to Russian forces, officials said. Still, US officials acknowledge that the issue is difficult to track.\n",
      "Summarize the preceding text.\n",
      "\n",
      "Russia has been capturing some of the US and NATO-provided weapons and equipment left on the battlefield in Ukraine and sending them to Iran, where the US believes Tehran will try to reverse-engineer the systems, four sources familiar with the matter told CNN. Over the last year, US, NATO and other Western officials have seen several instances of Russian forces seizing smaller, shoulder-fired weapons equipment including Javelin anti-tank and Stinger anti-aircraft systems that Ukrainian forces have at times been forced to leave behind on the battlefield, the sources told CNN. In many of those cases, Russia has then flown the equipment to Iran to dismantle and analyze, likely so the Iranian military can attempt to make their own version of the weapons, sources said. Russia believes that continuing to provide captured Western weapons to Iran will incentivize Tehran to maintain its support for Russia’s war in Ukraine, the sources said. US officials don’t believe that the issue is widespread or systematic, and the Ukrainian military has made it a habit since the beginning of the war to report to the Pentagon any losses of US-provided equipment to Russian forces, officials said. Still, US officials acknowledge that the issue is difficult to track.\n",
      "TLDR;\n"
     ]
    }
   ],
   "source": [
    "print(f\"{prompts_with_template_1[0]}\\n\")\n",
    "print(prompts_with_template_2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Summarize the preceding text.\n",
      "Original Length: 1262, Summary Length: 613\n",
      "The Pentagon has confirmed that Russia has been capturing some of the US and NATO-provided weapons and equipment left on the battlefield in Ukraine and sending them to Iran, where the US believes Tehran will try to reverse-engineer the systems, four sources familiar with the matter told CNN. Over the last year, US, NATO and other Western officials have seen several instances of Russian forces seizing smaller, shoulder-fired weapons equipment including Javelin anti-tank and Stinger anti-aircraft systems that Ukrainian forces have at times been forced to leave behind on the battlefield, the sources told CNN.\n",
      "====================================================================================\n",
      "\n",
      "Original Length: 1181, Summary Length: 552\n",
      "Officials in California issued evacuation warnings in portions of several counties amid powerful storms likely to deliver severe rainfall and cause widespread flooding across the central and northern parts of the state Friday. The most dangerous amount of rain could impact nearly 70,000 people along the central California coast, stretching from Salinas southward to San Luis Obispo and including parts of Ventura and Monterey counties, according to the Weather Prediction Center, which issued a Level 4 of 4 warning of excessive rainfall in the area.\n",
      "====================================================================================\n",
      "\n",
      "Original Length: 1260, Summary Length: 446\n",
      "The request is the latest in a string of efforts by states to restrict the rights of transgender youth, including in sports, to use the bathroom of their choice, and to receive gender-affirming medical care. The Supreme Court is set to hear a case this year on the rights of transgender students to use the bathroom of their choice. The court has also previously ruled that sex discrimination bans in federal law also apply to transgender people.\n",
      "====================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generation_1 = model.generate(prompts_with_template_1, long_generation_config)\n",
    "print(f\"Prompt: {prompt_template_summary_1}\")\n",
    "for summary, original_story in zip(generation_1.generation[\"text\"], news_stories):\n",
    "    # Let's just take the first 3 sentences, split by periods\n",
    "    summary = post_process_generations(summary)\n",
    "    print(f\"Original Length: {len(original_story)}, Summary Length: {len(summary)}\")\n",
    "    print(summary)\n",
    "    print(\"====================================================================================\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: TLDR;\n",
      "Original Length: 1262, Summary Length: 581\n",
      "Russia has been capturing some of the US and NATO-provided weapons and equipment left on the battlefield in Ukraine and sending them to Iran, where the US believes Tehran will try to reverse-engineer the systems, four sources familiar with the matter told CNN. Over the last year, US, NATO and other Western officials have seen several instances of Russian forces seizing smaller, shoulder-fired weapons equipment including Javelin anti-tank and Stinger anti-aircraft systems that Ukrainian forces have at times been forced to leave behind on the battlefield, the sources told CNN.\n",
      "====================================================================================\n",
      "\n",
      "Prompt: TLDR;\n",
      "Original Length: 1181, Summary Length: 138\n",
      "I'm glad I'm not in California right now. I'm glad I don't live in California, too. But I'm not sure what this has to do with the article.\n",
      "====================================================================================\n",
      "\n",
      "Prompt: TLDR;\n",
      "Original Length: 1260, Summary Length: 371\n",
      "West Virginia wants to prevent trans women and girls from playing sports. The state is appealing a ruling by the 4th Circuit Court of Appeals, which temporarily blocked the law from going into effect. The law, which was signed by Republican Governor Jim Justice in April, bans transgender athletes from competing in women's sports in middle and high schools and colleges.\n",
      "====================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generation_2 = model.generate(prompts_with_template_2, long_generation_config)\n",
    "for summary, original_story in zip(generation_2.generation[\"text\"], news_stories):\n",
    "    print(f\"Prompt: {prompt_template_summary_2}\")\n",
    "    # Let's just take the first 3 sentences, split by periods\n",
    "    summary = post_process_generations(summary)\n",
    "    print(f\"Original Length: {len(original_story)}, Summary Length: {len(summary)}\")\n",
    "    print(summary)\n",
    "    print(\"====================================================================================\")\n",
    "    print(\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Story 2 is about the possibility of severe flooding in California and an evacuation order being issued. Let's see what we get that from the three summaries and maybe which worked better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize the preceding text.|| Officials in California issued evacuation warnings in portions of several counties amid powerful storms likely to deliver severe rainfall and cause widespread flooding across the central and northern parts of the state Friday. The most dangerous amount of rain could impact nearly 70,000 people along the central California coast, stretching from Salinas southward to San Luis Obispo and including parts of Ventura and Monterey counties, according to the Weather Prediction Center, which issued a Level 4 of 4 warning of excessive rainfall in the area.\n",
      "====================================================================================\n",
      "Short Summary:|| This is a dangerous situation. Flooding is imminent and may have already occurred. Do not attempt to travel unless it is absolutely necessary to do so.\n",
      "====================================================================================\n",
      "TLDR;|| I'm glad I'm not in California right now. I'm glad I don't live in California, too. But I'm not sure what this has to do with the article.\n"
     ]
    }
   ],
   "source": [
    "print(f\"{prompt_template_summary_1}\\n{post_process_generations(generation_1.generation['text'][1])}\")\n",
    "print(\"====================================================================================\\n\")\n",
    "print(f\"{prompt_template_summary_2}\\n {post_process_generations(generation_2.generation['text'][1])}\")\n",
    "print(\"====================================================================================\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we improve the results by providing additional context to our instructions?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we prompt the model to provide a summary and try to do so in a compact way. We still post-process the text to grab the first three sentences, but hopefully the model tries to pack more information into those first sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Summarize the text in as few words as possible:\n",
      "Original Length: 1262, Summary Length: 46\n",
      "Russia is supplying Iran with US-made weapons.\n",
      "====================================================================================\n",
      "\n",
      "Prompt: Summarize the text in as few words as possible:\n",
      "Original Length: 1181, Summary Length: 141\n",
      "A. A powerful storm is likely to deliver severe rainfall and cause widespread flooding across the central and northern parts of the state. B.\n",
      "====================================================================================\n",
      "\n",
      "Prompt: Summarize the text in as few words as possible:\n",
      "Original Length: 1260, Summary Length: 410\n",
      "The West Virginia law prohibits transgender women and girls from playing in any public school sports that are designated for girls and women. It allows for some transgender boys and men to play in sports designated for boys and men. The law also states that “any student athlete who is unable to prove her biological sex” through a blood test may participate in sports for the gender with which she identifies.\n",
      "====================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_template_summary_3 = \"Summarize the text in as few words as possible:\"\n",
    "prompts_with_template_3 = [f\"{news_story}\\n{prompt_template_summary_3}\" for news_story in news_stories]\n",
    "generation_3 = model.generate(prompts_with_template_3, long_generation_config)\n",
    "for summary, original_story in zip(generation_3.generation[\"text\"], news_stories):\n",
    "    print(f\"Prompt: {prompt_template_summary_3}\")\n",
    "    # Let's just take the first 3 sentences, split by periods\n",
    "    summary = post_process_generations(summary)\n",
    "    print(f\"Original Length: {len(original_story)}, Summary Length: {len(summary)}\")\n",
    "    print(summary)\n",
    "    print(\"====================================================================================\")\n",
    "    print(\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPT, and generative models in general, have been reported to perform better when not prompted with \"declarative\" instructions or direct interogatives (See the [OPT Paper](https://arxiv.org/abs/2205.01068)). As such, let's ask for the summary as a question!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Briefly, what is this story about?\n",
      "Original Length: 1262, Summary Length: 582\n",
      "Russian has been capturing some of the US and NATO-provided weapons and equipment left on the battlefield in Ukraine and sending them to Iran, where the US believes Tehran will try to reverse-engineer the systems, four sources familiar with the matter told CNN. Over the last year, US, NATO and other Western officials have seen several instances of Russian forces seizing smaller, shoulder-fired weapons equipment including Javelin anti-tank and Stinger anti-aircraft systems that Ukrainian forces have at times been forced to leave behind on the battlefield, the sources told CNN.\n",
      "====================================================================================\n",
      "\n",
      "Prompt: Briefly, what is this story about?\n",
      "Original Length: 1181, Summary Length: 199\n",
      "A wildfire that started in the hills of Los Angeles quickly spread to more than 1,000 acres and forced evacuations in the area. The blaze, dubbed the La Tuna fire, was reported just before 3:30 p. m.\n",
      "====================================================================================\n",
      "\n",
      "Prompt: Briefly, what is this story about?\n",
      "Original Length: 1260, Summary Length: 430\n",
      "The law, known as the Save Women’s Sports Act, prohibits transgender women and girls from participating in school sports that align with their gender identity. It does not apply to men’s sports. The law says that “athletic teams or sports designated for females, women, or girls shall not be open to students of the male sex where selection for such teams is based on competitive skill or the activity involved is a contact sport.\n",
      "====================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_template_summary_4 = \"Briefly, what is this story about?\"\n",
    "prompts_with_template_4 = [f\"{news_story}\\n{prompt_template_summary_4}\" for news_story in news_stories]\n",
    "generation_4 = model.generate(prompts_with_template_4, long_generation_config)\n",
    "for summary, original_story in zip(generation_4.generation[\"text\"], news_stories):\n",
    "    print(f\"Prompt: {prompt_template_summary_4}\")\n",
    "    # Let's just take the first 3 sentences, split by periods\n",
    "    summary = post_process_generations(summary)\n",
    "    print(f\"Original Length: {len(original_story)}, Summary Length: {len(summary)}\")\n",
    "    print(summary)\n",
    "    print(\"====================================================================================\")\n",
    "    print(\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final example, rather than asking a question, we putting the task in a context that might be more natural for a generative model. That is, we ask it to \"sum up\" the article with a natural phrase prefix to be completed in a \"conversational\" way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: In short,\n",
      "Original Length: 1262, Summary Length: 219\n",
      "the US is fighting a proxy war against Russia in Syria, and Russia is fighting a proxy war against the US in Ukraine. It is a proxy war of a proxy war. The US is also fighting a proxy war against Russia in Central Asia.\n",
      "====================================================================================\n",
      "\n",
      "Prompt: In short,\n",
      "Original Length: 1181, Summary Length: 254\n",
      "California is in a state of emergency as a result of the largest storm in its history. “It’s a big storm, but it’s not going to be a huge storm,” said Jan Null, a meteorologist with Golden Gate Weather Services. “It’s going to be a very manageable storm.\n",
      "====================================================================================\n",
      "\n",
      "Prompt: In short,\n",
      "Original Length: 1260, Summary Length: 374\n",
      "the state is asking the Supreme Court to step in and allow it to enforce the law while the appeals process plays out. The Supreme Court is currently shorthanded, with only eight justices, and it’s unclear if the court will take up the case. Justice Samuel Alito, who handles emergency requests from the 4th Circuit, could act on his own to grant or deny the state’s request.\n",
      "====================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_template_summary_5 = \"In short,\"\n",
    "prompts_with_template_5 = [f\"{news_story} {prompt_template_summary_5}\" for news_story in news_stories]\n",
    "generation_5 = model.generate(prompts_with_template_5, long_generation_config)\n",
    "for summary, original_story in zip(generation_5.generation[\"text\"], news_stories):\n",
    "    print(f\"Prompt: {prompt_template_summary_5}\")\n",
    "    # Let's just take the first 3 sentences, split by periods\n",
    "    summary = post_process_generations(summary)\n",
    "    print(f\"Original Length: {len(original_story)}, Summary Length: {len(summary)}\")\n",
    "    print(summary)\n",
    "    print(\"====================================================================================\")\n",
    "    print(\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring Performance on CNN Daily Mail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cnn_dailymail (/Users/david/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85fda5af741d4f298ac7c317f9fff1af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data from the CNN Daily Mail Test set, the ROUGE metric scorer from Hugging Face, and a Tokenizer from OPT. The tokenizer is used to truncate the text such that it fits nicely into the OPT model context. We truncate the text to 1023, so that it is of length 1024 when the start-of-sentence token (`<s>`) is added.\n",
    "\n",
    "__NOTE__: All OPT models, regardless of size, used the same tokenizer. However, if you want to use a different type of model, a different tokenizer may be needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "dataloader = DataLoader(dataset[\"test\"], shuffle=False, batch_size=10)\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "prompt_template_summary_1 = \"How would you briefly summarize the text?\"\n",
    "prompt_template_summary_2 = \"In short,\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_article_text(article_text: str, tokenizer: AutoTokenizer, max_sequence_length: int = 1023) -> str:\n",
    "    tokenized_article = tokenizer.encode(article_text, truncation=True, max_length=max_sequence_length)\n",
    "    return tokenizer.decode(tokenized_article, skip_special_tokens=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll try two different prompts from the examples above and consider how well they each do in terms of rouge score against reference summaries on the CNN Daily Mail task, which is a common summarization benchmark. You can see a discussion of this dataset here: [CNN Daily Mail](https://huggingface.co/datasets/cnn_dailymail). \n",
    "\n",
    "__Note__: On a big model, like OPT-175, this process will likely take a bit of time, given the length of the articles and the fact that we are asking for 100 summaries."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the First prompt structure\n",
    "\n",
    "(text) How would you briefly summarize the text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Batch: 1\n",
      "Processing Batch: 2\n",
      "Processing Batch: 3\n",
      "Processing Batch: 4\n",
      "Processing Batch: 5\n",
      "Processing Batch: 6\n",
      "Processing Batch: 7\n",
      "Processing Batch: 8\n",
      "Processing Batch: 9\n",
      "Processing Batch: 10\n",
      "Final Rouge Score: 0.12556041350754338\n"
     ]
    }
   ],
   "source": [
    "max_batches = 10\n",
    "batch_rouge_scores = []\n",
    "for batch_number, batch in enumerate(dataloader, 1):\n",
    "    if batch_number > max_batches:\n",
    "        break\n",
    "    print(f\"Processing Batch: {batch_number}\")\n",
    "    truncated_articles = [truncate_article_text(text, opt_tokenizer) for text in batch[\"article\"]]\n",
    "    prompts = [f\"{article_text}\\n{prompt_template_summary_1}\" for article_text in truncated_articles]\n",
    "    prompt_batches = split_prompts_into_batches(prompts, batch_size=1)\n",
    "    for prompt_batch in prompt_batches:\n",
    "        summaries = model.generate(prompt_batch, long_generation_config).generation[\"text\"]\n",
    "        # Let's just take the first 3 sentences, split by periods\n",
    "        summaries = [post_process_generations(summary) for summary in summaries]\n",
    "        # References for the metric need to be in the form of list of lists\n",
    "        # (ROUGE can admit multiple references per prediction)\n",
    "        highlights = [[highlight] for highlight in batch[\"highlights\"]]\n",
    "        results = rouge.compute(\n",
    "            predictions=summaries,\n",
    "            references=highlights,\n",
    "            rouge_types=[\"rouge1\"],\n",
    "        )\n",
    "        batch_rouge_scores.append(results[\"rouge1\"])\n",
    "# Average all the ROUGE 1 scores together for the final one\n",
    "print(f\"Final Rouge Score: {sum(batch_rouge_scores)/len(batch_rouge_scores)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the second prompt structure\n",
    "\n",
    "(text) In short,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Batch: 1\n",
      "Processing Batch: 2\n",
      "Processing Batch: 3\n",
      "Processing Batch: 4\n",
      "Processing Batch: 5\n",
      "Processing Batch: 6\n",
      "Processing Batch: 7\n",
      "Processing Batch: 8\n",
      "Processing Batch: 9\n",
      "Processing Batch: 10\n",
      "Final Rouge Score: 0.20367009239505895\n"
     ]
    }
   ],
   "source": [
    "max_batches = 10\n",
    "batch_rouge_scores = []\n",
    "for batch_number, batch in enumerate(dataloader, 1):\n",
    "    if batch_number > max_batches:\n",
    "        break\n",
    "    print(f\"Processing Batch: {batch_number}\")\n",
    "    truncated_articles = [truncate_article_text(text, opt_tokenizer) for text in batch[\"article\"]]\n",
    "    prompts = [f\"{article_text}\\n{prompt_template_summary_2}\" for article_text in truncated_articles]\n",
    "    prompt_batches = split_prompts_into_batches(prompts, batch_size=1)\n",
    "    for prompt_batch in prompt_batches:\n",
    "        summaries = model.generate(prompt_batch, long_generation_config).generation[\"text\"]\n",
    "        # Let's just take the first 3 sentences, split by periods\n",
    "        summaries = [post_process_generations(summary) for summary in summaries]\n",
    "        # References for the metric need to be in the form of list of lists\n",
    "        # (ROUGE can admit multiple references per prediction)\n",
    "        highlights = [[highlight] for highlight in batch[\"highlights\"]]\n",
    "        results = rouge.compute(\n",
    "            predictions=summaries,\n",
    "            references=highlights,\n",
    "            rouge_types=[\"rouge1\"],\n",
    "        )\n",
    "        batch_rouge_scores.append(results[\"rouge1\"])\n",
    "# Average all the ROUGE 1 scores together for the final one\n",
    "print(f\"Final Rouge Score: {sum(batch_rouge_scores)/len(batch_rouge_scores)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second prompt, as measured by ROUGE 1 scores, appears to produce summaries of higher quality than the first prompt. This is likely due to the way it is structured. It fits into the \"generative\" training setting a bit better than asking a point blank question."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opt_notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
