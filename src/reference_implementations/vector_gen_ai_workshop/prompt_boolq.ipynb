{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from random import sample\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import kscope\n",
    "import pandas as pd\n",
    "from metrics import report_metrics\n",
    "from transformers import AutoTokenizer\n",
    "from utils import get_label_token_ids, get_label_with_highest_likelihood, split_prompts_into_batches"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conecting to the Service"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we connect to the Kaleidoscope service through which we'll interact with the LLMs and see which models are avaiable to us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a client connection to the kscope service\n",
    "client = kscope.Client(gateway_host=\"llm.cluster.local\", gateway_port=3001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show all model instances that are currently active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '9b805833-72a0-4107-ad3c-d83c1e6f573d',\n",
       "  'name': 'OPT-175B',\n",
       "  'state': 'ACTIVE'},\n",
       " {'id': '7a398e3c-2f11-495e-9b5b-c53e4c89ed21',\n",
       "  'name': 'OPT-6.7B',\n",
       "  'state': 'ACTIVE'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.model_instances"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we obtain a handle to a model. In this example, let's use the OPT-175B model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = client.load_model(\"OPT-175B\")\n",
    "# If this model is not actively running, it will get launched in the background.\n",
    "# In this case, wait until it moves into an \"ACTIVE\" state before proceeding.\n",
    "while model.state != \"ACTIVE\":\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "moderated_generation_config = {\"max_tokens\": 20, \"top_k\": 4, \"top_p\": 1.0, \"rep_penalty\": 1.5, \"temperature\": 0.7}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's ask the model some questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"\\nI don't know and I don't care about the capitals of other countries.\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation = model.generate(\"What is the capital of Canada?\", moderated_generation_config)\n",
    "# Extract the text from the returned generation\n",
    "generation.generation[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"\\nWhen they became a Dominion, like in 1901\\nThat doesn't answer the question.  There\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation = model.generate(\"When did Canada become an independent country?\", moderated_generation_config)\n",
    "# Extract the text from the returned generation\n",
    "generation.generation[\"text\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving on to BOOL Q"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to configure the model to generate in the way we want it to. So we set a number of important parameters. For a discussion of the configuration parameters see: `src/reference_implementations/prompting_vector_llms/CONFIG_README.md`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_generation_config = {\"max_tokens\": 1, \"top_k\": 1, \"top_p\": 1.0, \"rep_penalty\": 1.2, \"temperature\": 0.01}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions and Preprocessing the Bool Q Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to have the model attempt to answer questions based on some context. Each question has a boolean answer. We'll compare zero and few-shot prompts along with two different label spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boolq_preprocessor(path: str) -> Tuple[List[str], List[str], List[str], List[int]]:\n",
    "    boolq_df = pd.read_csv(path)\n",
    "    titles = boolq_df[\"Title\"].tolist()\n",
    "    passages = boolq_df[\"Passage\"].tolist()\n",
    "    questions = boolq_df[\"Question\"].tolist()\n",
    "    labels = boolq_df[\"Answer\"].apply(lambda x: 1 if x else 0).tolist()\n",
    "    return titles, passages, questions, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in a sampling of the BoolQ test dataset and a small set of \"training\" examples from the training dataset for\n",
    "# few-shot prompting\n",
    "bool_q_test_titles, bool_q_test_passages, bool_q_test_questions, bool_q_test_labels = boolq_preprocessor(\n",
    "    \"boolq_task_datasets/test_sample_dataset.csv\"\n",
    ")\n",
    "bool_q_train_titles, bool_q_train_passages, bool_q_train_questions, bool_q_train_labels = boolq_preprocessor(\n",
    "    \"boolq_task_datasets/example_dataset.csv\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In creating prompts, demonstrations are used for few-shot examples. If `demonstrations` in the `create_prompts` function is an empty string then the prompt is zero shot (that is, it includes no demonstrations). We follow the prompt structure used by the original [GPT-3 paper](https://arxiv.org/pdf/2005.14165.pdf) for the BoolQ task. That is \n",
    "\n",
    "{title} -- {passage}\n",
    "\n",
    "question: {question}\n",
    "\n",
    "answer: {answer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_demonstrations(\n",
    "    demo_titles: List[str],\n",
    "    demo_passages: List[str],\n",
    "    demo_questions: List[str],\n",
    "    demo_labels: List[int],\n",
    "    label_map: Dict[int, str],\n",
    "    n_demos: Optional[int],\n",
    ") -> str:\n",
    "    # n_demos controls how many demonstration examples are included. That is, n_demo-shot prompts are created\n",
    "    demonstrations = []\n",
    "    for demo_title, demo_passage, demo_question, demo_label in zip(\n",
    "        demo_titles, demo_passages, demo_questions, demo_labels\n",
    "    ):\n",
    "        label_str = label_map[demo_label]\n",
    "        demonstration = f\"{demo_title} -- {demo_passage}\\nquestion: {demo_question}?\\nanswer: {label_str}\\n\\n\"\n",
    "        demonstrations.append(demonstration)\n",
    "    demonstration_str = \"\".join(sample(demonstrations, n_demos)) if n_demos else \"\".join(demonstrations)\n",
    "    return demonstration_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompts(\n",
    "    demonstrations: str, test_titles: List[str], test_passages: List[str], test_questions: List[str]\n",
    ") -> List[str]:\n",
    "    prompts = []\n",
    "    for test_title, test_passage, test_question in zip(test_titles, test_passages, test_questions):\n",
    "        prompt = f\"{demonstrations}{test_title} -- {test_passage}\\nquestion: {test_question}?\\nanswer:\"\n",
    "        prompts.append(prompt)\n",
    "    return prompts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bool Q and Prompt Examples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bool Q Task Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snellen chart -- A Snellen chart is an eye chart that can be used to measure visual acuity. Snellen charts are named after the Dutch ophthalmologist Herman Snellen, who developed the chart in 1862. Many ophthalmologists and vision scientists now use an improved chart known as the LogMAR chart.\n",
      "question: do all eye doctors use the same eye chart?\n",
      "answer: No\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    create_demonstrations(\n",
    "        bool_q_train_titles, bool_q_train_passages, bool_q_train_questions, bool_q_train_labels, {0: \"No\", 1: \"Yes\"}, 1\n",
    "    )\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bool Q Zero-Shot Prompt Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rabies transmission -- Transmission between humans is extremely rare, although it can happen through organ transplants, or through bites.\n",
      "question: can a person transmit rabies to another person?\n",
      "answer:\n"
     ]
    }
   ],
   "source": [
    "print(create_prompts(\"\", bool_q_test_titles, bool_q_test_passages, bool_q_test_questions)[24])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bool Q Few-Shot Prompt Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 100 (TV series) -- In March 2017, The CW renewed the series for a fifth season, which premiered on April 24, 2018. In May 2018, the series was renewed for a sixth season.\n",
      "question: are they gonna make a season 5 of the 100?\n",
      "answer: Yes\n",
      "\n",
      "Environmental issues in Australia -- Climate change is now a major political talking point in Australia in the last two decades. Persistent drought, and resulting water restrictions during the first decade of the twenty-first century, are an example of natural events' tangible effect on economic and political realities .\n",
      "question: are there any major water concerns for australia?\n",
      "answer: Yes\n",
      "\n",
      "Snellen chart -- A Snellen chart is an eye chart that can be used to measure visual acuity. Snellen charts are named after the Dutch ophthalmologist Herman Snellen, who developed the chart in 1862. Many ophthalmologists and vision scientists now use an improved chart known as the LogMAR chart.\n",
      "question: do all eye doctors use the same eye chart?\n",
      "answer: No\n",
      "\n",
      "Rabies transmission -- Transmission between humans is extremely rare, although it can happen through organ transplants, or through bites.\n",
      "question: can a person transmit rabies to another person?\n",
      "answer:\n"
     ]
    }
   ],
   "source": [
    "example_demonstrations = create_demonstrations(\n",
    "    bool_q_train_titles, bool_q_train_passages, bool_q_train_questions, bool_q_train_labels, {0: \"No\", 1: \"Yes\"}, 3\n",
    ")\n",
    "print(create_prompts(example_demonstrations, bool_q_test_titles, bool_q_test_passages, bool_q_test_questions)[24])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-Shot Prompting\n",
    "\n",
    "In this section, we won't include any demonstrations in our prompts and will measure the accuracy of the models answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {0: \"No\", 1: \"Yes\"}\n",
    "label_ordering = [\"No\", \"Yes\"]\n",
    "prompts = create_prompts(\"\", bool_q_test_titles, bool_q_test_passages, bool_q_test_questions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a generated response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shear wall -- In structural engineering, a shear wall is a structural system composed of braced panels (also known as shear panels) to counter the effects of lateral load acting on a structure. Wind and seismic loads are the most common building codes, including the International Building Code (where it is called a braced wall line) and Uniform Building Code, all exterior wall lines in wood or steel frame construction must be braced. Depending on the size of the building some interior walls must be braced as well.\n",
      "question: is a shear wall a load bearing wall?\n",
      "answer:\n"
     ]
    }
   ],
   "source": [
    "print(prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' yes']\n"
     ]
    }
   ],
   "source": [
    "generation_example = model.generate(prompts[0], generation_config=short_generation_config)\n",
    "print(generation_example.generation[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number 1 Complete\n",
      "Batch number 2 Complete\n",
      "Batch number 3 Complete\n",
      "Batch number 4 Complete\n",
      "Batch number 5 Complete\n",
      "Batch number 6 Complete\n",
      "Batch number 7 Complete\n",
      "Batch number 8 Complete\n",
      "Batch number 9 Complete\n",
      "Batch number 10 Complete\n"
     ]
    }
   ],
   "source": [
    "# For memory management, we split the prompts into batches of size 10\n",
    "predicted_labels = []\n",
    "prompt_batches = split_prompts_into_batches(prompts)\n",
    "for batch_num, prompt_batch in enumerate(prompt_batches):\n",
    "    generations = model.generate(prompt_batch, generation_config=short_generation_config)\n",
    "    generated_tokens = generations.generation[\"text\"]\n",
    "    predicted_labels.extend(generated_tokens)\n",
    "    print(f\"Batch number {batch_num+1} Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy: 0.79\n",
      "Confusion Matrix with ordering ['no', 'yes']\n",
      "[[22 17]\n",
      " [ 4 57]]\n",
      "========================================================\n",
      "Label: no, F1: 0.676923076923077, Precision: 0.5641025641025641, Recall: 0.8461538461538461\n",
      "Label: yes, F1: 0.8444444444444444, Precision: 0.9344262295081968, Recall: 0.7702702702702703\n"
     ]
    }
   ],
   "source": [
    "# Map the labels from integers to strings for comparison to the string predicted labels in the confusion matrix\n",
    "bool_q_text_labels_string = [label_map[label] for label in bool_q_test_labels]\n",
    "report_metrics(predicted_labels, bool_q_text_labels_string, labels_order=label_ordering)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-Shot Examples\n",
    "\n",
    "In this section, we use a single example in our prompt before asking our model to answer the question that we care about for each data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {0: \"No\", 1: \"Yes\"}\n",
    "label_ordering = [\"No\", \"Yes\"]\n",
    "demonstrations_1 = create_demonstrations(\n",
    "    bool_q_train_titles, bool_q_train_passages, bool_q_train_questions, bool_q_train_labels, label_map, 1\n",
    ")\n",
    "prompts = create_prompts(demonstrations_1, bool_q_test_titles, bool_q_test_passages, bool_q_test_questions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a generated response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 100 (TV series) -- In March 2017, The CW renewed the series for a fifth season, which premiered on April 24, 2018. In May 2018, the series was renewed for a sixth season.\n",
      "question: are they gonna make a season 5 of the 100?\n",
      "answer: Yes\n",
      "\n",
      "Shear wall -- In structural engineering, a shear wall is a structural system composed of braced panels (also known as shear panels) to counter the effects of lateral load acting on a structure. Wind and seismic loads are the most common building codes, including the International Building Code (where it is called a braced wall line) and Uniform Building Code, all exterior wall lines in wood or steel frame construction must be braced. Depending on the size of the building some interior walls must be braced as well.\n",
      "question: is a shear wall a load bearing wall?\n",
      "answer:\n"
     ]
    }
   ],
   "source": [
    "print(prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' No']\n"
     ]
    }
   ],
   "source": [
    "generation_example = model.generate(prompts[0], generation_config=short_generation_config)\n",
    "print(generation_example.generation[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number 1 Complete\n",
      "Batch number 2 Complete\n",
      "Batch number 3 Complete\n",
      "Batch number 4 Complete\n",
      "Batch number 5 Complete\n",
      "Batch number 6 Complete\n",
      "Batch number 7 Complete\n",
      "Batch number 8 Complete\n",
      "Batch number 9 Complete\n",
      "Batch number 10 Complete\n"
     ]
    }
   ],
   "source": [
    "# For memory management, we split the prompts into batches of size 10\n",
    "predicted_labels = []\n",
    "prompt_batches = split_prompts_into_batches(prompts)\n",
    "for batch_num, prompt_batch in enumerate(prompt_batches):\n",
    "    generations = model.generate(prompt_batch, generation_config=short_generation_config)\n",
    "    generated_tokens = generations.generation[\"text\"]\n",
    "    predicted_labels.extend(generated_tokens)\n",
    "    print(f\"Batch number {batch_num+1} Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy: 0.82\n",
      "Confusion Matrix with ordering ['no', 'yes']\n",
      "[[26 13]\n",
      " [ 5 56]]\n",
      "========================================================\n",
      "Label: no, F1: 0.7428571428571428, Precision: 0.6666666666666666, Recall: 0.8387096774193549\n",
      "Label: yes, F1: 0.8615384615384616, Precision: 0.9180327868852459, Recall: 0.8115942028985508\n"
     ]
    }
   ],
   "source": [
    "# Map the labels from integers to strings for comparison to the string predicted labels in the confusion matrix\n",
    "bool_q_text_labels_string = [label_map[label] for label in bool_q_test_labels]\n",
    "report_metrics(predicted_labels, bool_q_text_labels_string, labels_order=label_ordering)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-Shot Examples (Note that this takes quite a long time to run!)\n",
    "\n",
    "In this section, we use five fixed examples in our prompt before asking our model to answer the question that we care about for each data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {0: \"No\", 1: \"Yes\"}\n",
    "label_ordering = [\"No\", \"Yes\"]\n",
    "demonstrations_1 = create_demonstrations(\n",
    "    bool_q_train_titles, bool_q_train_passages, bool_q_train_questions, bool_q_train_labels, label_map, 5\n",
    ")\n",
    "prompts = create_prompts(demonstrations_1, bool_q_test_titles, bool_q_test_passages, bool_q_test_questions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a generated response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mariana Trench -- This was followed by the unmanned ROVs Kaikō in 1996 and Nereus in 2009. The first three expeditions directly measured very similar depths of 10,902 to 10,916 m (35,768 to 35,814 ft). The fourth was made by Canadian film director James Cameron in 2012. On 26 March, he reached the bottom of the Mariana Trench in the submersible vessel Deepsea Challenger.\n",
      "question: have we reached the bottom of the mariana trench?\n",
      "answer: Yes\n",
      "\n",
      "Maple syrup -- Maple syrup is a syrup usually made from the xylem sap of sugar maple, red maple, or black maple trees, although it can also be made from other maple species. In cold climates, these trees store starch in their trunks and roots before winter; the starch is then converted to sugar that rises in the sap in late winter and early spring. Maple trees are tapped by drilling holes into their trunks and collecting the exuded sap, which is processed by heating to evaporate much of the water, leaving the concentrated syrup.\n",
      "question: does maple syrup come straight from the tree?\n",
      "answer: No\n",
      "\n",
      "Triamcinolone acetonide -- Triamcinolone acetonide as an intra-articular injectable has been used to treat a variety of musculoskeletal conditions. When applied as a topical ointment, applied to the skin, it is used to mitigate blistering from poison ivy, oak, and sumac, . When combined with Nystatin, it is used to treat skin infections with discomfort from fungus, though it should not be used on the eyes, mouth, or genital area. It provides relatively immediate relief and is used before using oral prednisone. Oral and dental paste preparations are used for treating aphthous ulcers.\n",
      "question: can triamcinolone cream be used for poison ivy?\n",
      "answer: Yes\n",
      "\n",
      "The 100 (TV series) -- In March 2017, The CW renewed the series for a fifth season, which premiered on April 24, 2018. In May 2018, the series was renewed for a sixth season.\n",
      "question: are they gonna make a season 5 of the 100?\n",
      "answer: Yes\n",
      "\n",
      "Environmental issues in Australia -- Climate change is now a major political talking point in Australia in the last two decades. Persistent drought, and resulting water restrictions during the first decade of the twenty-first century, are an example of natural events' tangible effect on economic and political realities .\n",
      "question: are there any major water concerns for australia?\n",
      "answer: Yes\n",
      "\n",
      "Shear wall -- In structural engineering, a shear wall is a structural system composed of braced panels (also known as shear panels) to counter the effects of lateral load acting on a structure. Wind and seismic loads are the most common building codes, including the International Building Code (where it is called a braced wall line) and Uniform Building Code, all exterior wall lines in wood or steel frame construction must be braced. Depending on the size of the building some interior walls must be braced as well.\n",
      "question: is a shear wall a load bearing wall?\n",
      "answer:\n"
     ]
    }
   ],
   "source": [
    "print(prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Yes']\n"
     ]
    }
   ],
   "source": [
    "generation_example = model.generate(prompts[0], generation_config=short_generation_config)\n",
    "print(generation_example.generation[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number 1 Complete\n",
      "Batch number 2 Complete\n",
      "Batch number 3 Complete\n",
      "Batch number 4 Complete\n",
      "Batch number 5 Complete\n",
      "Batch number 6 Complete\n",
      "Batch number 7 Complete\n",
      "Batch number 8 Complete\n",
      "Batch number 9 Complete\n",
      "Batch number 10 Complete\n"
     ]
    }
   ],
   "source": [
    "# For memory management, we split the prompts into batches of size 10\n",
    "predicted_labels = []\n",
    "prompt_batches = split_prompts_into_batches(prompts)\n",
    "for batch_num, prompt_batch in enumerate(prompt_batches):\n",
    "    generations = model.generate(prompt_batch, generation_config=short_generation_config)\n",
    "    generated_tokens = generations.generation[\"text\"]\n",
    "    predicted_labels.extend(generated_tokens)\n",
    "    print(f\"Batch number {batch_num+1} Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy: 0.87\n",
      "Confusion Matrix with ordering ['no', 'yes']\n",
      "[[29 10]\n",
      " [ 3 58]]\n",
      "========================================================\n",
      "Label: no, F1: 0.8169014084507042, Precision: 0.7435897435897436, Recall: 0.90625\n",
      "Label: yes, F1: 0.8992248062015503, Precision: 0.9508196721311475, Recall: 0.8529411764705882\n"
     ]
    }
   ],
   "source": [
    "# Map the labels from integers to strings for comparison to the string predicted labels in the confusion matrix\n",
    "bool_q_text_labels_string = [label_map[label] for label in bool_q_test_labels]\n",
    "report_metrics(predicted_labels, bool_q_text_labels_string, labels_order=label_ordering)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Deeper Approach than Verbalization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we need to have better control over the generations? That is, we have specific labels for the model to generate (we'll see an example of this in the CoPA task). The model won't always want to pick those labels. Many papers (GPT-3, OPT), use model estimates of likelihood to select the label the model thinks makes the most sense.\n",
    "\n",
    "Rather than working with the generated output of the model, we'll actually investigate the outputs from the output projection layer of the LLM.\n",
    "\n",
    "As an example, consider the case that, rather than yes/no, we want to know whether, based on the question, the model thinks true/false is more likely."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we consider what happens with zero-shot generation for a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {0: \"False\", 1: \"True\"}\n",
    "label_ordering = [\"False\", \"True\"]\n",
    "prompts = create_prompts(\"\", bool_q_test_titles, bool_q_test_passages, bool_q_test_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "Shear wall -- In structural engineering, a shear wall is a structural system composed of braced panels (also known as shear panels) to counter the effects of lateral load acting on a structure. Wind and seismic loads are the most common building codes, including the International Building Code (where it is called a braced wall line) and Uniform Building Code, all exterior wall lines in wood or steel frame construction must be braced. Depending on the size of the building some interior walls must be braced as well.\n",
      "question: is a shear wall a load bearing wall?\n",
      "answer:\n",
      "----------------------------\n",
      "Response:\n",
      " yes\n",
      "\n",
      "\n",
      "Prompt:\n",
      "Cannabis in Connecticut -- Cannabis in Connecticut is illegal for recreational use, but possession of small amounts is decriminalized. Medical usage is permitted.\n",
      "question: is it illegal to smoke weed in ct?\n",
      "answer:\n",
      "----------------------------\n",
      "Response:\n",
      " yes\n",
      "\n",
      "\n",
      "Prompt:\n",
      "Croatia at the FIFA World Cup -- Croatia national football team have appeared in the FIFA World Cup on five occasions (in 1998, 2002, 2006, 2014 and 2018) since gaining independence in 1991. Before that, from 1930 to 1990 Croatia was part of Yugoslavia. For World Cup records and appearances in that period, see Yugoslavia national football team and Serbia at the FIFA World Cup. Their best result thus far was silver position at the 2018 final, where they lost 4-2 to France.\n",
      "question: has croatia ever placed in the world cup?\n",
      "answer:\n",
      "----------------------------\n",
      "Response:\n",
      " yes\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generation_examples = model.generate(prompts[0:3], generation_config=short_generation_config)\n",
    "for index, generated_text in enumerate(generation_examples.generation[\"text\"]):\n",
    "    print(f\"Prompt:\\n{prompts[index]}\")\n",
    "    print(\"----------------------------\")\n",
    "    print(f\"Response:\\n{generated_text}\\n\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the model wants to respond with yes or no, which is reasonable given how we phrased the question. However, if we were expecting true or false responses, we'd have a problem. We can use the model's likelihood estimates over the vocabulary to get a sense for whether the model believes true or false would have been the better response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'decoder.output_projection'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We're interested in the activations from the last layer of the model, because this will allow us to calculate the\n",
    "# likelihoods.\n",
    "last_layer_name = model.module_names[-1]\n",
    "last_layer_name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last layer of the model corresponds to the \"probabilities\" of each token in the model vocabulary. That is, it is proportional to the conditional probability\n",
    "$$\n",
    "P(y_t \\vert y_{<t}, x),\n",
    "$$\n",
    "The probability distribution over the vocabulary of the next token given the preceding tokens $y_{<t}$, and the prompt text $x$. Thus, for each token $y_{t}$ in our input, we get back a 50K vector corresponding to the likelihood over the vocabulary of $y_{t+1}$. We only care about the last token in our input, as it houses the likelihood of the, as yet, unseen token the model will generate."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll stick with a zero-shot prompt for this task to demonstrate the utility of taking over the generation process"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to instantiate a tokenizer to obtain appropriate token indices for our labels. \n",
    "\n",
    "__NOTE__: All OPT models, regardless of size, used the same tokenizer. However, if you want to use a different type of model, a different tokenizer may be needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' False True'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "# extract the tokenizer ids associated with our labels\n",
    "label_token_ids = get_label_token_ids(tokenizer, prompts[0], label_ordering)\n",
    "# If you ever need to move back from token ids, you can use tokenizer.decode or tokenizer.batch_decode\n",
    "tokenizer.decode(label_token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activations matrix shape: torch.Size([121, 50272])\n",
      "Tokenized prompt length: 121\n",
      "Likelihoods of the labels: tensor([3.7656, 3.5000])\n",
      "Predicted Label: False\n"
     ]
    }
   ],
   "source": [
    "activations = model.get_activations(prompts[0], [last_layer_name], short_generation_config)\n",
    "last_layer_matrix = activations.activations[0][last_layer_name]\n",
    "# The shape of this tensor should be number of input tokens by the vocabulary size (n x 50272)\n",
    "print(f\"Activations matrix shape: {last_layer_matrix.shape}\")\n",
    "print(f\"Tokenized prompt length: {len(tokenizer.encode(prompts[0], add_special_tokens=False))}\")\n",
    "print(f\"Likelihoods of the labels: {last_layer_matrix[-1][label_token_ids].float()}\")\n",
    "predicted_label = get_label_with_highest_likelihood(last_layer_matrix, label_token_ids, label_map)\n",
    "print(f\"Predicted Label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number 1 Complete\n",
      "Batch number 2 Complete\n",
      "Batch number 3 Complete\n",
      "Batch number 4 Complete\n",
      "Batch number 5 Complete\n",
      "Batch number 6 Complete\n",
      "Batch number 7 Complete\n",
      "Batch number 8 Complete\n",
      "Batch number 9 Complete\n",
      "Batch number 10 Complete\n"
     ]
    }
   ],
   "source": [
    "# For memory management, we split the prompts into batches of size 10\n",
    "predicted_labels = []\n",
    "prompt_batches = split_prompts_into_batches(prompts)\n",
    "for batch_num, prompt_batch in enumerate(prompt_batches):\n",
    "    activations = model.get_activations(prompt_batch, [last_layer_name], short_generation_config)\n",
    "    print(f\"Batch number {batch_num+1} Complete\")\n",
    "    for activations_single_prompt in activations.activations:\n",
    "        last_layer_matrix = activations_single_prompt[last_layer_name]\n",
    "        predicted_label = get_label_with_highest_likelihood(last_layer_matrix, label_token_ids, label_map)\n",
    "        predicted_labels.append(predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy: 0.62\n",
      "Confusion Matrix with ordering ['false', 'true']\n",
      "[[36  3]\n",
      " [35 26]]\n",
      "========================================================\n",
      "Label: false, F1: 0.6545454545454545, Precision: 0.9230769230769231, Recall: 0.5070422535211268\n",
      "Label: true, F1: 0.5777777777777777, Precision: 0.4262295081967213, Recall: 0.896551724137931\n"
     ]
    }
   ],
   "source": [
    "# Map the labels from integers to strings for comparison to the string predicted labels in the confusion matrix\n",
    "bool_q_text_labels_string = [label_map[label] for label in bool_q_test_labels]\n",
    "report_metrics(predicted_labels, bool_q_text_labels_string, labels_order=label_ordering)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opt_notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
