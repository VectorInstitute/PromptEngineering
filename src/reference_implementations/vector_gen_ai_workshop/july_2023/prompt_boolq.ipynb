{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from random import sample\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import kscope\n",
    "import pandas as pd\n",
    "from metrics import report_metrics\n",
    "from transformers import AutoTokenizer\n",
    "from utils import get_label_token_ids, get_label_with_highest_likelihood, split_prompts_into_batches"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conecting to the Service"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we connect to the Kaleidoscope service through which we'll interact with the LLMs and see which models are avaiable to us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a client connection to the kscope service\n",
    "client = kscope.Client(gateway_host=\"llm.cluster.local\", gateway_port=6001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show all model instances that are currently active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.model_instances"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we obtain a handle to a model. In this example, let's use the OPT-175B model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/david/Desktop/VectorRepositories/PromptEngineering/src/reference_implementations/vector_gen_ai_workshop/prompt_boolq.ipynb Cell 8\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/david/Desktop/VectorRepositories/PromptEngineering/src/reference_implementations/vector_gen_ai_workshop/prompt_boolq.ipynb#Y131sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39mload_model(\u001b[39m\"\u001b[39m\u001b[39mOPT-175B\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/david/Desktop/VectorRepositories/PromptEngineering/src/reference_implementations/vector_gen_ai_workshop/prompt_boolq.ipynb#Y131sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# If this model is not actively running, it will get launched in the background.\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/david/Desktop/VectorRepositories/PromptEngineering/src/reference_implementations/vector_gen_ai_workshop/prompt_boolq.ipynb#Y131sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# In this case, wait until it moves into an \"ACTIVE\" state before proceeding.\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/david/Desktop/VectorRepositories/PromptEngineering/src/reference_implementations/vector_gen_ai_workshop/prompt_boolq.ipynb#Y131sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mwhile\u001b[39;00m model\u001b[39m.\u001b[39;49mstate \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mACTIVE\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/david/Desktop/VectorRepositories/PromptEngineering/src/reference_implementations/vector_gen_ai_workshop/prompt_boolq.ipynb#Y131sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/prompt_engineering/lib/python3.9/site-packages/kscope/kaleidoscope_sdk.py:216\u001b[0m, in \u001b[0;36mModel.state\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    214\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstate\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    215\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Returns a string describing the state of the model\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_session\u001b[39m.\u001b[39;49mget_model_instance(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mid)[\u001b[39m\"\u001b[39m\u001b[39mstate\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/prompt_engineering/lib/python3.9/site-packages/kscope/kaleidoscope_sdk.py:155\u001b[0m, in \u001b[0;36mGatewaySession.get_model_instance\u001b[0;34m(self, model_instance_id)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_model_instance\u001b[39m(\u001b[39mself\u001b[39m, model_instance_id: \u001b[39mstr\u001b[39m):\n\u001b[1;32m    153\u001b[0m     url \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_addr(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmodels/instances/\u001b[39m\u001b[39m{\u001b[39;00mmodel_instance_id\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 155\u001b[0m     response \u001b[39m=\u001b[39m get(url, auth_key\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mauth_key)\n\u001b[1;32m    156\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/prompt_engineering/lib/python3.9/site-packages/kscope/utils.py:45\u001b[0m, in \u001b[0;36mget\u001b[0;34m(addr, auth_key, headers)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39mif\u001b[39;00m auth_key:\n\u001b[1;32m     43\u001b[0m     headers[\u001b[39m\"\u001b[39m\u001b[39mAuthorization\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBearer \u001b[39m\u001b[39m{\u001b[39;00mauth_key\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 45\u001b[0m resp \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mget(addr, headers\u001b[39m=\u001b[39;49mheaders)\n\u001b[1;32m     46\u001b[0m check_response(resp)\n\u001b[1;32m     48\u001b[0m \u001b[39mreturn\u001b[39;00m resp\u001b[39m.\u001b[39mjson()\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/prompt_engineering/lib/python3.9/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39;49m\u001b[39mget\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, params\u001b[39m=\u001b[39;49mparams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/prompt_engineering/lib/python3.9/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/prompt_engineering/lib/python3.9/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    582\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    583\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    584\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    585\u001b[0m }\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/prompt_engineering/lib/python3.9/site-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    700\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    703\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    704\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/prompt_engineering/lib/python3.9/site-packages/requests/adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    488\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 489\u001b[0m         resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    490\u001b[0m             method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    491\u001b[0m             url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    492\u001b[0m             body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    493\u001b[0m             headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    494\u001b[0m             redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m             assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    496\u001b[0m             preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    497\u001b[0m             decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    498\u001b[0m             retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    499\u001b[0m             timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    500\u001b[0m         )\n\u001b[1;32m    502\u001b[0m     \u001b[39m# Send the request.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39mproxy_pool\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/prompt_engineering/lib/python3.9/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    704\u001b[0m     conn,\n\u001b[1;32m    705\u001b[0m     method,\n\u001b[1;32m    706\u001b[0m     url,\n\u001b[1;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    711\u001b[0m )\n\u001b[1;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[1;32m    717\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/prompt_engineering/lib/python3.9/site-packages/urllib3/connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    444\u001b[0m             httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[1;32m    445\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    450\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    451\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/prompt_engineering/lib/python3.9/site-packages/urllib3/connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    442\u001b[0m     \u001b[39m# Python 3\u001b[39;00m\n\u001b[1;32m    443\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m         httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    445\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m         \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m         \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m         \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    449\u001b[0m         six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/prompt_engineering/lib/python3.9/http/client.py:1377\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1376\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1377\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   1378\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[1;32m   1379\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/prompt_engineering/lib/python3.9/http/client.py:320\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    321\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[1;32m    322\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/prompt_engineering/lib/python3.9/http/client.py:281\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 281\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    282\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[1;32m    283\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/prompt_engineering/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    705\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = client.load_model(\"OPT-175B\")\n",
    "# If this model is not actively running, it will get launched in the background.\n",
    "# In this case, wait until it moves into an \"ACTIVE\" state before proceeding.\n",
    "while model.state != \"ACTIVE\":\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "moderate_generation_config = {\"max_tokens\": 20, \"top_k\": 4, \"top_p\": 1.0, \"rep_penalty\": 1.5, \"temperature\": 0.7}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's ask the model some questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"\\nI don't know and I don't care about the capitals of other countries.\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation = model.generate(\"What is the capital of Canada?\", moderate_generation_config)\n",
    "# Extract the text from the returned generation\n",
    "generation.generation[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"\\nWhen they became a Dominion, like in 1901\\nThat doesn't answer the question.  There\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation = model.generate(\"When did Canada become an independent country?\", moderate_generation_config)\n",
    "# Extract the text from the returned generation\n",
    "generation.generation[\"text\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving on to BOOL Q"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to configure the model to generate in the way we want it to. So we set a number of important parameters. For a discussion of the configuration parameters see: `src/reference_implementations/prompting_vector_llms/CONFIG_README.md`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_generation_config = {\"max_tokens\": 1, \"top_k\": 1, \"top_p\": 1.0, \"rep_penalty\": 1.2, \"temperature\": 0.01}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions and Preprocessing the Bool Q Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to have the model attempt to answer questions based on some context. Each question has a boolean answer. We'll compare zero and few-shot prompts along with two different label spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boolq_preprocessor(path: str) -> Tuple[List[str], List[str], List[str], List[int]]:\n",
    "    boolq_df = pd.read_csv(path)\n",
    "    titles = boolq_df[\"Title\"].tolist()\n",
    "    passages = boolq_df[\"Passage\"].tolist()\n",
    "    questions = boolq_df[\"Question\"].tolist()\n",
    "    labels = boolq_df[\"Answer\"].apply(lambda x: 1 if x else 0).tolist()\n",
    "    return titles, passages, questions, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in a sampling of the BoolQ test dataset and a small set of \"training\" examples from the training dataset for\n",
    "# few-shot prompting\n",
    "bool_q_test_titles, bool_q_test_passages, bool_q_test_questions, bool_q_test_labels = boolq_preprocessor(\n",
    "    \"boolq_task_datasets/test_sample_dataset.csv\"\n",
    ")\n",
    "bool_q_train_titles, bool_q_train_passages, bool_q_train_questions, bool_q_train_labels = boolq_preprocessor(\n",
    "    \"boolq_task_datasets/example_dataset.csv\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In creating prompts, demonstrations are used for few-shot examples. If `demonstrations` in the `create_prompts` function is an empty string then the prompt is zero shot (that is, it includes no demonstrations). We follow the prompt structure used by the original [GPT-3 paper](https://arxiv.org/pdf/2005.14165.pdf) for the BoolQ task. That is \n",
    "\n",
    "{title} -- {passage}\n",
    "\n",
    "question: {question}\n",
    "\n",
    "answer: {answer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_demonstrations(\n",
    "    demo_titles: List[str],\n",
    "    demo_passages: List[str],\n",
    "    demo_questions: List[str],\n",
    "    demo_labels: List[int],\n",
    "    label_map: Dict[int, str],\n",
    "    n_demos: Optional[int],\n",
    ") -> str:\n",
    "    # n_demos controls how many demonstration examples are included. That is, n_demo-shot prompts are created\n",
    "    demonstrations = []\n",
    "    for demo_title, demo_passage, demo_question, demo_label in zip(\n",
    "        demo_titles, demo_passages, demo_questions, demo_labels\n",
    "    ):\n",
    "        label_str = label_map[demo_label]\n",
    "        demonstration = f\"{demo_title} -- {demo_passage}\\nquestion: {demo_question}?\\nanswer: {label_str}\\n\\n\"\n",
    "        demonstrations.append(demonstration)\n",
    "    demonstration_str = \"\".join(sample(demonstrations, n_demos)) if n_demos else \"\".join(demonstrations)\n",
    "    return demonstration_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompts(\n",
    "    demonstrations: str, test_titles: List[str], test_passages: List[str], test_questions: List[str]\n",
    ") -> List[str]:\n",
    "    prompts = []\n",
    "    for test_title, test_passage, test_question in zip(test_titles, test_passages, test_questions):\n",
    "        prompt = f\"{demonstrations}{test_title} -- {test_passage}\\nquestion: {test_question}?\\nanswer:\"\n",
    "        prompts.append(prompt)\n",
    "    return prompts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bool Q and Prompt Examples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bool Q Task Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snellen chart -- A Snellen chart is an eye chart that can be used to measure visual acuity. Snellen charts are named after the Dutch ophthalmologist Herman Snellen, who developed the chart in 1862. Many ophthalmologists and vision scientists now use an improved chart known as the LogMAR chart.\n",
      "question: do all eye doctors use the same eye chart?\n",
      "answer: No\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    create_demonstrations(\n",
    "        bool_q_train_titles, bool_q_train_passages, bool_q_train_questions, bool_q_train_labels, {0: \"No\", 1: \"Yes\"}, 1\n",
    "    )\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bool Q Zero-Shot Prompt Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rabies transmission -- Transmission between humans is extremely rare, although it can happen through organ transplants, or through bites.\n",
      "question: can a person transmit rabies to another person?\n",
      "answer:\n"
     ]
    }
   ],
   "source": [
    "print(create_prompts(\"\", bool_q_test_titles, bool_q_test_passages, bool_q_test_questions)[24])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bool Q Few-Shot Prompt Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 100 (TV series) -- In March 2017, The CW renewed the series for a fifth season, which premiered on April 24, 2018. In May 2018, the series was renewed for a sixth season.\n",
      "question: are they gonna make a season 5 of the 100?\n",
      "answer: Yes\n",
      "\n",
      "Environmental issues in Australia -- Climate change is now a major political talking point in Australia in the last two decades. Persistent drought, and resulting water restrictions during the first decade of the twenty-first century, are an example of natural events' tangible effect on economic and political realities .\n",
      "question: are there any major water concerns for australia?\n",
      "answer: Yes\n",
      "\n",
      "Snellen chart -- A Snellen chart is an eye chart that can be used to measure visual acuity. Snellen charts are named after the Dutch ophthalmologist Herman Snellen, who developed the chart in 1862. Many ophthalmologists and vision scientists now use an improved chart known as the LogMAR chart.\n",
      "question: do all eye doctors use the same eye chart?\n",
      "answer: No\n",
      "\n",
      "Rabies transmission -- Transmission between humans is extremely rare, although it can happen through organ transplants, or through bites.\n",
      "question: can a person transmit rabies to another person?\n",
      "answer:\n"
     ]
    }
   ],
   "source": [
    "example_demonstrations = create_demonstrations(\n",
    "    bool_q_train_titles, bool_q_train_passages, bool_q_train_questions, bool_q_train_labels, {0: \"No\", 1: \"Yes\"}, 3\n",
    ")\n",
    "print(create_prompts(example_demonstrations, bool_q_test_titles, bool_q_test_passages, bool_q_test_questions)[24])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-Shot Prompting\n",
    "\n",
    "In this section, we won't include any demonstrations in our prompts and will measure the accuracy of the models answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {0: \"No\", 1: \"Yes\"}\n",
    "label_ordering = [\"No\", \"Yes\"]\n",
    "prompts = create_prompts(\"\", bool_q_test_titles, bool_q_test_passages, bool_q_test_questions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a generated response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shear wall -- In structural engineering, a shear wall is a structural system composed of braced panels (also known as shear panels) to counter the effects of lateral load acting on a structure. Wind and seismic loads are the most common building codes, including the International Building Code (where it is called a braced wall line) and Uniform Building Code, all exterior wall lines in wood or steel frame construction must be braced. Depending on the size of the building some interior walls must be braced as well.\n",
      "question: is a shear wall a load bearing wall?\n",
      "answer:\n"
     ]
    }
   ],
   "source": [
    "print(prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' yes']\n"
     ]
    }
   ],
   "source": [
    "generation_example = model.generate(prompts[0], generation_config=short_generation_config)\n",
    "print(generation_example.generation[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number 1 Complete\n",
      "Batch number 2 Complete\n",
      "Batch number 3 Complete\n",
      "Batch number 4 Complete\n",
      "Batch number 5 Complete\n",
      "Batch number 6 Complete\n",
      "Batch number 7 Complete\n",
      "Batch number 8 Complete\n",
      "Batch number 9 Complete\n",
      "Batch number 10 Complete\n"
     ]
    }
   ],
   "source": [
    "# For memory management, we split the prompts into batches of size 10\n",
    "predicted_labels = []\n",
    "prompt_batches = split_prompts_into_batches(prompts)\n",
    "for batch_num, prompt_batch in enumerate(prompt_batches):\n",
    "    generations = model.generate(prompt_batch, generation_config=short_generation_config)\n",
    "    generated_tokens = generations.generation[\"text\"]\n",
    "    predicted_labels.extend(generated_tokens)\n",
    "    print(f\"Batch number {batch_num+1} Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy: 0.79\n",
      "Confusion Matrix with ordering ['no', 'yes']\n",
      "[[22 17]\n",
      " [ 4 57]]\n",
      "========================================================\n",
      "Label: no, F1: 0.676923076923077, Precision: 0.5641025641025641, Recall: 0.8461538461538461\n",
      "Label: yes, F1: 0.8444444444444444, Precision: 0.9344262295081968, Recall: 0.7702702702702703\n"
     ]
    }
   ],
   "source": [
    "# Map the labels from integers to strings for comparison to the string predicted labels in the confusion matrix\n",
    "bool_q_text_labels_string = [label_map[label] for label in bool_q_test_labels]\n",
    "report_metrics(predicted_labels, bool_q_text_labels_string, labels_order=label_ordering)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-Shot Examples\n",
    "\n",
    "In this section, we use a single example in our prompt before asking our model to answer the question that we care about for each data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {0: \"No\", 1: \"Yes\"}\n",
    "label_ordering = [\"No\", \"Yes\"]\n",
    "demonstrations_1 = create_demonstrations(\n",
    "    bool_q_train_titles, bool_q_train_passages, bool_q_train_questions, bool_q_train_labels, label_map, 1\n",
    ")\n",
    "prompts = create_prompts(demonstrations_1, bool_q_test_titles, bool_q_test_passages, bool_q_test_questions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a generated response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 100 (TV series) -- In March 2017, The CW renewed the series for a fifth season, which premiered on April 24, 2018. In May 2018, the series was renewed for a sixth season.\n",
      "question: are they gonna make a season 5 of the 100?\n",
      "answer: Yes\n",
      "\n",
      "Shear wall -- In structural engineering, a shear wall is a structural system composed of braced panels (also known as shear panels) to counter the effects of lateral load acting on a structure. Wind and seismic loads are the most common building codes, including the International Building Code (where it is called a braced wall line) and Uniform Building Code, all exterior wall lines in wood or steel frame construction must be braced. Depending on the size of the building some interior walls must be braced as well.\n",
      "question: is a shear wall a load bearing wall?\n",
      "answer:\n"
     ]
    }
   ],
   "source": [
    "print(prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' No']\n"
     ]
    }
   ],
   "source": [
    "generation_example = model.generate(prompts[0], generation_config=short_generation_config)\n",
    "print(generation_example.generation[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number 1 Complete\n",
      "Batch number 2 Complete\n",
      "Batch number 3 Complete\n",
      "Batch number 4 Complete\n",
      "Batch number 5 Complete\n",
      "Batch number 6 Complete\n",
      "Batch number 7 Complete\n",
      "Batch number 8 Complete\n",
      "Batch number 9 Complete\n",
      "Batch number 10 Complete\n"
     ]
    }
   ],
   "source": [
    "# For memory management, we split the prompts into batches of size 10\n",
    "predicted_labels = []\n",
    "prompt_batches = split_prompts_into_batches(prompts)\n",
    "for batch_num, prompt_batch in enumerate(prompt_batches):\n",
    "    generations = model.generate(prompt_batch, generation_config=short_generation_config)\n",
    "    generated_tokens = generations.generation[\"text\"]\n",
    "    predicted_labels.extend(generated_tokens)\n",
    "    print(f\"Batch number {batch_num+1} Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy: 0.82\n",
      "Confusion Matrix with ordering ['no', 'yes']\n",
      "[[26 13]\n",
      " [ 5 56]]\n",
      "========================================================\n",
      "Label: no, F1: 0.7428571428571428, Precision: 0.6666666666666666, Recall: 0.8387096774193549\n",
      "Label: yes, F1: 0.8615384615384616, Precision: 0.9180327868852459, Recall: 0.8115942028985508\n"
     ]
    }
   ],
   "source": [
    "# Map the labels from integers to strings for comparison to the string predicted labels in the confusion matrix\n",
    "bool_q_text_labels_string = [label_map[label] for label in bool_q_test_labels]\n",
    "report_metrics(predicted_labels, bool_q_text_labels_string, labels_order=label_ordering)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-Shot Examples (Note that this takes quite a long time to run!)\n",
    "\n",
    "In this section, we use five fixed examples in our prompt before asking our model to answer the question that we care about for each data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {0: \"No\", 1: \"Yes\"}\n",
    "label_ordering = [\"No\", \"Yes\"]\n",
    "demonstrations_1 = create_demonstrations(\n",
    "    bool_q_train_titles, bool_q_train_passages, bool_q_train_questions, bool_q_train_labels, label_map, 5\n",
    ")\n",
    "prompts = create_prompts(demonstrations_1, bool_q_test_titles, bool_q_test_passages, bool_q_test_questions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a generated response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mariana Trench -- This was followed by the unmanned ROVs Kaik≈ç in 1996 and Nereus in 2009. The first three expeditions directly measured very similar depths of 10,902 to 10,916 m (35,768 to 35,814 ft). The fourth was made by Canadian film director James Cameron in 2012. On 26 March, he reached the bottom of the Mariana Trench in the submersible vessel Deepsea Challenger.\n",
      "question: have we reached the bottom of the mariana trench?\n",
      "answer: Yes\n",
      "\n",
      "Maple syrup -- Maple syrup is a syrup usually made from the xylem sap of sugar maple, red maple, or black maple trees, although it can also be made from other maple species. In cold climates, these trees store starch in their trunks and roots before winter; the starch is then converted to sugar that rises in the sap in late winter and early spring. Maple trees are tapped by drilling holes into their trunks and collecting the exuded sap, which is processed by heating to evaporate much of the water, leaving the concentrated syrup.\n",
      "question: does maple syrup come straight from the tree?\n",
      "answer: No\n",
      "\n",
      "Triamcinolone acetonide -- Triamcinolone acetonide as an intra-articular injectable has been used to treat a variety of musculoskeletal conditions. When applied as a topical ointment, applied to the skin, it is used to mitigate blistering from poison ivy, oak, and sumac, . When combined with Nystatin, it is used to treat skin infections with discomfort from fungus, though it should not be used on the eyes, mouth, or genital area. It provides relatively immediate relief and is used before using oral prednisone. Oral and dental paste preparations are used for treating aphthous ulcers.\n",
      "question: can triamcinolone cream be used for poison ivy?\n",
      "answer: Yes\n",
      "\n",
      "The 100 (TV series) -- In March 2017, The CW renewed the series for a fifth season, which premiered on April 24, 2018. In May 2018, the series was renewed for a sixth season.\n",
      "question: are they gonna make a season 5 of the 100?\n",
      "answer: Yes\n",
      "\n",
      "Environmental issues in Australia -- Climate change is now a major political talking point in Australia in the last two decades. Persistent drought, and resulting water restrictions during the first decade of the twenty-first century, are an example of natural events' tangible effect on economic and political realities .\n",
      "question: are there any major water concerns for australia?\n",
      "answer: Yes\n",
      "\n",
      "Shear wall -- In structural engineering, a shear wall is a structural system composed of braced panels (also known as shear panels) to counter the effects of lateral load acting on a structure. Wind and seismic loads are the most common building codes, including the International Building Code (where it is called a braced wall line) and Uniform Building Code, all exterior wall lines in wood or steel frame construction must be braced. Depending on the size of the building some interior walls must be braced as well.\n",
      "question: is a shear wall a load bearing wall?\n",
      "answer:\n"
     ]
    }
   ],
   "source": [
    "print(prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Yes']\n"
     ]
    }
   ],
   "source": [
    "generation_example = model.generate(prompts[0], generation_config=short_generation_config)\n",
    "print(generation_example.generation[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number 1 Complete\n",
      "Batch number 2 Complete\n",
      "Batch number 3 Complete\n",
      "Batch number 4 Complete\n",
      "Batch number 5 Complete\n",
      "Batch number 6 Complete\n",
      "Batch number 7 Complete\n",
      "Batch number 8 Complete\n",
      "Batch number 9 Complete\n",
      "Batch number 10 Complete\n"
     ]
    }
   ],
   "source": [
    "# For memory management, we split the prompts into batches of size 10\n",
    "predicted_labels = []\n",
    "prompt_batches = split_prompts_into_batches(prompts)\n",
    "for batch_num, prompt_batch in enumerate(prompt_batches):\n",
    "    generations = model.generate(prompt_batch, generation_config=short_generation_config)\n",
    "    generated_tokens = generations.generation[\"text\"]\n",
    "    predicted_labels.extend(generated_tokens)\n",
    "    print(f\"Batch number {batch_num+1} Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy: 0.87\n",
      "Confusion Matrix with ordering ['no', 'yes']\n",
      "[[29 10]\n",
      " [ 3 58]]\n",
      "========================================================\n",
      "Label: no, F1: 0.8169014084507042, Precision: 0.7435897435897436, Recall: 0.90625\n",
      "Label: yes, F1: 0.8992248062015503, Precision: 0.9508196721311475, Recall: 0.8529411764705882\n"
     ]
    }
   ],
   "source": [
    "# Map the labels from integers to strings for comparison to the string predicted labels in the confusion matrix\n",
    "bool_q_text_labels_string = [label_map[label] for label in bool_q_test_labels]\n",
    "report_metrics(predicted_labels, bool_q_text_labels_string, labels_order=label_ordering)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Deeper Approach than Verbalization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we need to have better control over the generations? That is, we have specific labels for the model to generate (we'll see an example of this in the CoPA task). The model won't always want to pick those labels. Many papers (GPT-3, OPT), use model estimates of likelihood to select the label the model thinks makes the most sense.\n",
    "\n",
    "Rather than working with the generated output of the model, we'll actually investigate the outputs from the output projection layer of the LLM.\n",
    "\n",
    "As an example, consider the case that, rather than yes/no, we want to know whether, based on the question, the model thinks true/false is more likely."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we consider what happens with zero-shot generation for a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {0: \"False\", 1: \"True\"}\n",
    "label_ordering = [\"False\", \"True\"]\n",
    "prompts = create_prompts(\"\", bool_q_test_titles, bool_q_test_passages, bool_q_test_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "Shear wall -- In structural engineering, a shear wall is a structural system composed of braced panels (also known as shear panels) to counter the effects of lateral load acting on a structure. Wind and seismic loads are the most common building codes, including the International Building Code (where it is called a braced wall line) and Uniform Building Code, all exterior wall lines in wood or steel frame construction must be braced. Depending on the size of the building some interior walls must be braced as well.\n",
      "question: is a shear wall a load bearing wall?\n",
      "answer:\n",
      "----------------------------\n",
      "Response:\n",
      " yes\n",
      "\n",
      "\n",
      "Prompt:\n",
      "Cannabis in Connecticut -- Cannabis in Connecticut is illegal for recreational use, but possession of small amounts is decriminalized. Medical usage is permitted.\n",
      "question: is it illegal to smoke weed in ct?\n",
      "answer:\n",
      "----------------------------\n",
      "Response:\n",
      " yes\n",
      "\n",
      "\n",
      "Prompt:\n",
      "Croatia at the FIFA World Cup -- Croatia national football team have appeared in the FIFA World Cup on five occasions (in 1998, 2002, 2006, 2014 and 2018) since gaining independence in 1991. Before that, from 1930 to 1990 Croatia was part of Yugoslavia. For World Cup records and appearances in that period, see Yugoslavia national football team and Serbia at the FIFA World Cup. Their best result thus far was silver position at the 2018 final, where they lost 4-2 to France.\n",
      "question: has croatia ever placed in the world cup?\n",
      "answer:\n",
      "----------------------------\n",
      "Response:\n",
      " yes\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generation_examples = model.generate(prompts[0:3], generation_config=short_generation_config)\n",
    "for index, generated_text in enumerate(generation_examples.generation[\"text\"]):\n",
    "    print(f\"Prompt:\\n{prompts[index]}\")\n",
    "    print(\"----------------------------\")\n",
    "    print(f\"Response:\\n{generated_text}\\n\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the model wants to respond with yes or no, which is reasonable given how we phrased the question. However, if we were expecting true or false responses, we'd have a problem. We can use the model's likelihood estimates over the vocabulary to get a sense for whether the model believes true or false would have been the better response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'decoder.output_projection'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We're interested in the activations from the last layer of the model, because this will allow us to calculate the\n",
    "# likelihoods.\n",
    "last_layer_name = model.module_names[-1]\n",
    "last_layer_name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last layer of the model corresponds to the \"probabilities\" of each token in the model vocabulary. That is, it is proportional to the conditional probability\n",
    "$$\n",
    "P(y_t \\vert y_{<t}, x),\n",
    "$$\n",
    "The probability distribution over the vocabulary of the next token given the preceding tokens $y_{<t}$, and the prompt text $x$. Thus, for each token $y_{t}$ in our input, we get back a 50K vector corresponding to the likelihood over the vocabulary of $y_{t+1}$. We only care about the last token in our input, as it houses the likelihood of the, as yet, unseen token the model will generate."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll stick with a zero-shot prompt for this task to demonstrate the utility of taking over the generation process"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to instantiate a tokenizer to obtain appropriate token indices for our labels. \n",
    "\n",
    "__NOTE__: All OPT models, regardless of size, used the same tokenizer. However, if you want to use a different type of model, a different tokenizer may be needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' False True'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "# extract the tokenizer ids associated with our labels\n",
    "label_token_ids = get_label_token_ids(tokenizer, prompts[0], label_ordering)\n",
    "# If you ever need to move back from token ids, you can use tokenizer.decode or tokenizer.batch_decode\n",
    "tokenizer.decode(label_token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activations matrix shape: torch.Size([121, 50272])\n",
      "Tokenized prompt length: 121\n",
      "Likelihoods of the labels: tensor([3.7656, 3.5000])\n",
      "Predicted Label: False\n"
     ]
    }
   ],
   "source": [
    "activations = model.get_activations(prompts[0], [last_layer_name], short_generation_config)\n",
    "last_layer_matrix = activations.activations[0][last_layer_name]\n",
    "# The shape of this tensor should be number of input tokens by the vocabulary size (n x 50272)\n",
    "print(f\"Activations matrix shape: {last_layer_matrix.shape}\")\n",
    "print(f\"Tokenized prompt length: {len(tokenizer.encode(prompts[0], add_special_tokens=False))}\")\n",
    "print(f\"Likelihoods of the labels: {last_layer_matrix[-1][label_token_ids].float()}\")\n",
    "predicted_label = get_label_with_highest_likelihood(last_layer_matrix, label_token_ids, label_map)\n",
    "print(f\"Predicted Label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number 1 Complete\n",
      "Batch number 2 Complete\n",
      "Batch number 3 Complete\n",
      "Batch number 4 Complete\n",
      "Batch number 5 Complete\n",
      "Batch number 6 Complete\n",
      "Batch number 7 Complete\n",
      "Batch number 8 Complete\n",
      "Batch number 9 Complete\n",
      "Batch number 10 Complete\n"
     ]
    }
   ],
   "source": [
    "# For memory management, we split the prompts into batches of size 10\n",
    "predicted_labels = []\n",
    "prompt_batches = split_prompts_into_batches(prompts)\n",
    "for batch_num, prompt_batch in enumerate(prompt_batches):\n",
    "    activations = model.get_activations(prompt_batch, [last_layer_name], short_generation_config)\n",
    "    print(f\"Batch number {batch_num+1} Complete\")\n",
    "    for activations_single_prompt in activations.activations:\n",
    "        last_layer_matrix = activations_single_prompt[last_layer_name]\n",
    "        predicted_label = get_label_with_highest_likelihood(last_layer_matrix, label_token_ids, label_map)\n",
    "        predicted_labels.append(predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy: 0.62\n",
      "Confusion Matrix with ordering ['false', 'true']\n",
      "[[36  3]\n",
      " [35 26]]\n",
      "========================================================\n",
      "Label: false, F1: 0.6545454545454545, Precision: 0.9230769230769231, Recall: 0.5070422535211268\n",
      "Label: true, F1: 0.5777777777777777, Precision: 0.4262295081967213, Recall: 0.896551724137931\n"
     ]
    }
   ],
   "source": [
    "# Map the labels from integers to strings for comparison to the string predicted labels in the confusion matrix\n",
    "bool_q_text_labels_string = [label_map[label] for label in bool_q_test_labels]\n",
    "report_metrics(predicted_labels, bool_q_text_labels_string, labels_order=label_ordering)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opt_notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
