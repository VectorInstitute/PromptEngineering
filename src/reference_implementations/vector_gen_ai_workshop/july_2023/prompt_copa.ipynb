{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "from random import sample\n",
    "from typing import List, Tuple\n",
    "\n",
    "import evaluate\n",
    "import kscope\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from evaluate import EvaluationModule\n",
    "from utils import (\n",
    "    copa_preprocessor,\n",
    "    create_first_prompt,\n",
    "    create_first_prompt_label,\n",
    "    create_second_prompt,\n",
    "    create_second_prompt_label,\n",
    "    split_prompts_into_batches,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conecting to the Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we connect to the service through which we'll interact with the LLMs and see which models are avaiable to us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = kscope.Client(gateway_host=\"llm.cluster.local\", gateway_port=3001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show all model instances that are currently active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '9b805833-72a0-4107-ad3c-d83c1e6f573d',\n",
       "  'name': 'OPT-175B',\n",
       "  'state': 'ACTIVE'},\n",
       " {'id': 'e1b1e312-f4a6-4c3d-9183-45005b8545ab',\n",
       "  'name': 'OPT-6.7B',\n",
       "  'state': 'ACTIVE'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.model_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = client.load_model(\"OPT-175B\")\n",
    "# If this model is not actively running, it will get launched in the background.\n",
    "# In this case, wait until it moves into an \"ACTIVE\" state before proceeding.\n",
    "while model.state != \"ACTIVE\":\n",
    "    time.sleep(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to configure the model to generate in the way we want it to. So we set a number of important parameters. For a discussion of the configuration parameters see: `src/reference_implementations/prompting_vector_llms/CONFIG_README.md`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = {\"max_tokens\": 25, \"top_k\": 4, \"top_p\": 1.0, \"rep_penalty\": 1.2, \"temperature\": 1.0}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Balanced Choice of Plausible Alternatives (CoPA)\n",
    "\n",
    "We'll work on an updated (harder) version of the CoPA dataset. We're only going to work with a small subset of the true development set in order to expedite LLM evaluation. \n",
    "\n",
    "The task, in short, is, given a context and a premise of either cause or effect, the model must choose between two distinct sentences to determine which is the logical following sentence. An example is:\n",
    "\n",
    "From the following choices,\n",
    "1) The author faded into obscurity.\n",
    "2) It was adapted into a movie.\n",
    "\n",
    "and an __effect__ premise, which logically follows the sentence \"The book became a huge failure.\" The answer is \"The author faded into obscurity.\" You can inspect the preprocessed dataset at \n",
    "\n",
    "`src/reference_implementations/prompting_vector_llms/prompt_ensembling/resources/copa_sample.tsv`\n",
    "\n",
    "We print out some of the demonstrations below that we've setup below for additional reference.\n",
    "\n",
    "__NOTE__: Construction of the prompts and some other functions that are used throughout this notebook have been pulled into a utils file \n",
    "\n",
    "`src/reference_implementations/prompting_vector_llms/prompt_ensembling/utils.py`\n",
    "\n",
    "In general, when we see an \"effect\" premise the string \", so\" is added to the phrase to be completed. If the premise is \"cause\" then then string \", because\" is added to the phrase to be completed. We strip out the ending period to improve fluency. See the demonstrations below for an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many of the initial data points should be reserved for demonstrations\n",
    "demonstration_candidates = 50\n",
    "# Number of demonstrations to be used per prompt\n",
    "n_demonstrations = 20\n",
    "\n",
    "copa_data_set = copa_preprocessor(\"copa_task_dataset/copa_sample.tsv\")\n",
    "\n",
    "demonstration_pool = copa_data_set[0:demonstration_candidates]\n",
    "test_pool = copa_data_set[demonstration_candidates:]\n",
    "demonstrations = sample(demonstration_pool, n_demonstrations)\n",
    "prompts: List[str] = []\n",
    "labels: List[str] = []\n",
    "int_labels: List[int] = []\n",
    "choices: List[Tuple[str, str]] = []\n",
    "for premise, label, phrase, first_choice, second_choice in test_pool:\n",
    "    int_labels.append(label)\n",
    "    choices.append((first_choice, second_choice))\n",
    "    labels.append(create_first_prompt_label(first_choice.lower(), second_choice.lower(), label))\n",
    "    prompts.append(create_first_prompt(demonstrations, premise, phrase, first_choice, second_choice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose the sentence that best completes the phrase\n",
      "\n",
      "\"the student's phone rang.\" or \"the student took notes.\"\n",
      "The teacher covered a lot of material, because the student took notes.\n",
      "\n",
      "\"the author faded into obscurity.\" or \"it was adapted into a movie.\"\n",
      "The book became a huge failure, so the author faded into obscurity.\n",
      "\n",
      "\"she contacted her lawyer.\" or \"she cancelled her appointments.\"\n",
      "The woman was summoned for jury duty, so she cancelled her appointments.\n",
      "\n",
      "\"i brought my umbrella to work.\" or \"i brought my laptop to work.\"\n",
      "The clouds looked dark, so i brought my umbrella to work.\n",
      "\n",
      "\"she believed her superiors were acting unethically.\" or \"she aspired to hold an executive position in the firm.\"\n",
      "The woman resigned from her job, because she believed her superiors were acting unethically.\n",
      "\n",
      "\"she read about the site's history.\" or \"she excavated ancient artifacts.\"\n",
      "The archeologist dug up the site, so she excavated ancient artifacts.\n",
      "\n",
      "\"the fugitive remained at large.\" or \"the police dropped the case.\"\n",
      "The fugitive hid from the police, so the fugitive remained at large.\n",
      "\n",
      "\"she felt shy.\" or \"she wanted attention.\"\n",
      "The woman exaggerated the details of the story, because she wanted attention.\n",
      "\n",
      "\"she got her ears pierced.\" or \"she got a tattoo.\"\n",
      "The girl wanted to wear earrings, so she got her ears pierced.\n",
      "\n",
      "\"her friend choked.\" or \"her friend fell over.\"\n",
      "The girl pushed her friend, so her friend fell over.\n",
      "\n",
      "\"the caller waited on the line.\" or \"the caller's phone lost reception.\"\n",
      "The secretary put the caller on hold, so the caller waited on the line.\n",
      "\n",
      "\"it scratched me.\" or \"i petted it.\"\n",
      "I snapped at the cat, because it scratched me.\n",
      "\n",
      "\"its products received positive consumer reviews.\" or \"some of its products were manufactured defectively.\"\n",
      "The company lost money, because some of its products were manufactured defectively.\n",
      "\n",
      "\"the teapot whistled.\" or \"the teapot cooled.\"\n",
      "The water in the teapot started to boil, so the teapot whistled.\n",
      "\n",
      "\"i was preparing to clean the bathroom.\" or \"i was preparing to wash my hands.\"\n",
      "I took off the rubber gloves, because i was preparing to wash my hands.\n",
      "\n",
      "\"it migrated for the winter.\" or \"it injured its wing.\"\n",
      "The bird couldn't fly, because it injured its wing.\n",
      "\n",
      "\"the pants were new.\" or \"the pocket had a hole.\"\n",
      "The pants had no defects, because the pants were new.\n",
      "\n",
      "\"it moved its headquarters to a suburban location.\" or \"it increased its marketing efforts for new products.\"\n",
      "The company was seeking highly specialized talent, so it moved its headquarters to a suburban location.\n",
      "\n",
      "\"the applicant failed a background check.\" or \"the applicant had experience for the job.\"\n",
      "The executive decided to hire the applicant, because the applicant had experience for the job.\n",
      "\n",
      "\"she threw away her napkin after eating.\" or \"she put her napkin on her lap before eating.\"\n",
      "The girl demonstrated poor etiquette, so she threw away her napkin after eating.\n",
      "\n",
      "\"They rested.\" or \"They kissed.\"\n",
      "The couple was very tired, so  \n"
     ]
    }
   ],
   "source": [
    "print(prompts[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this prompt performs on a small sample of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_generation_text(original_texts: List[str]) -> List[str]:\n",
    "    responses = []\n",
    "    for single_generation in original_texts:\n",
    "        generation_text: List[str] = re.findall(r\".*?[.!\\?]\", single_generation)\n",
    "        response_text = generation_text[0] if len(generation_text) > 0 else single_generation\n",
    "        responses.append(response_text.strip())\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split prompts into batches for memory management.\n",
    "responses = model.generate(prompts[0:3], generation_config).generation[\"text\"]\n",
    "# Process the responses\n",
    "responses = process_generation_text(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: \n",
      "Label: they rested.\n",
      "\n",
      "\n",
      "Response: I discarded the new issue.\n",
      "Label: i discarded the new issue.\n",
      "\n",
      "\n",
      "Response: \n",
      "Label: she felt self-conscious.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for response, label in zip(responses, labels[0:3]):\n",
    "    print(f\"Response: {response}\\nLabel: {label}\\n\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring generated Responses for the 20-shot Prompt Above\n",
    "\n",
    "Here we consider the performance of the demonstration prompt above on our subsampling of the CoPA dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run all of the examples through the model and collect the responses into the responses list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number 1 Complete\n",
      "Batch number 2 Complete\n",
      "Batch number 3 Complete\n",
      "Batch number 4 Complete\n",
      "Batch number 5 Complete\n",
      "Batch number 6 Complete\n",
      "Batch number 7 Complete\n",
      "Batch number 8 Complete\n",
      "Batch number 9 Complete\n",
      "Batch number 10 Complete\n"
     ]
    }
   ],
   "source": [
    "responses = []\n",
    "prompt_batches = split_prompts_into_batches(prompts)\n",
    "for batch_number, prompt_batch in enumerate(prompt_batches):\n",
    "    generations = model.generate(prompt_batch, generation_config)\n",
    "    print(f\"Batch number {batch_number+1} Complete\")\n",
    "    responses.extend(process_generation_text(generations.generation[\"text\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NOTE__: We have to find a way to map from the generated responses to one of the two choices in order make a prediction and measure accuracy. The simplest technique is \"Exact Match,\" where you select one of the choices if the model response exactly matches it. Otherwise you select at random. This approach is pretty brittle and we'll compare it to using a Rouge score below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exact Match Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_metric = evaluate.load(\"exact_match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_response_via_EM(response: str, first_choice: str, second_choice: str, em_metric: EvaluationModule) -> int:\n",
    "    response = response.lower()\n",
    "    first_choice = first_choice.lower()\n",
    "    second_choice = second_choice.lower()\n",
    "    # Use the rouge metric to score the response against the first choice or second choice as reference\n",
    "    em_score_0 = em_metric.compute(\n",
    "        predictions=[response], references=[first_choice], ignore_case=True, ignore_punctuation=True\n",
    "    )\n",
    "    em_score_1 = em_metric.compute(\n",
    "        predictions=[response], references=[second_choice], ignore_case=True, ignore_punctuation=True\n",
    "    )\n",
    "    # If the first score is larger we select the first choice, if they are equal we always choose the second\n",
    "    return 0 if em_score_0[\"exact_match\"] > em_score_1[\"exact_match\"] else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.57\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "for response, label_int, (first_choice, second_choice) in zip(responses, int_labels, choices):\n",
    "    predicted_label = score_response_via_EM(response, first_choice, second_choice, em_metric)\n",
    "    if predicted_label == label_int:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "print(f\"Accuracy: {correct/total}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rouge Scoring"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform scoring based on the generated text, by considering the rouge score of the responses using the label as the reference. We choose between the two available choices for the logical completion of the reference phrase. The model has provided a response and we treat each choice as a reference for the ROUGE metric. We take as the model's prediction the phrase with the highest ROUGE score compared to the response text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_metric = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_response_via_rouge(\n",
    "    response: str, first_choice: str, second_choice: str, rouge_metric: EvaluationModule\n",
    ") -> int:\n",
    "    response = response.lower()\n",
    "    first_choice = first_choice.lower()\n",
    "    second_choice = second_choice.lower()\n",
    "    # Use the rouge metric to score the response against the first choice or second choice as reference\n",
    "    rouge_0 = rouge_metric.compute(predictions=[response], references=[first_choice])\n",
    "    rouge_1 = rouge_metric.compute(predictions=[response], references=[second_choice])\n",
    "    # We take the average of the unigram and bi-gram rouge scores for the first and second choice results.\n",
    "    score_0 = (rouge_0[\"rouge1\"] + rouge_0[\"rouge2\"]) / 2.0\n",
    "    score_1 = (rouge_1[\"rouge1\"] + rouge_1[\"rouge2\"]) / 2.0\n",
    "    # If the first score is larger we select the first choice\n",
    "    return 0 if score_0 > score_1 else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "for response, label_int, (first_choice, second_choice) in zip(responses, int_labels, choices):\n",
    "    predicted_label = score_response_via_rouge(response, first_choice, second_choice, rouge_metric)\n",
    "    if predicted_label == label_int:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "print(f\"Accuracy: {correct/total}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Alternative Prompt Structure\n",
    "\n",
    "The above prompt structure doesn't quite seem to be performing the task we want very well. So let's try something different. Rather than having the two options followed by the premise, why don't we just have the phrases to be completed as examples and then have a final phrase to be completed. An example is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "demonstration_candidates = 50\n",
    "# Number of demonstrations to be used per prompt\n",
    "n_demonstrations = 20\n",
    "\n",
    "demonstration_pool = copa_data_set[0:demonstration_candidates]\n",
    "test_pool = copa_data_set[demonstration_candidates:]\n",
    "\n",
    "demonstrations = sample(demonstration_pool, n_demonstrations)\n",
    "prompts = []\n",
    "labels = []\n",
    "int_labels = []\n",
    "choices = []\n",
    "for premise, label, phrase, first_choice, second_choice in test_pool:\n",
    "    int_labels.append(label)\n",
    "    choices.append((first_choice, second_choice))\n",
    "    labels.append(create_second_prompt_label(first_choice.lower(), second_choice.lower(), label))\n",
    "    prompts.append(create_second_prompt(demonstrations, premise, phrase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete the phrase with a logical phrase.\n",
      "\n",
      "The girl came across an unfamiliar word in her textbook, so she looked the term up in the dictionary.\n",
      "\n",
      "The bird couldn't fly, because it injured its wing.\n",
      "\n",
      "The host cancelled the party, because she was certain she had the flu.\n",
      "\n",
      "The woman was summoned for jury duty, so she cancelled her appointments.\n",
      "\n",
      "The girl went down the hill on her bike, so her bike sped up.\n",
      "\n",
      "The book became a huge failure, so the author faded into obscurity.\n",
      "\n",
      "I snapped at the cat, because it scratched me.\n",
      "\n",
      "Everyone in the class turned to stare at the student, because the student's phone rang.\n",
      "\n",
      "The man felt thankful to be alive, because he was cured of cancer.\n",
      "\n",
      "The student misspelled the word, so the teacher corrected her.\n",
      "\n",
      "The woman hired a public relations consultant, because she decided to run for office.\n",
      "\n",
      "The pants had no defects, because the pants were new.\n",
      "\n",
      "My skin broke out into a rash, because i brushed against poison ivy in my yard.\n",
      "\n",
      "The girl wanted to wear earrings, so she got her ears pierced.\n",
      "\n",
      "The group was offended by the woman's faux pas, so the woman apologized.\n",
      "\n",
      "The woman lavished her friend with flattery, because she wanted to ask her friend for a favor.\n",
      "\n",
      "The child kicked the stack of blocks, so the blocks scattered all over the rug.\n",
      "\n",
      "The woman asked the man to leave, because he insulted her.\n",
      "\n",
      "The security guard could not identify the thief, because the surveillance camera was out of focus.\n",
      "\n",
      "The company lost money, because some of its products were manufactured defectively.\n",
      "\n",
      "The couple was very tired, so \n"
     ]
    }
   ],
   "source": [
    "print(prompts[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This prompt structure poses an interesting challenge. The model doesn't have a sense of the \"choices\" it should select from. So it is unlikely to generate responses that roughly match our expected values. Let's try one to see what we get for generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_____. (take a nap)\\n\\nThe boy complained, but he was ignored by his parents.\\n\\nThe man']\n"
     ]
    }
   ],
   "source": [
    "generation_example = model.generate(prompts[0], generation_config=generation_config)\n",
    "print(generation_example.generation[\"text\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what do we do? We can score the candidate responses by log likelihood, as estimated by the model, and choose the higher one as our label. That is, we complete the prompt with both labels and then extract the log-likelihoods of that input text from the perspective of the model. See the comments in the code below for more details on how this is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_prompts_with_choices(prompt_batch: List[Tuple[str, Tuple[str, str]]]) -> List[str]:\n",
    "    # We want to complete our prompt with the two possible choices and score those completions using our LM.\n",
    "    prompts_with_choices = []\n",
    "    for prompt, (first_choice, second_choice) in prompt_batch:\n",
    "        prompts_with_choices.append(f\"{prompt}{first_choice}\")\n",
    "        prompts_with_choices.append(f\"{prompt}{second_choice}\")\n",
    "    return prompts_with_choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number 1 Complete\n",
      "Batch number 2 Complete\n",
      "Batch number 3 Complete\n",
      "Batch number 4 Complete\n",
      "Batch number 5 Complete\n",
      "Batch number 6 Complete\n",
      "Batch number 7 Complete\n",
      "Batch number 8 Complete\n",
      "Batch number 9 Complete\n",
      "Batch number 10 Complete\n",
      "Batch number 11 Complete\n",
      "Batch number 12 Complete\n",
      "Batch number 13 Complete\n",
      "Batch number 14 Complete\n",
      "Batch number 15 Complete\n",
      "Batch number 16 Complete\n",
      "Batch number 17 Complete\n",
      "Batch number 18 Complete\n",
      "Batch number 19 Complete\n",
      "Batch number 20 Complete\n"
     ]
    }
   ],
   "source": [
    "likelihoods = []\n",
    "prompts_and_choices = pair_prompts_with_choices(list(zip(prompts, choices)))\n",
    "# prompts and choices is now twice as long as the original prompts and choices because the prompts have been completed\n",
    "# with the two possible choices\n",
    "prompt_batches = split_prompts_into_batches(prompts_and_choices)\n",
    "for batch_number, prompt_batch in enumerate(prompt_batches):\n",
    "    activations = model.get_activations(prompt_batch, [], generation_config)\n",
    "    print(f\"Batch number {batch_number+1} Complete\")\n",
    "    # Log probs stores all of the activations associated with the input prompt (which has been completed with one of\n",
    "    # the two sentences)\n",
    "    for logprobs, tokens in zip(activations.logprobs, activations.tokens):\n",
    "        # We only really care about the logprobs associated with the sentence to be completed\n",
    "        # (i.e. not the demonstrations or the question). So search for the last endline in the tokens and only\n",
    "        # sum the logprobs from there.\n",
    "        index = list(reversed(tokens)).index(\"\\n\") - 1\n",
    "        likelihoods.append(sum(logprobs[-index:]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a log likelihood for each prompt completion corresponding to completion with the first or second potential phrase. In the code below, we pair those up and compute which has the higher likelihood between the two options. This then becomes our \"predicted\" label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_likelihoods_to_labels(likelihoods: List[float]) -> Tuple[List[int], List[torch.Tensor]]:\n",
    "    # Need to group logprobs in twos because they represent likelihoods of the two completions\n",
    "    assert len(likelihoods) % 2 == 0\n",
    "    paired_likelihoods = [likelihoods[x : x + 2] for x in range(0, len(likelihoods), 2)]\n",
    "    predicted_labels = []\n",
    "    predicted_probs = []\n",
    "    softmax = nn.Softmax(dim=0)\n",
    "    for paired_likelihood in paired_likelihoods:\n",
    "        # Paired likelihood is the logprob sum for the first and second choice together\n",
    "        distribution = softmax(torch.tensor(paired_likelihood))\n",
    "        predicted_labels.append(np.argmax(paired_likelihood, axis=0))\n",
    "        predicted_probs.append(distribution)\n",
    "    return predicted_labels, predicted_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.77\n"
     ]
    }
   ],
   "source": [
    "prompt_1_pred_labels, prompt_1_pred_probs = post_process_likelihoods_to_labels(likelihoods)\n",
    "total = 0\n",
    "correct = 0\n",
    "for predicted_label, label_int in zip(prompt_1_pred_labels, int_labels):\n",
    "    if predicted_label == label_int:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "print(f\"Accuracy: {correct/total}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opt_notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
