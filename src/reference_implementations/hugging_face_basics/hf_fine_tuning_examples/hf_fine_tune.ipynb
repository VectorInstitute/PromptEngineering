{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from ag_news_trainer import infer, train\n",
    "from custom_dataloaders import construct_dataloaders\n",
    "from roberta_classification_model import RobertaClsModel\n",
    "from torch import cuda\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose your dataset. Make sure that the number of classes in your model matches the number of different labels in that dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AG News Dataset for classifying news headlines.\n",
    "dataset_name = \"ag_news\"\n",
    "dataset_num_labels = 4\n",
    "\n",
    "## Uncomment the code below to use the SST2 dataset for sentiment analysis, \n",
    "# dataset_name = \"SetFit/sst2\"\n",
    "# dataset_num_labels = 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose your pre-trained model and setup the dataloaders.\n",
    "\n",
    "By default, the HuggingFace Transformer models will provide the dense hidden states of the last layer, one vector for each token in the input. These vectors are not directly usable for our task of classification at the sequence level. \n",
    "\n",
    "One popular way to address this limitation is to add a \"classification head\"- a linear projection layer (`nn.Dense`)- on top of one of these token vectors in the output. For bi-directional encoder-only transformers such as BERT and RoBERTa, this layer will be added to the virtual token \\[CLS\\] at the beginning of the input. For decoder-only transformers such as GPT and OPT, this projection layer might be added to the last token in the sentence.\n",
    "\n",
    "HuggingFace provides a convenient way to add this layer to your pre-trained model. For a wide range of base models including RoBERTa and OPT, you can load the pre-trained model with the projection layer added and initialized for you using the `AutoModelForSequenceClassification` class:\n",
    "\n",
    "```python\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\")\n",
    "```\n",
    "\n",
    "To demonstrate how this useful abstraction works, we've manually added a classification head on top of a HuggingFace **RoBERTa** model in a custom torch.nn module. We encourage you to take a look at our implementation in *roberta_classification_model.py* and see whether the behavior differs from that of AutoModelForSequenceClassification.\n",
    "\n",
    "Please note that if you need to experiment with a base model other than RoBERTa- for example, OPT- you will need to set `use_hf_sequence_classification = False` and use the HuggingFace AutoModelForSequenceClassification instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_hf_sequence_classification = True  # set to True to use the HuggingFace abstraction\n",
    "hf_model_name = \"roberta-base\"  \n",
    "\n",
    "# Uncomment the code below to use facebook/opt-125m as the base model.\n",
    "# hf_model_name = \"facebook/opt-125m\"  # Also try \"facebook/opt-125m\" for OPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(hf_model_name)\n",
    "\n",
    "# Set the maximum number of tokens in each input.\n",
    "tokenizer.model_max_length = 256\n",
    "train_dataloader, val_dataloader, test_dataloader = construct_dataloaders(\n",
    "    batch_size=8, train_split_ratio=0.8, tokenizer=tokenizer, dataset_name=\"ag_news\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the different variables we'd like for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if cuda.is_available() else \"cpu\"\n",
    "print(f\"Detected Device {device}\")\n",
    "# We'll provide two options. First we create our own model on top of the vanilla RoBERTa model. The second is to use\n",
    "# HuggingFace's AutoModel class, which essentially does the same thing for RoBERTa, but with support additional base\n",
    "# models such as OPT and GPT-J.\n",
    "use_hf_sequence_classification = True\n",
    "classifier_model = (\n",
    "    AutoModelForSequenceClassification.from_pretrained(hf_model_name, num_labels=4)\n",
    "    if use_hf_sequence_classification\n",
    "    else RobertaClsModel()\n",
    ")\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "n_training_epochs = 1\n",
    "n_training_steps = 300"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model on the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Begin Model Training...\")\n",
    "train(\n",
    "    classifier_model,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    loss_function,\n",
    "    device,\n",
    "    n_training_epochs,\n",
    "    n_training_steps,\n",
    ")\n",
    "print(\"Training Complete\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the final model to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Saving model...\")\n",
    "hf_model_name_formatted = hf_model_name.split(\"/\")[-1]\n",
    "dataset_name_formatted = dataset_name.split(\"/\")[-1]\n",
    "output_model_file = f\"./{hf_model_name_formatted}_{dataset_name_formatted}.bin\"\n",
    "torch.save(classifier_model, output_model_file)\n",
    "print(\"Model saved to\", output_model_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model back up and perform inference on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading model...\")\n",
    "classifier_model = torch.load(output_model_file)\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "print(\"Evaluating model on test set...\")\n",
    "test_accuracy, test_loss = infer(classifier_model, loss_function, test_dataloader, device)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}%\")\n",
    "print(\"Model evaluated.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf_fine_tune_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
