{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from custom_dataloaders import construct_dataloaders\n",
    "from hf_trainer import infer, train\n",
    "from roberta_classification_model import RobertaClsModel\n",
    "from torch import cuda\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose your dataset. Make sure that the number of classes in your model matches the number of different labels in that dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AG News Dataset for classifying news headlines.\n",
    "dataset_name = \"ag_news\"\n",
    "dataset_num_labels = 4\n",
    "\n",
    "# Uncomment the code below to use the SST2 dataset for sentiment analysis.\n",
    "# NOTE: If you're going to use the SST2 dataset, you need to make sure that use_hf_sequence_classification = True\n",
    "# The custom RoBERTa model is only defined for ag_news.\n",
    "# NOTE: For SST2 to train well, you'll need to adjust the learning rate and weight decay in the hf_trainer file\n",
    "# A good place to start is lr=0.00001, weight_decay=0.001\n",
    "# dataset_name = \"SetFit/sst2\"\n",
    "# dataset_num_labels = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose your pre-trained model and setup the dataloaders.\n",
    "\n",
    "By default, the HuggingFace Transformer models will provide the dense hidden states of the last layer, one vector for each token in the input. These vectors are not directly usable for our task of classification at the sequence level. While they can be combined using the \"attention mechanism\" into a single class-specific sequence-level representation, we opt for an easier solution here.\n",
    "\n",
    "This can be done by adding a \"classification head\"- a linear projection layer (`nn.Dense`)- on top of one of these token vectors in the output. For bi-directional encoder-only transformers such as BERT and RoBERTa, there is a specific token at the beginning of the input, \\[CLS\\], that representation the entire document. This layer will be added on top of the vector of the \\[CLS\\] token. For decoder-only transformers such as GPT and OPT, this projection layer might be added to the last non-pad token in the sentence.\n",
    "\n",
    "The HuggingFace Transformers library provides a convenient way to add this layer to your pre-trained model. For a wide range of base models including RoBERTa and OPT, you can load the pre-trained model with the projection layer added and initialized for you using the `AutoModelForSequenceClassification` class:\n",
    "\n",
    "```python\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\")\n",
    "```\n",
    "\n",
    "To demonstrate how this useful abstraction works, we've manually added a classification head on top of a HuggingFace [**RoBERTa**](https://arxiv.org/abs/1907.11692) model in a custom torch.nn module. The RoBERTa model is very similar to the BERT model, with a few minor differences. For example the next-sentence prediction task was removed in pretraining of RoBERTa.\n",
    "\n",
    "We encourage you to take a look at our implementation in *roberta_classification_model.py* and see whether the behavior differs from that of AutoModelForSequenceClassification. Note that there is also an implementation of the \"decoder-only\" style head in *gpt2_classification_model.py*.\n",
    "\n",
    "Please note that if you need to experiment with a base model other than RoBERTa- for example, OPT- you will need to set `use_hf_sequence_classification = False` and use the HuggingFace AutoModelForSequenceClassification instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: If you're going to use the SST2 dataset, you need to make sure that use_hf_sequence_classification = True\n",
    "# The custom RoBERTa model is only defined for ag_news\n",
    "use_hf_sequence_classification = True  # set to True to use the HuggingFace abstraction\n",
    "hf_model_name = \"roberta-base\"\n",
    "\n",
    "# Uncomment the code below to use facebook/opt-125m as the base model.\n",
    "# Note that using OPT-125m requires the use_hf_sequence_classification = True\n",
    "# use_hf_sequence_classification = True\n",
    "# hf_model_name = \"facebook/opt-125m\"  # Also try \"facebook/opt-125m\" for OPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset ag_news (/h/arashaf/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "835d790c94524a9a94415806fcf51191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /h/arashaf/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548/cache-e3899bce123fc79a.arrow\n",
      "Loading cached processed dataset at /h/arashaf/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548/cache-9deb23ce346562f6.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data example encoding: tensor([    0, 22886,  1641,   354, 46707,     7,   492,  1920,   364,    12,\n",
      "         6380,  9388,    13,   614,    12, 10111,  4247,    20,  6955,   935,\n",
      "        22424,   189,   120,    10,   828,    55, 11138,   220,    76,   511,\n",
      "            5,  2443,    14,  7299,  1641,   354, 46707,    18,   213, 11913,\n",
      "         8569,   806,     6,  6431,    42,   186,     6,    40,   492,  1434,\n",
      "            9,   614,    12, 10111,  1905,  4247,     5,   276,  1920,   364,\n",
      "           12,  6380,  9388,    14,    16,   122,   129,   577,    15, 35456,\n",
      "          906,  2793,  4247,     4,     2,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1])\n",
      "Training data example decoding: Intellisync to give push e-mail capability for low-cost phones The wireless airwaves may get a bit more crowded next year following the announcement that Intellisync's goAnywhere technology, unveiled this week, will give users of low-cost feature phones the same push e-mail capability that is now only available on pricier smart phones.\n"
     ]
    }
   ],
   "source": [
    "# Create a tokenizer instance for a pretrained model vocabulary.\n",
    "tokenizer = AutoTokenizer.from_pretrained(hf_model_name)\n",
    "\n",
    "# Set the maximum number of tokens in each input.\n",
    "tokenizer.model_max_length = 512\n",
    "# Create data loader objects for train, validation, and test splits.\n",
    "train_dataloader, val_dataloader, test_dataloader = construct_dataloaders(\n",
    "    batch_size=8, train_split_ratio=0.8, tokenizer=tokenizer, dataset_name=dataset_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the different variables we'd like for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Device cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if cuda.is_available() else \"cpu\"\n",
    "print(f\"Detected Device {device}\")\n",
    "# We'll provide two options. First we create our own model on top of the vanilla RoBERTa model. The second is to use\n",
    "# HuggingFace's AutoModel class, which essentially does the same thing for RoBERTa, but with support additional base\n",
    "# models such as OPT and GPT-J.\n",
    "classifier_model = (\n",
    "    AutoModelForSequenceClassification.from_pretrained(hf_model_name, num_labels=dataset_num_labels)\n",
    "    if use_hf_sequence_classification\n",
    "    else RobertaClsModel()\n",
    ")\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "n_training_epochs = 1\n",
    "n_training_steps = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model on the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Model Training...\n",
      "Starting Epoch 0\n",
      "Completed batch number: 100 of 12000 in loader\n",
      "Training Loss over last 100 steps: 1.075119908452034\n",
      "Training Accuracy over last 100 steps: 57.301980198019805%\n",
      "Validation Loss: 0.48847083688951004\n",
      "Validation Accuracy: 83.08823529411765%\n",
      "Completed batch number: 200 of 12000 in loader\n",
      "Training Loss over last 100 steps: 0.42622626300901173\n",
      "Training Accuracy over last 100 steps: 86.75%\n",
      "Validation Loss: 0.29673660466191815\n",
      "Validation Accuracy: 91.42156862745098%\n",
      "Completed batch number: 300 of 12000 in loader\n",
      "Training Loss over last 100 steps: 0.30904635064303876\n",
      "Training Accuracy over last 100 steps: 90.0%\n",
      "Validation Loss: 0.32352423938173874\n",
      "Validation Accuracy: 89.2156862745098%\n",
      "Training rounds complete. Validating on entire validation set.\n",
      "Completed 300 of 3000...\n",
      "Completed 600 of 3000...\n",
      "Completed 900 of 3000...\n",
      "Completed 1200 of 3000...\n",
      "Completed 1500 of 3000...\n",
      "Completed 1800 of 3000...\n",
      "Completed 2100 of 3000...\n",
      "Completed 2400 of 3000...\n",
      "Completed 2700 of 3000...\n",
      "Completed 3000 of 3000...\n",
      "------------------------------------------------\n",
      "Training Loss Epoch: 0.6014593096691311\n",
      "Validation Loss: 0.3064484531842172\n",
      "Validation accuracy: 89.6375\n",
      "------------------------------------------------\n",
      "Training Complete\n"
     ]
    }
   ],
   "source": [
    "print(\"Begin Model Training...\")\n",
    "# Initiates an Adam optimizer and runs the training loop.\n",
    "train(\n",
    "    classifier_model,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    loss_function,\n",
    "    device,\n",
    "    n_training_epochs,\n",
    "    n_training_steps,\n",
    ")\n",
    "print(\"Training Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once training is complete, we save the fine-tuned model to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n",
      "Model saved to ./roberta-base_ag_news.bin\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving model...\")\n",
    "hf_model_name_formatted = hf_model_name.split(\"/\")[-1]\n",
    "dataset_name_formatted = dataset_name.split(\"/\")[-1]\n",
    "output_model_file = f\"./{hf_model_name_formatted}_{dataset_name_formatted}.bin\"\n",
    "torch.save(classifier_model, output_model_file)\n",
    "print(\"Model saved to\", output_model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load the model saved above, perform inference on the test set and measure loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Model loaded.\n",
      "Evaluating model on test set...\n",
      "Completed 300 of 950...\n",
      "Completed 600 of 950...\n",
      "Completed 900 of 950...\n",
      "Test Loss: 0.31733756162422266\n",
      "Test Accuracy: 89.4342105263158%\n",
      "Model evaluated.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading model...\")\n",
    "classifier_model = torch.load(output_model_file)\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "print(\"Evaluating model on test set...\")\n",
    "test_accuracy, test_loss = infer(classifier_model, loss_function, test_dataloader, device)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}%\")\n",
    "print(\"Model evaluated.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompt_engineering",
   "language": "python",
   "name": "prompt_engineering"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
