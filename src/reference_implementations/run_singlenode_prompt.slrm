#!/bin/bash

#SBATCH --job-name=prompt-singlenode-experiments
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=4
#SBATCH --gres=gpu:1
#SBATCH --mem=16G
#SBATCH --partition=rtx6000
#SBATCH --qos=normal
#SBATCH --output=job_%x_%j.out
#SBATCH --error=job_%x_%j.err

# Note:
#	  ntasks: Total number of processes to use across world
#	  ntasks-per-node: How many processes each node should create
#		- If this is equal to the number of GPUs on the node, each GPU will run
#			a copy of the `srun ...` code
#		- `jax.distributed.initialize` requires that each GPU run a copy of the
#			code, in order to call initialize with no arguments

# Set location of host and access port
MAIN_HOST=$(hostname -s)
export MASTER_ADDR=$MAIN_HOST
export MASTER_PORT=52069

# Set NCCL options
# export NCCL_DEBUG=INFO
# NCCL backend to communicate between GPU workers is not provided in vector's cluster.
# Disable this option in slurm.
export NCCL_IB_DISABLE=1

if [[ "${SLURM_JOB_PARTITION}" == "t4v2" ]] || \
    [[ "${SLURM_JOB_PARTITION}" == "rtx6000" ]]; then
    echo export NCCL_SOCKET_IFNAME=bond0 on "${SLURM_JOB_PARTITION}"
    export NCCL_SOCKET_IFNAME=bond0
fi

# Process input args
SCRIPT=$1
LOG_DIR=$2
LOG_PATH="${LOG_DIR}/log_${SLURM_JOB_ID}_rank_\${SLURM_PROCID}.log"
EXP_TYPE=$3
LR=$4
WITH_INST=$5
LEN=$6

echo "Placing logs in: ${LOG_DIR}"

echo "World size: ${SLURM_NTASKS}"
echo "Number of nodes: ${SLURM_NNODES}"
NUM_GPUs=$(nvidia-smi --query-gpu=name --format=csv,noheader | wc -l)
echo "GPUs per node: ${NUM_GPUs}"

# Make logging directories.
mkdir -p "${LOG_DIR}"

# Run on all nodes
/opt/slurm/bin/srun -N"${SLURM_NNODES}" -l \
    bash -c "bash ${SCRIPT} LEN=${LEN} EXP_TYPE=${EXP_TYPE} LR=${LR} WITH_INST=${WITH_INST}>> ${LOG_PATH} 2>&1"
